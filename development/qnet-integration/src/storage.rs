//! Persistent storage implementation for QNet blockchain

use rocksdb::{DB, Options, ColumnFamily, ColumnFamilyDescriptor, WriteBatch};
use qnet_state::{Block, Account, Transaction};
use crate::errors::{IntegrationError, IntegrationResult};
use std::path::Path;
use std::collections::HashMap;
use std::time::{SystemTime, UNIX_EPOCH};
use std::sync::{Arc, RwLock};
use hex;
use sha3::{Sha3_256, Digest};
use bincode;
use futures;
use serde_json::{json, Value};
use serde::{Serialize, Deserialize};
use chrono;

/// Failover event for tracking producer failures
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct FailoverEvent {
    pub height: u64,
    pub failed_producer: String,
    pub emergency_producer: String,
    pub reason: String,
    pub timestamp: i64,
    pub block_type: String, // "microblock" or "macroblock"
}

pub struct PersistentStorage {
    db: DB,
}

#[derive(Debug, Clone, Default)]
pub struct StorageStats {
    pub total_blocks: u64,
    pub total_transactions: u64,
    pub total_accounts: u64,
    pub latest_height: u64,
}

/// Transaction pool with TTL cleanup for efficient microblock storage
/// Stores transactions separately from microblocks to avoid duplication
#[derive(Debug)]
pub struct TransactionPool {
    /// Map of transaction hash to transaction
    transactions: Arc<RwLock<HashMap<[u8; 32], Transaction>>>,
    /// Map of transaction hash to creation timestamp
    creation_times: Arc<RwLock<HashMap<[u8; 32], u64>>>,
    /// TTL in hours after which transactions are eligible for cleanup
    cleanup_after_hours: u32,
}

impl TransactionPool {
    /// Create new transaction pool with default TTL of 24 hours
    pub fn new() -> Self {
        Self {
            transactions: Arc::new(RwLock::new(HashMap::new())),
            creation_times: Arc::new(RwLock::new(HashMap::new())),
            cleanup_after_hours: 24, // 24 hours retention for local hot storage
        }
    }
    
    /// Store transaction with current timestamp
    pub fn store_transaction(&self, tx_hash: [u8; 32], transaction: Transaction) -> Result<(), IntegrationError> {
        let current_time = SystemTime::now()
            .duration_since(UNIX_EPOCH)
            .map_err(|e| IntegrationError::Other(format!("Time error: {}", e)))?
            .as_secs();
            
        {
            let mut transactions = self.transactions.write()
                .map_err(|e| IntegrationError::Other(format!("Lock error: {}", e)))?;
            let mut creation_times = self.creation_times.write()
                .map_err(|e| IntegrationError::Other(format!("Lock error: {}", e)))?;
                
            transactions.insert(tx_hash, transaction);
            creation_times.insert(tx_hash, current_time);
        }
        
        Ok(())
    }
    
    /// Get transaction by hash
    pub fn get_transaction(&self, tx_hash: &[u8; 32]) -> Option<Transaction> {
        self.transactions.read()
            .ok()?
            .get(tx_hash)
            .cloned()
    }
    
    /// Get multiple transactions by hashes
    pub fn get_transactions(&self, tx_hashes: &[[u8; 32]]) -> Vec<Option<Transaction>> {
        if let Ok(transactions) = self.transactions.read() {
            tx_hashes.iter()
                .map(|hash| transactions.get(hash).cloned())
                .collect()
        } else {
            vec![None; tx_hashes.len()]
        }
    }
    
    /// Clean up old transactions (only removes duplicates, not original blockchain data)
    pub fn cleanup_old_duplicates(&self) -> Result<usize, IntegrationError> {
        let current_time = SystemTime::now()
            .duration_since(UNIX_EPOCH)
            .map_err(|e| IntegrationError::Other(format!("Time error: {}", e)))?
            .as_secs();
            
        let cutoff_time = current_time.saturating_sub(self.cleanup_after_hours as u64 * 3600);
        let mut removed_count = 0;
        
        {
            let mut transactions = self.transactions.write()
                .map_err(|e| IntegrationError::Other(format!("Lock error: {}", e)))?;
            let mut creation_times = self.creation_times.write()
                .map_err(|e| IntegrationError::Other(format!("Lock error: {}", e)))?;
            
            // Only remove transactions older than TTL 
            // In production, we should also check if transaction is already in finalized blocks
            let old_hashes: Vec<[u8; 32]> = creation_times.iter()
                .filter(|(_, &time)| time < cutoff_time)
                .map(|(hash, _)| *hash)
                .collect();
                
            for hash in old_hashes {
                transactions.remove(&hash);
                creation_times.remove(&hash);
                removed_count += 1;
            }
        }
        
        if removed_count > 0 {
            println!("[TransactionPool] üßπ Cleaned up {} old transaction duplicates", removed_count);
        }
        
        Ok(removed_count)
    }
    
    /// Get pool statistics
    pub fn get_stats(&self) -> Result<(usize, usize), IntegrationError> {
        let tx_count = self.transactions.read()
            .map_err(|e| IntegrationError::Other(format!("Lock error: {}", e)))?
            .len();
        let time_count = self.creation_times.read()
            .map_err(|e| IntegrationError::Other(format!("Lock error: {}", e)))?
            .len();
            
        Ok((tx_count, time_count))
    }
}

// ============================================================================
// TIERED STORAGE ARCHITECTURE
// ============================================================================
// 
// QNET uses Transaction/Compute Sharding for parallel processing,
// NOT State Sharding for storage division.
//
// SHARDING = Parallel transaction PROCESSING (CPU cores)
// STORAGE  = Tiered by node type (Light/Full/Super)
//
// ALL nodes receive ALL blocks. Storage differs by:
// - What data is kept (headers vs full blocks)
// - How long data is kept (pruning window)
//
// ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
// ‚îÇ                    STORAGE TIERS                            ‚îÇ
// ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
// ‚îÇ                                                              ‚îÇ
// ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê              ‚îÇ
// ‚îÇ  ‚îÇ  Light  ‚îÇ  ‚îÇ  Full   ‚îÇ  ‚îÇ Super/Bootstrap ‚îÇ              ‚îÇ
// ‚îÇ  ‚îÇ  Node   ‚îÇ  ‚îÇ  Node   ‚îÇ  ‚îÇ     Node        ‚îÇ              ‚îÇ
// ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò              ‚îÇ
// ‚îÇ       ‚îÇ            ‚îÇ                ‚îÇ                        ‚îÇ
// ‚îÇ  Headers      Full blocks      Full blocks                   ‚îÇ
// ‚îÇ  only         + pruning        NO pruning                    ‚îÇ
// ‚îÇ  (1K blocks)  (30 days)        (full history)                ‚îÇ
// ‚îÇ       ‚îÇ            ‚îÇ                ‚îÇ                        ‚îÇ
// ‚îÇ   ~100 MB       ~500 GB           ~2 TB                      ‚îÇ
// ‚îÇ                                                              ‚îÇ
// ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
//
// ============================================================================

/// Storage tier configuration for different node types
/// This is about WHAT and HOW LONG to store, NOT which shards
#[derive(Debug, Clone)]
pub struct StorageTierConfig {
    /// Whether to store full transaction data or just block headers
    pub store_full_blocks: bool,
    /// Maximum storage size in bytes
    pub max_storage_bytes: u64,
    /// Pruning window in blocks (0 = no pruning, keep all history)
    pub pruning_window_blocks: u64,
    /// Whether to apply aggressive compression to old blocks
    pub compress_old_blocks: bool,
}

impl StorageTierConfig {
    /// Light node: Headers only, ~100MB max, keep last 1000 headers
    /// - Mobile wallets, IoT devices
    /// - Only verify block headers, not transactions
    /// - Rely on Full/Super nodes for transaction data
    pub fn light() -> Self {
        Self {
            store_full_blocks: false,
            max_storage_bytes: 100 * 1024 * 1024, // 100 MB
            pruning_window_blocks: 1_000, // Keep last 1000 block headers
            compress_old_blocks: false, // Headers are already small
        }
    }
    
    /// Full node: Full blocks + pruning, ~500GB max, keep 30 days
    /// - Desktop/server nodes
    /// - Full transaction verification
    /// - Participate in consensus (if reputation >= 70%)
    /// - Prune old blocks to manage storage
    pub fn full() -> Self {
        Self {
            store_full_blocks: true,
            max_storage_bytes: 500 * 1024 * 1024 * 1024, // 500 GB
            pruning_window_blocks: 2_592_000, // ~30 days at 1 block/sec
            compress_old_blocks: true, // Apply Zstd-22 to blocks > 7 days old
        }
    }
    
    /// Super/Bootstrap node: Full blocks, NO pruning, ~2TB
    /// - High-performance servers
    /// - Store complete blockchain history
    /// - Always participate in consensus
    /// - Serve historical data to other nodes
    pub fn super_node() -> Self {
        Self {
            store_full_blocks: true,
            max_storage_bytes: 2 * 1024 * 1024 * 1024 * 1024, // 2 TB
            pruning_window_blocks: 0, // No pruning - keep ALL history
            compress_old_blocks: true, // Apply progressive compression
        }
    }
    
    /// Check if this tier should store full block data
    pub fn should_store_full_block(&self) -> bool {
        self.store_full_blocks
    }
    
    /// Check if a block at given height should be pruned
    pub fn should_prune_block(&self, block_height: u64, current_height: u64) -> bool {
        if self.pruning_window_blocks == 0 {
            return false; // No pruning for this tier
        }
        
        // Keep blocks within the pruning window
        if current_height < self.pruning_window_blocks {
            return false; // Not enough blocks yet
        }
        
        block_height < current_height - self.pruning_window_blocks
    }
    
    /// Get the compression level for a block based on its age
    /// Returns Zstd compression level (0 = none, 3 = light, 9 = medium, 22 = max)
    pub fn get_compression_level(&self, block_age_seconds: u64) -> i32 {
        if !self.compress_old_blocks {
            return 3; // Light compression for all
        }
        
        match block_age_seconds {
            0..=3600 => 3,           // < 1 hour: light (Zstd-3)
            3601..=86400 => 9,       // 1h - 1 day: medium (Zstd-9)
            86401..=604800 => 15,    // 1d - 7 days: heavy (Zstd-15)
            _ => 22,                  // > 7 days: maximum (Zstd-22)
        }
    }
}

// ============================================================================
// GRACEFUL DEGRADATION SYSTEM
// ============================================================================
// When storage fills up, nodes automatically degrade to lower tiers:
// Super ‚Üí Full ‚Üí Light
// This ensures the node keeps running even with limited storage.
// ============================================================================

/// Storage health status
#[derive(Debug, Clone, Copy, PartialEq)]
pub enum StorageHealth {
    /// Storage is healthy (< 70% full)
    Healthy,
    /// Storage is getting full (70-85% full) - start aggressive pruning
    Warning,
    /// Storage is almost full (85-95% full) - emergency pruning
    Critical,
    /// Storage is full (>= 95%) - graceful degradation
    Full,
}

impl StorageHealth {
    pub fn from_percentage(percentage: f64) -> Self {
        match percentage {
            p if p < 70.0 => StorageHealth::Healthy,
            p if p < 85.0 => StorageHealth::Warning,
            p if p < 95.0 => StorageHealth::Critical,
            _ => StorageHealth::Full,
        }
    }
    
    pub fn as_str(&self) -> &'static str {
        match self {
            StorageHealth::Healthy => "HEALTHY",
            StorageHealth::Warning => "WARNING",
            StorageHealth::Critical => "CRITICAL",
            StorageHealth::Full => "FULL",
        }
    }
}

/// Graceful degradation manager
/// Automatically downgrades node storage tier when disk fills up
pub struct GracefulDegradation {
    /// Original storage mode (what user configured)
    original_mode: StorageMode,
    /// Current effective mode (may be degraded)
    current_mode: StorageMode,
    /// Whether degradation is active
    is_degraded: bool,
    /// Timestamp when degradation started
    degraded_since: Option<u64>,
}

impl GracefulDegradation {
    pub fn new(mode: StorageMode) -> Self {
        Self {
            original_mode: mode,
            current_mode: mode,
            is_degraded: false,
            degraded_since: None,
        }
    }
    
    /// Check if we need to degrade based on storage health
    pub fn check_and_degrade(&mut self, health: StorageHealth) -> Option<StorageMode> {
        match health {
            StorageHealth::Full => {
                // Degrade to next lower tier
                let new_mode = match self.current_mode {
                    StorageMode::Super => {
                        println!("[GracefulDegradation] üîª Super ‚Üí Full: Storage full, switching to pruning mode");
                        StorageMode::Full
                    },
                    StorageMode::Full => {
                        println!("[GracefulDegradation] üîª Full ‚Üí Light: Storage full, switching to headers-only mode");
                        StorageMode::Light
                    },
                    StorageMode::Light => {
                        // Already at lowest tier, can't degrade further
                        println!("[GracefulDegradation] ‚ö†Ô∏è Already at Light mode, cannot degrade further!");
                        return None;
                    }
                };
                
                self.current_mode = new_mode;
                self.is_degraded = true;
                self.degraded_since = Some(
                    std::time::SystemTime::now()
                        .duration_since(std::time::UNIX_EPOCH)
                        .unwrap_or_default()
                        .as_secs()
                );
                
                Some(new_mode)
            },
            StorageHealth::Healthy if self.is_degraded => {
                // Storage is healthy again, try to restore original mode
                // Only restore if we've been degraded for at least 1 hour
                let now = std::time::SystemTime::now()
                    .duration_since(std::time::UNIX_EPOCH)
                    .unwrap_or_default()
                    .as_secs();
                
                if let Some(since) = self.degraded_since {
                    if now - since > 3600 {
                        println!("[GracefulDegradation] üî∫ Restoring to {} mode (storage healthy)", 
                            match self.original_mode {
                                StorageMode::Light => "Light",
                                StorageMode::Full => "Full",
                                StorageMode::Super => "Super",
                            });
                        self.current_mode = self.original_mode;
                        self.is_degraded = false;
                        self.degraded_since = None;
                        return Some(self.original_mode);
                    }
                }
                None
            },
            _ => None,
        }
    }
    
    pub fn get_current_mode(&self) -> StorageMode {
        self.current_mode
    }
    
    pub fn is_degraded(&self) -> bool {
        self.is_degraded
    }
}

// ============================================================================
// LIGHT NODE ROTATION (Auto-cleanup old headers)
// ============================================================================
// Light nodes automatically delete old block headers to maintain ~100MB size
// This is a FIFO queue - oldest headers are deleted first
// ============================================================================

/// Light node header rotation configuration
pub struct LightNodeRotation {
    /// Maximum number of headers to keep
    max_headers: u64,
    /// Current header count
    current_count: u64,
}

impl LightNodeRotation {
    pub fn new(max_headers: u64) -> Self {
        Self {
            max_headers,
            current_count: 0,
        }
    }
    
    /// Check if we need to rotate (delete old headers)
    pub fn needs_rotation(&self) -> bool {
        self.current_count >= self.max_headers
    }
    
    /// Get number of headers to delete
    pub fn headers_to_delete(&self) -> u64 {
        if self.current_count > self.max_headers {
            self.current_count - self.max_headers
        } else {
            0
        }
    }
    
    /// Update count after adding a header
    pub fn increment(&mut self) {
        self.current_count += 1;
    }
    
    /// Update count after deleting headers
    pub fn decrement(&mut self, count: u64) {
        self.current_count = self.current_count.saturating_sub(count);
    }
}

impl PersistentStorage {
    /// Save raw data with a custom key
    pub fn save_raw(&self, key: &str, data: &[u8]) -> IntegrationResult<()> {
        self.db.put(key.as_bytes(), data)?;
        Ok(())
    }
    
    /// Load raw data with a custom key
    pub fn load_raw(&self, key: &str) -> IntegrationResult<Option<Vec<u8>>> {
        match self.db.get(key.as_bytes())? {
            Some(data) => Ok(Some(data)),
            None => Ok(None),
        }
    }
    
    pub fn new(data_dir: &str) -> IntegrationResult<Self> {
        let path = Path::new(data_dir);
        std::fs::create_dir_all(path)?;
        
        // Simple, reliable RocksDB configuration
        let mut opts = Options::default();
        opts.create_if_missing(true);
        opts.create_missing_column_families(true);
        
        // Basic settings that work reliably
        opts.set_max_open_files(1000);
        opts.set_use_fsync(false);
        opts.set_bytes_per_sync(1048576);
        opts.set_max_write_buffer_number(4);
        opts.set_write_buffer_size(67108864); // 64MB
        opts.set_target_file_size_base(67108864); // 64MB
        opts.set_min_write_buffer_number_to_merge(2);
        opts.set_level_zero_stop_writes_trigger(12);
        opts.set_level_zero_slowdown_writes_trigger(8);
        opts.set_compaction_style(rocksdb::DBCompactionStyle::Level);
        opts.set_max_background_jobs(4);
        opts.set_disable_auto_compactions(false);
        
        // Optimize for failover events storage
        let mut failover_opts = Options::default();
        failover_opts.set_compression_type(rocksdb::DBCompressionType::Lz4);
        
        let cfs = vec![
            ColumnFamilyDescriptor::new("blocks", Options::default()),
            ColumnFamilyDescriptor::new("transactions", Options::default()),
            ColumnFamilyDescriptor::new("accounts", Options::default()),
            ColumnFamilyDescriptor::new("metadata", Options::default()),
            ColumnFamilyDescriptor::new("microblocks", Options::default()),
            ColumnFamilyDescriptor::new("consensus", Options::default()),
            ColumnFamilyDescriptor::new("sync_state", Options::default()),
            ColumnFamilyDescriptor::new("pending_rewards", Options::default()),
            ColumnFamilyDescriptor::new("node_registry", Options::default()),
            ColumnFamilyDescriptor::new("ping_history", Options::default()),
            ColumnFamilyDescriptor::new("failover_events", failover_opts),
            ColumnFamilyDescriptor::new("snapshots", Options::default()),
            ColumnFamilyDescriptor::new("tx_index", Options::default()), // O(1) transaction lookups
            ColumnFamilyDescriptor::new("tx_by_address", Options::default()), // Index: address -> [tx_hashes]
            ColumnFamilyDescriptor::new("attestations", Options::default()), // Light node attestations
            ColumnFamilyDescriptor::new("heartbeats", Options::default()),   // Full/Super node heartbeats
            ColumnFamilyDescriptor::new("poh_state", Options::default()),    // PoH state for fast validation (v2.19.13)
        ];
        
        let db = match DB::open_cf_descriptors(&opts, path, cfs) {
            Ok(db) => db,
            Err(e) => {
                eprintln!("‚ùå RocksDB Error: {}", e);
                return Err(IntegrationError::StorageError(format!("RocksDB initialization failed: {}", e)));
            }
        };
        
        Ok(Self { db })
    }
    
    pub async fn save_block(&self, block: &qnet_state::Block) -> IntegrationResult<()> {
        let block_cf = self.db.cf_handle("blocks")
            .ok_or_else(|| IntegrationError::StorageError("blocks column family not found".to_string()))?;
        let tx_cf = self.db.cf_handle("transactions")
            .ok_or_else(|| IntegrationError::StorageError("transactions column family not found".to_string()))?;
        let tx_index_cf = self.db.cf_handle("tx_index")
            .ok_or_else(|| IntegrationError::StorageError("tx_index column family not found".to_string()))?;
        let tx_by_addr_cf = self.db.cf_handle("tx_by_address")
            .ok_or_else(|| IntegrationError::StorageError("tx_by_address column family not found".to_string()))?;
        
        let block_key = format!("block_{}", block.height);
        let block_data = bincode::serialize(block)
            .map_err(|e| IntegrationError::SerializationError(e.to_string()))?;
        
        let mut batch = WriteBatch::default();
        batch.put_cf(&block_cf, block_key.as_bytes(), &block_data);
        
        // Store block hash mapping
        let hash_key = format!("hash_{}", block.height);
        let hash_data = bincode::serialize(&block.hash())
            .map_err(|e| IntegrationError::SerializationError(e.to_string()))?;
        batch.put_cf(&block_cf, hash_key.as_bytes(), &hash_data);
        
        // Store transactions with Zstd-3 compression for O(1) lookups
        // OPTIMIZATION: Zstd-3 is fast (~500MB/s) and provides ~30-50% reduction
        // Pattern compression is done in background to not block consensus
        for tx in &block.transactions {
            let tx_key = format!("tx_{}", tx.hash);
            let tx_data = bincode::serialize(tx)
                .map_err(|e| IntegrationError::SerializationError(e.to_string()))?;
            
            // PRODUCTION: Compress transactions with fast Zstd-3 (non-blocking)
            // ~30-50% reduction, <1ms per TX, doesn't block block production
            let compressed_tx = zstd::encode_all(&tx_data[..], 3)
                .unwrap_or_else(|_| tx_data.clone());
            
            batch.put_cf(&tx_cf, tx_key.as_bytes(), &compressed_tx);
            
            // INDEX: tx_hash -> block_height for O(1) transaction location
            batch.put_cf(&tx_index_cf, tx_key.as_bytes(), &block.height.to_be_bytes());
            
            // INDEX: address -> tx_hash for account transaction queries
            // Key format: addr_{address}_{timestamp}_{tx_hash} for chronological ordering
            let timestamp = tx.timestamp;
            let from_key = format!("addr_{}_{:016x}_{}", tx.from, timestamp, tx.hash);
            batch.put_cf(&tx_by_addr_cf, from_key.as_bytes(), tx.hash.as_bytes());
            
            if let Some(ref to) = tx.to {
                let to_key = format!("addr_{}_{:016x}_{}", to, timestamp, tx.hash);
                batch.put_cf(&tx_by_addr_cf, to_key.as_bytes(), tx.hash.as_bytes());
            }
        }
        
        // Update chain height
        let metadata_cf = self.db.cf_handle("metadata")
            .ok_or_else(|| IntegrationError::StorageError("metadata column family not found".to_string()))?;
        batch.put_cf(&metadata_cf, b"chain_height", &block.height.to_be_bytes());
        
        self.db.write(batch)?;
        Ok(())
    }
    
    pub fn get_chain_height(&self) -> IntegrationResult<u64> {
        let metadata_cf = self.db.cf_handle("metadata")
            .ok_or_else(|| IntegrationError::StorageError("metadata column family not found".to_string()))?;
        
        match self.db.get_cf(&metadata_cf, b"chain_height")? {
            Some(data) => {
                if data.len() >= 8 {
                    let height_bytes: [u8; 8] = data[0..8].try_into()
                        .map_err(|_| IntegrationError::StorageError("Invalid height data".to_string()))?;
                    Ok(u64::from_be_bytes(height_bytes))
                } else {
                    Ok(0)
                }
            }
            None => Ok(0),
        }
    }
    
    /// Set chain height to a specific value (for fork resolution)
    pub fn set_chain_height(&self, height: u64) -> IntegrationResult<()> {
        let metadata_cf = self.db.cf_handle("metadata")
            .ok_or_else(|| IntegrationError::StorageError("metadata column family not found".to_string()))?;
        
        self.db.put_cf(&metadata_cf, b"chain_height", &height.to_be_bytes())?;
        Ok(())
    }
    
    /// DATA CONSISTENCY: Reset chain height to 0 (DANGEROUS - requires explicit confirmation)
    /// This function will ONLY work if QNET_FORCE_RESET=1 AND QNET_CONFIRM_RESET=YES
    pub fn reset_chain_height(&self) -> IntegrationResult<()> {
        // SAFETY: Double-check that user REALLY wants to reset
        let force_reset = std::env::var("QNET_FORCE_RESET").unwrap_or_default();
        let confirm_reset = std::env::var("QNET_CONFIRM_RESET").unwrap_or_default();
        
        if force_reset != "1" || confirm_reset != "YES" {
            println!("[Storage] ‚ö†Ô∏è REFUSING to reset chain height!");
            println!("[Storage]    To reset, set BOTH:");
            println!("[Storage]    - QNET_FORCE_RESET=1");
            println!("[Storage]    - QNET_CONFIRM_RESET=YES");
            return Err(IntegrationError::StorageError(
                "Chain height reset blocked - missing confirmation flags".to_string()
            ));
        }
        
        // Additional safety: Log the reset with timestamp
        let timestamp = chrono::Utc::now();
        println!("[Storage] ‚ö†Ô∏è‚ö†Ô∏è‚ö†Ô∏è CHAIN HEIGHT RESET INITIATED ‚ö†Ô∏è‚ö†Ô∏è‚ö†Ô∏è");
        println!("[Storage]    Timestamp: {}", timestamp);
        println!("[Storage]    Requested by: QNET_FORCE_RESET + QNET_CONFIRM_RESET");
        
        let metadata_cf = self.db.cf_handle("metadata")
            .ok_or_else(|| IntegrationError::StorageError("metadata column family not found".to_string()))?;
        
        // Get current height before reset for logging
        let current_height = match self.get_chain_height() {
            Ok(h) => h,
            Err(_) => 0,
        };
        
        // Set height to 0
        let height_bytes = 0u64.to_be_bytes();
        self.db.put_cf(&metadata_cf, b"chain_height", height_bytes)?;
        
        println!("[Storage] ‚úÖ Chain height reset: {} -> 0", current_height);
        println!("[Storage] ‚ö†Ô∏è  Data loss: {} blocks deleted", current_height);
        Ok(())
    }
    
    pub fn get_block_hash(&self, height: u64) -> IntegrationResult<Option<String>> {
        let block_cf = self.db.cf_handle("blocks")
            .ok_or_else(|| IntegrationError::StorageError("blocks column family not found".to_string()))?;
        
        let hash_key = format!("hash_{}", height);
        match self.db.get_cf(&block_cf, hash_key.as_bytes())? {
            Some(data) => {
                let hash: [u8; 32] = bincode::deserialize(&data)
                    .map_err(|e| IntegrationError::SerializationError(e.to_string()))?;
                Ok(Some(hex::encode(hash)))
            }
            None => Ok(None),
        }
    }
    
    pub async fn load_block_by_height(&self, height: u64) -> IntegrationResult<Option<qnet_state::Block>> {
        let block_cf = self.db.cf_handle("blocks")
            .ok_or_else(|| IntegrationError::StorageError("blocks column family not found".to_string()))?;
        
        let block_key = format!("block_{}", height);
        match self.db.get_cf(&block_cf, block_key.as_bytes())? {
            Some(data) => {
                let block: qnet_state::Block = bincode::deserialize(&data)
                    .map_err(|e| IntegrationError::SerializationError(e.to_string()))?;
                Ok(Some(block))
            }
            None => Ok(None),
        }
    }
    
    pub async fn save_account(&self, account: &qnet_state::Account) -> IntegrationResult<()> {
        let accounts_cf = self.db.cf_handle("accounts")
            .ok_or_else(|| IntegrationError::StorageError("accounts column family not found".to_string()))?;
        
        let account_data = bincode::serialize(account)
            .map_err(|e| IntegrationError::SerializationError(e.to_string()))?;
        
        self.db.put_cf(&accounts_cf, account.address.as_bytes(), &account_data)?;
        Ok(())
    }
    
    pub fn save_microblock(&self, height: u64, data: &[u8]) -> IntegrationResult<()> {
        let microblocks_cf = self.db.cf_handle("microblocks")
            .ok_or_else(|| IntegrationError::StorageError("microblocks column family not found".to_string()))?;
        let metadata_cf = self.db.cf_handle("metadata")
            .ok_or_else(|| IntegrationError::StorageError("metadata column family not found".to_string()))?;
        
        let key = format!("microblock_{}", height);
        
        // Use batch write to update both microblock and chain height atomically
        let mut batch = WriteBatch::default();
        batch.put_cf(&microblocks_cf, key.as_bytes(), data);
        
        // CRITICAL FIX: Update chain height when saving microblock
        batch.put_cf(&metadata_cf, b"chain_height", &height.to_be_bytes());
        
        self.db.write(batch)?;
        Ok(())
    }
    
    /// PRODUCTION: Save activation code with AES-256-GCM encryption
    /// Key is derived from activation code and NEVER stored in database
    pub fn save_activation_code(&self, code: &str, node_type: u8, timestamp: u64) -> IntegrationResult<()> {
        let metadata_cf = self.db.cf_handle("metadata")
            .ok_or_else(|| IntegrationError::StorageError("metadata column family not found".to_string()))?;
        
        // Get device signature for migration tracking (NOT for encryption!)
        let device_signature = Self::get_device_signature_for_tracking();
        let server_ip = Self::get_server_ip();
        
        // SECURITY: Create activation data (includes code for self-validation)
        let activation_data = format!("{}:{}:{}:{}:{}", 
            code, node_type, timestamp, device_signature, server_ip);
        
        // PRODUCTION: Encrypt with AES-256-GCM (quantum-resistant)
        // Key is derived from activation code - NOT stored in database!
        let (encrypted_data, nonce) = Self::encrypt_with_aes_gcm(&activation_data, code)?;
        
        // Create storage record (nonce is public, encryption_key is NOT stored!)
        let storage_record = format!("{}:{}", 
            hex::encode(&nonce),  // Nonce (12 bytes, can be public)
            hex::encode(&encrypted_data)  // Encrypted data
        );
        
        self.db.put_cf(&metadata_cf, b"activation_code", storage_record.as_bytes())?;
        
        // CRITICAL: Do NOT save encryption key to database!
        // Key is derived from activation code when needed
        
        println!("[Storage] üîê Activation code encrypted with AES-256-GCM (key NOT stored)");
        Ok(())
    }
    
    /// PRODUCTION: Load activation code with AES-256-GCM decryption
    /// Key is derived from activation code (env var or Genesis BOOTSTRAP_ID)
    pub fn load_activation_code(&self) -> IntegrationResult<Option<(String, u8, u64)>> {
        let metadata_cf = self.db.cf_handle("metadata")
            .ok_or_else(|| IntegrationError::StorageError("metadata column family not found".to_string()))?;
        
        match self.db.get_cf(&metadata_cf, b"activation_code")? {
            Some(encrypted_data) => {
                let encrypted_str = String::from_utf8_lossy(&encrypted_data);
                
                // Check if this is NEW format (nonce:encrypted) or LEGACY format (has state_key)
                if encrypted_str.contains(':') && encrypted_str.split(':').count() == 2 {
                    // NEW FORMAT: AES-256-GCM encrypted
                    let parts: Vec<&str> = encrypted_str.split(':').collect();
                    let nonce_hex = parts[0];
                    let encrypted_hex = parts[1];
                    
                    // Get activation code for decryption key
                    let activation_code = Self::get_activation_code_for_decryption()?;
                    
                    // Parse nonce and encrypted data
                    let nonce_bytes = hex::decode(nonce_hex)
                        .map_err(|e| IntegrationError::SecurityError(format!("Invalid nonce: {}", e)))?;
                    let encrypted_bytes = hex::decode(encrypted_hex)
                        .map_err(|e| IntegrationError::SecurityError(format!("Invalid encrypted data: {}", e)))?;
                    
                    if nonce_bytes.len() != 12 {
                        return Err(IntegrationError::SecurityError("Invalid nonce length".to_string()));
                    }
                    
                    let mut nonce_array = [0u8; 12];
                    nonce_array.copy_from_slice(&nonce_bytes);
                    
                    // PRODUCTION: Decrypt with AES-256-GCM
                    let decrypted_data = Self::decrypt_with_aes_gcm(&encrypted_bytes, &nonce_array, &activation_code)?;
                    
                    let decrypted_parts: Vec<&str> = decrypted_data.split(':').collect();
                    
                    // AES-256 format: code:node_type:timestamp:device_signature:server_ip
                    if decrypted_parts.len() >= 5 {
                        let saved_code = decrypted_parts[0];
                        let node_type = decrypted_parts[1].parse::<u8>().unwrap_or(1);
                        let timestamp = decrypted_parts[2].parse::<u64>().unwrap_or(0);
                        let stored_device_signature = decrypted_parts[3];
                        let stored_server_ip = decrypted_parts[4];
                        
                        // SECURITY: Validate that decrypted code matches activation code used for decryption
                        if saved_code != activation_code {
                            return Err(IntegrationError::SecurityError(
                                "Decryption succeeded but activation code mismatch - wrong code provided".to_string()
                            ));
                        }
                        
                        // PRODUCTION: Log device migration if detected
                        let current_device = Self::get_device_signature_for_tracking();
                        if stored_device_signature != current_device {
                            println!("[Storage] üîÑ Device signature changed (migration or new hardware):");
                            println!("   Stored: {}...", &stored_device_signature[..8.min(stored_device_signature.len())]);
                            println!("   Current: {}...", &current_device[..8.min(current_device.len())]);
                        }
                        
                        // Log IP changes (normal for migrations)
                        let current_server_ip = Self::get_server_ip();
                        if current_server_ip != stored_server_ip {
                            println!("[Storage] üìç Server IP changed: {} ‚Üí {} (migration/restart)", 
                                     stored_server_ip, current_server_ip);
                        }
                        
                        println!("[Storage] ‚úÖ Activation code loaded and validated (AES-256-GCM)");
                        return Ok(Some((saved_code.to_string(), node_type, timestamp)));
                    } else {
                        return Err(IntegrationError::SecurityError("Invalid AES-256 activation format".to_string()));
                    }
                } else {
                    // LEGACY FORMAT: Check for old XOR encryption with state_key
                    println!("[Storage] üîÑ Detected legacy activation format - attempting migration");
                    
                    match self.db.get_cf(&metadata_cf, b"state_key")? {
                        Some(_) => {
                            // Legacy XOR format exists - load and re-save with AES-256
                            return self.load_legacy_activation_code(&encrypted_data);
                        }
                        None => {
                            return Err(IntegrationError::SecurityError(
                                "Unknown activation code format".to_string()
                            ));
                        }
                    }
                }
            }
            None => Ok(None),
        }
    }
    
    /// Load legacy activation code format for backwards compatibility
    fn load_legacy_activation_code(&self, data: &[u8]) -> IntegrationResult<Option<(String, u8, u64)>> {
        let activation_str = String::from_utf8_lossy(data);
        let parts: Vec<&str> = activation_str.split(':').collect();
        
        if parts.len() == 3 {
            println!("‚ö†Ô∏è  WARNING: Using legacy activation format (upgrading to secure format recommended)");
            let code = parts[0].to_string();
            let node_type = parts[1].parse::<u8>().unwrap_or(1);
            let timestamp = parts[2].parse::<u64>().unwrap_or(0);
            Ok(Some((code, node_type, timestamp)))
        } else {
            Ok(None)
        }
    }
    
    /// Clear activation code (for security)
    pub fn clear_activation_code(&self) -> IntegrationResult<()> {
        let metadata_cf = self.db.cf_handle("metadata")
            .ok_or_else(|| IntegrationError::StorageError("metadata column family not found".to_string()))?;
        
        self.db.delete_cf(&metadata_cf, b"activation_code")?;
        self.db.delete_cf(&metadata_cf, b"state_key")?;
        self.db.delete_cf(&metadata_cf, b"activation_burn_tx")?;
        Ok(())
    }
    
    /// Get burn transaction hash for activation code (for XOR decryption)
    pub fn get_activation_burn_tx(&self) -> IntegrationResult<String> {
        let metadata_cf = self.db.cf_handle("metadata")
            .ok_or_else(|| IntegrationError::StorageError("metadata column family not found".to_string()))?;
        
        match self.db.get_cf(&metadata_cf, b"activation_burn_tx")? {
            Some(data) => {
                let burn_tx = String::from_utf8_lossy(&data).to_string();
                Ok(burn_tx)
            }
            None => {
                // No burn_tx stored - return empty (Genesis nodes or legacy activations)
                Err(IntegrationError::StorageError("No burn_tx stored for activation".to_string()))
            }
        }
    }
    
    /// Save burn transaction hash for activation code
    pub fn save_activation_burn_tx(&self, burn_tx: &str) -> IntegrationResult<()> {
        let metadata_cf = self.db.cf_handle("metadata")
            .ok_or_else(|| IntegrationError::StorageError("metadata column family not found".to_string()))?;
        
        self.db.put_cf(&metadata_cf, b"activation_burn_tx", burn_tx.as_bytes())?;
        println!("[Storage] üîó Burn TX saved for activation: {}...", &burn_tx[..8.min(burn_tx.len())]);
        Ok(())
    }
    
    /// Update activation code for device migration (preserves activation, updates device)
    pub fn update_activation_for_migration(&self, code: &str, node_type: u8, timestamp: u64, new_device_signature: &str) -> IntegrationResult<()> {
        let metadata_cf = self.db.cf_handle("metadata")
            .ok_or_else(|| IntegrationError::StorageError("metadata column family not found".to_string()))?;
        
        // Generate new node identity with migration indicator
        let migration_identity = Self::generate_migration_identity(code, node_type, timestamp, new_device_signature)?;
        let server_ip = Self::get_server_ip();
        
        // Create new state key for migrated device
        let state_key = Self::derive_state_key(code, &migration_identity)?;
        
        // PRODUCTION: Save with AES-256-GCM (same as save_activation_code)
        let activation_data = format!("{}:{}:{}:{}:{}", 
            code, node_type, timestamp, new_device_signature, server_ip);
        
        // Encrypt with AES-256-GCM (key from activation code, NOT stored!)
        let (encrypted_data, nonce) = Self::encrypt_with_aes_gcm(&activation_data, code)?;
        
        let storage_record = format!("{}:{}", 
            hex::encode(&nonce),
            hex::encode(&encrypted_data)
        );
        
        self.db.put_cf(&metadata_cf, b"activation_code", storage_record.as_bytes())?;
        
        // CRITICAL: Do NOT save encryption key - it's derived from activation code!
        
        println!("[Storage] ‚úÖ Activation migrated to device: {} (AES-256-GCM)", &new_device_signature[..16.min(new_device_signature.len())]);
        Ok(())
    }
    
    /// Generate migration identity for device changes
    fn generate_migration_identity(code: &str, node_type: u8, timestamp: u64, new_device_signature: &str) -> IntegrationResult<String> {
        use sha3::{Sha3_256, Digest};
        
        // Identity components for migrated device
        let mut identity_components = Vec::new();
        
        // Core: activation code + migration info
        identity_components.push(code.to_string());
        identity_components.push(format!("node_type:{}", node_type));
        identity_components.push(format!("timestamp:{}", timestamp));
        identity_components.push(format!("device_signature:{}", new_device_signature));
        
        // Add migration marker
        identity_components.push("migration_enabled".to_string());
        
        // Generate deterministic identity from transfer data
        let combined = identity_components.join("|");
        let identity_hash = hex::encode(Sha3_256::digest(combined.as_bytes()));
        
        // Use first 16 characters for transfer identity
        Ok(identity_hash[..16].to_string())
    }
    
    /// Generate cryptographic node identity from activation code (universal device support)
    fn generate_node_identity(code: &str, node_type: u8, timestamp: u64) -> IntegrationResult<String> {
        use sha3::{Sha3_256, Digest};
        
        // GENESIS PERIOD FIX: Simplified identity for bootstrap phase
        let is_genesis_bootstrap = std::env::var("QNET_BOOTSTRAP_ID").is_ok() || 
                                  std::env::var("QNET_GENESIS_BOOTSTRAP").unwrap_or_default() == "1";
        
        // Primary components: activation code + node config
        let mut identity_components = Vec::new();
        
        // Core: activation code itself (unique and immutable)
        identity_components.push(code.to_string());
        
        // Node configuration (stable across device migrations)
        identity_components.push(format!("node_type:{}", node_type));
        identity_components.push(format!("timestamp:{}", timestamp));
        
        if is_genesis_bootstrap {
            // PRODUCTION: STABLE Genesis identity - only immutable components
            // This ensures Genesis nodes have consistent identity across Docker restarts
            let bootstrap_id = std::env::var("QNET_BOOTSTRAP_ID").unwrap_or_else(|_| "001".to_string());
            
            // Use only stable, immutable components for Genesis identity
            identity_components.push(format!("genesis_bootstrap_id:{}", bootstrap_id));
            identity_components.push(format!("network:qnet_mainnet"));
            identity_components.push(format!("genesis_version:v1.0"));
            
            // Deterministic hash from activation code only
            let primary_hash = hex::encode(Sha3_256::digest(code.as_bytes()));
            identity_components.push(format!("stable_code_hash:{}", &primary_hash[..16]));
            
            println!("[IDENTITY] üîê Genesis stable identity components: activation_code + bootstrap_id");
        } else {
            // PRODUCTION: Full identity with system info (after bootstrap)
            identity_components.push(format!("user:{}", 
                std::env::var("USER").unwrap_or_else(|_| "qnet".to_string())
            ));
            
            // Add hostname (may change but helps with uniqueness)
            if let Ok(hostname) = std::env::var("HOSTNAME") {
                identity_components.push(format!("hostname:{}", hostname));
            }
            
            // Universal device support: use activation code as primary entropy source
            let primary_hash = hex::encode(Sha3_256::digest(code.as_bytes()));
            identity_components.push(format!("code_hash:{}", &primary_hash[..16]));
        }
        
        // Generate deterministic identity from activation code
        let combined = identity_components.join("|");
        let identity_hash = hex::encode(Sha3_256::digest(combined.as_bytes()));
        
        // Use first 16 characters for node identity
        Ok(identity_hash[..16].to_string())
    }
    
    /// Get server IP address
    fn get_server_ip() -> String {
        use std::process::Command;
        
        // Try to get public IP
        if let Ok(output) = Command::new("curl")
            .arg("-s")
            .arg("--max-time")
            .arg("2")
            .arg("https://api.ipify.org")
            .output() {
            if let Ok(ip) = String::from_utf8(output.stdout) {
                if !ip.trim().is_empty() {
                    return ip.trim().to_string();
                }
            }
        }
        
        // Fallback to local IP
        if let Ok(output) = Command::new("hostname").arg("-I").output() {
            if let Ok(ip) = String::from_utf8(output.stdout) {
                if let Some(first_ip) = ip.split_whitespace().next() {
                    return first_ip.to_string();
                }
            }
        }
        
        "unknown".to_string()
    }
    
    /// Derive state key from activation code and node identity
    fn derive_state_key(code: &str, node_identity: &str) -> IntegrationResult<String> {
        use sha3::{Sha3_256, Digest};
        
        // Create deterministic key from activation code
        let key_material = format!("{}:{}:state_key", code, node_identity);
        let key_hash = hex::encode(Sha3_256::digest(key_material.as_bytes()));
        
        // Use first 32 characters as state key
        Ok(key_hash[..32].to_string())
    }
    
    /// PRODUCTION: Get activation code for decryption from environment or generate for Genesis
    fn get_activation_code_for_decryption() -> IntegrationResult<String> {
        // Priority 1: Check QNET_ACTIVATION_CODE environment variable
        if let Ok(code) = std::env::var("QNET_ACTIVATION_CODE") {
            if !code.is_empty() {
                return Ok(code);
            }
        }
        
        // Priority 2: Generate for Genesis nodes from BOOTSTRAP_ID
        if let Ok(bootstrap_id) = std::env::var("QNET_BOOTSTRAP_ID") {
            match bootstrap_id.as_str() {
                "001" | "002" | "003" | "004" | "005" => {
                    let genesis_code = format!("QNET-BOOT-{:0>4}-STRAP", bootstrap_id);
                    return Ok(genesis_code);
                }
                _ => {}
            }
        }
        
        // No activation code available
        Err(IntegrationError::ValidationError(
            "No activation code available for decryption. Set QNET_ACTIVATION_CODE env var or QNET_BOOTSTRAP_ID for Genesis nodes".to_string()
        ))
    }
    
    /// PRODUCTION: Get device signature for tracking (NOT for encryption!)
    fn get_device_signature_for_tracking() -> String {
        use sha3::{Sha3_256, Digest};
        
        let mut hasher = Sha3_256::new();
        
        // Hardware fingerprint for tracking
        if let Ok(hostname) = std::env::var("HOSTNAME") {
            hasher.update(hostname.as_bytes());
        }
        if let Ok(user) = std::env::var("USER") {
            hasher.update(user.as_bytes());
        }
        
        // Add timestamp component for Docker containers (they have random hostnames)
        let is_docker = std::env::var("DOCKER_ENV").is_ok();
        if is_docker {
            // For Docker: use container ID if available
            if let Ok(container_id) = std::env::var("HOSTNAME") {
                if container_id.len() == 12 {
                    hasher.update(b"docker_container:");
                    hasher.update(container_id.as_bytes());
                }
            }
        }
        
        format!("device_{}", hex::encode(&hasher.finalize()[..16]))
    }
    
    /// PRODUCTION: Derive AES-256 encryption key from activation code (for database security)
    /// Key is NEVER stored - computed from activation code each time
    fn derive_encryption_key_from_code(code: &str) -> [u8; 32] {
        use sha3::{Sha3_256, Digest};
        
        let mut hasher = Sha3_256::new();
        hasher.update(code.as_bytes());
        hasher.update(b"QNET_DB_ENCRYPTION_V1");  // Salt for database encryption
        
        let hash = hasher.finalize();
        hash.into()
    }
    
    /// PRODUCTION: Encrypt data with AES-256-GCM (quantum-resistant symmetric encryption)
    /// Uses existing aes-gcm dependency from quantum_crypto module
    fn encrypt_with_aes_gcm(data: &str, activation_code: &str) -> IntegrationResult<(Vec<u8>, [u8; 12])> {
        use aes_gcm::{Aes256Gcm, KeyInit, Nonce};
        use aes_gcm::aead::Aead;
        use rand::Rng;
        
        // Derive encryption key from activation code
        let key_bytes = Self::derive_encryption_key_from_code(activation_code);
        let cipher = Aes256Gcm::new(&key_bytes.into());
        
        // Generate random nonce (12 bytes for GCM)
        let mut nonce_bytes = [0u8; 12];
        rand::thread_rng().fill(&mut nonce_bytes);
        let nonce = Nonce::from_slice(&nonce_bytes);
        
        // Encrypt with authenticated encryption (AEAD)
        let encrypted = cipher.encrypt(nonce, data.as_bytes())
            .map_err(|e| IntegrationError::SecurityError(format!("AES-GCM encryption failed: {}", e)))?;
        
        Ok((encrypted, nonce_bytes))
    }
    
    /// PRODUCTION: Decrypt data with AES-256-GCM
    fn decrypt_with_aes_gcm(encrypted_data: &[u8], nonce: &[u8; 12], activation_code: &str) -> IntegrationResult<String> {
        use aes_gcm::{Aes256Gcm, KeyInit, Nonce};
        use aes_gcm::aead::Aead;
        
        // Derive encryption key from activation code (same as encryption)
        let key_bytes = Self::derive_encryption_key_from_code(activation_code);
        let cipher = Aes256Gcm::new(&key_bytes.into());
        
        let nonce_ref = Nonce::from_slice(nonce);
        
        // Decrypt and verify authentication tag
        let decrypted = cipher.decrypt(nonce_ref, encrypted_data)
            .map_err(|e| IntegrationError::SecurityError(format!("AES-GCM decryption failed: {}", e)))?;
        
        String::from_utf8(decrypted)
            .map_err(|e| IntegrationError::SecurityError(format!("UTF-8 decoding failed: {}", e)))
    }
    
    pub fn load_microblock(&self, height: u64) -> IntegrationResult<Option<Vec<u8>>> {
        let microblocks_cf = self.db.cf_handle("microblocks")
            .ok_or_else(|| IntegrationError::StorageError("microblocks column family not found".to_string()))?;
        
        let key = format!("microblock_{}", height);
        match self.db.get_cf(&microblocks_cf, key.as_bytes())? {
            Some(data) => Ok(Some(data)),
            None => Ok(None),
        }
    }
    
    /// Delete a microblock at the specified height (for fork resolution)
    pub fn delete_microblock(&self, height: u64) -> IntegrationResult<()> {
        let microblocks_cf = self.db.cf_handle("microblocks")
            .ok_or_else(|| IntegrationError::StorageError("microblocks column family not found".to_string()))?;
        
        let key = format!("microblock_{}", height);
        self.db.delete_cf(&microblocks_cf, key.as_bytes())?;
        
        Ok(())
    }
    
    // ========================================================================
    // POH STATE STORAGE (v2.19.13)
    // ========================================================================
    // Separate PoH state storage for fast validation without loading full blocks
    // This is critical for scalability - PoH validation should be O(1) not O(block_size)
    // ========================================================================
    
    /// Save PoH state for a block height
    /// Called automatically when saving microblocks
    pub fn save_poh_state(&self, poh_state: &qnet_state::PoHState) -> IntegrationResult<()> {
        let poh_cf = self.db.cf_handle("poh_state")
            .ok_or_else(|| IntegrationError::StorageError("poh_state column family not found".to_string()))?;
        
        let key = format!("poh_{}", poh_state.height);
        let data = bincode::serialize(poh_state)
            .map_err(|e| IntegrationError::SerializationError(e.to_string()))?;
        
        self.db.put_cf(&poh_cf, key.as_bytes(), &data)?;
        Ok(())
    }
    
    /// Load PoH state for a block height
    /// Returns None if height doesn't exist or PoH data not available
    pub fn load_poh_state(&self, height: u64) -> IntegrationResult<Option<qnet_state::PoHState>> {
        let poh_cf = self.db.cf_handle("poh_state")
            .ok_or_else(|| IntegrationError::StorageError("poh_state column family not found".to_string()))?;
        
        let key = format!("poh_{}", height);
        match self.db.get_cf(&poh_cf, key.as_bytes())? {
            Some(data) => {
                let poh_state = bincode::deserialize::<qnet_state::PoHState>(&data)
                    .map_err(|e| IntegrationError::SerializationError(e.to_string()))?;
                Ok(Some(poh_state))
            }
            None => Ok(None),
        }
    }
    
    /// Delete PoH state for a block height (for fork resolution)
    pub fn delete_poh_state(&self, height: u64) -> IntegrationResult<()> {
        let poh_cf = self.db.cf_handle("poh_state")
            .ok_or_else(|| IntegrationError::StorageError("poh_state column family not found".to_string()))?;
        
        let key = format!("poh_{}", height);
        self.db.delete_cf(&poh_cf, key.as_bytes())?;
        Ok(())
    }
    
    /// Get the latest PoH state (for continuing PoH sequence)
    pub fn get_latest_poh_state(&self) -> IntegrationResult<Option<qnet_state::PoHState>> {
        let chain_height = self.get_chain_height()?;
        if chain_height == 0 {
            return Ok(None);
        }
        self.load_poh_state(chain_height)
    }
    
    pub fn get_latest_macroblock_hash(&self) -> Result<[u8; 32], IntegrationError> {
        let metadata_cf = self.db.cf_handle("metadata")
            .ok_or_else(|| IntegrationError::StorageError("metadata column family not found".to_string()))?;
        
        match self.db.get_cf(&metadata_cf, b"latest_macroblock_hash")? {
            Some(data) if data.len() >= 32 => {
                let mut hash = [0u8; 32];
                hash.copy_from_slice(&data[..32]);
                Ok(hash)
            },
            _ => Ok([0u8; 32]), // Default genesis hash
        }
    }
    
    pub async fn save_macroblock(&self, height: u64, macroblock: &qnet_state::MacroBlock) -> IntegrationResult<()> {
        let microblocks_cf = self.db.cf_handle("microblocks")
            .ok_or_else(|| IntegrationError::StorageError("microblocks column family not found".to_string()))?;
        let metadata_cf = self.db.cf_handle("metadata")
            .ok_or_else(|| IntegrationError::StorageError("metadata column family not found".to_string()))?;
        
        let key = format!("macroblock_{}", height);
        let data = bincode::serialize(macroblock)
            .map_err(|e| IntegrationError::SerializationError(e.to_string()))?;
        
        let mut batch = WriteBatch::default();
        batch.put_cf(&microblocks_cf, key.as_bytes(), &data);
        
        // Update latest macroblock hash
        let hash = macroblock.hash();
        batch.put_cf(&metadata_cf, b"latest_macroblock_hash", &hash);
        
        self.db.write(batch)?;
        Ok(())
    }
    
    /// Get macroblock by its index (height / 90)
    pub fn get_macroblock_by_height(&self, macroblock_index: u64) -> IntegrationResult<Option<Vec<u8>>> {
        let microblocks_cf = self.db.cf_handle("microblocks")
            .ok_or_else(|| IntegrationError::StorageError("microblocks column family not found".to_string()))?;
        
        // CRITICAL FIX: Macroblocks are stored with key "macroblock_{index}"
        // where index is the macroblock number (1 for blocks 1-90, 2 for blocks 91-180, etc)
        // NOT the block height! This matches save_macroblock which uses round_number
        let key = format!("macroblock_{}", macroblock_index);
        
        match self.db.get_cf(&microblocks_cf, key.as_bytes())? {
            Some(data) => Ok(Some(data)),
            None => Ok(None),
        }
    }
    
    pub fn get_stats(&self) -> IntegrationResult<StorageStats> {
        let mut stats = StorageStats::default();
        
        // Get chain height
        stats.latest_height = self.get_chain_height()?;
        
        // Count blocks
        let block_cf = self.db.cf_handle("blocks")
            .ok_or_else(|| IntegrationError::StorageError("blocks column family not found".to_string()))?;
        let mut block_count = 0u64;
        let iter = self.db.iterator_cf(&block_cf, rocksdb::IteratorMode::Start);
        for item in iter {
            let (key, _) = item?;
            if std::str::from_utf8(&key).unwrap_or("").starts_with("block_") {
                block_count += 1;
            }
        }
        stats.total_blocks = block_count;
        
        // Count transactions  
        let tx_cf = self.db.cf_handle("transactions")
            .ok_or_else(|| IntegrationError::StorageError("transactions column family not found".to_string()))?;
        let mut tx_count = 0u64;
        let iter = self.db.iterator_cf(&tx_cf, rocksdb::IteratorMode::Start);
        for item in iter {
            let (key, _) = item?;
            if std::str::from_utf8(&key).unwrap_or("").starts_with("tx_") {
                tx_count += 1;
            }
        }
        stats.total_transactions = tx_count;
        
        // Count accounts
        let accounts_cf = self.db.cf_handle("accounts")
            .ok_or_else(|| IntegrationError::StorageError("accounts column family not found".to_string()))?;
        let mut account_count = 0u64;
        let iter = self.db.iterator_cf(&accounts_cf, rocksdb::IteratorMode::Start);
        for _item in iter {
            account_count += 1;
        }
        stats.total_accounts = account_count;
        
        Ok(stats)
    }

    /// Save consensus round state for recovery after restart
    pub fn save_consensus_state(&self, round: u64, state: &[u8]) -> IntegrationResult<()> {
        let consensus_cf = self.db.cf_handle("consensus")
            .ok_or_else(|| IntegrationError::StorageError("consensus column family not found".to_string()))?;
        
        let key = format!("round_{}", round);
        self.db.put_cf(&consensus_cf, key.as_bytes(), state)?;
        
        // Update latest round for quick lookup
        self.db.put_cf(&consensus_cf, b"latest_round", &round.to_be_bytes())?;
        
        Ok(())
    }
    
    /// Load consensus round state for recovery
    pub fn load_consensus_state(&self, round: u64) -> IntegrationResult<Option<Vec<u8>>> {
        let consensus_cf = self.db.cf_handle("consensus")
            .ok_or_else(|| IntegrationError::StorageError("consensus column family not found".to_string()))?;
        
        let key = format!("round_{}", round);
        Ok(self.db.get_cf(&consensus_cf, key.as_bytes())?)
    }
    
    /// Get latest consensus round from storage
    pub fn get_latest_consensus_round(&self) -> IntegrationResult<u64> {
        let consensus_cf = self.db.cf_handle("consensus")
            .ok_or_else(|| IntegrationError::StorageError("consensus column family not found".to_string()))?;
        
        match self.db.get_cf(&consensus_cf, b"latest_round")? {
            Some(bytes) => {
                let round = u64::from_be_bytes(bytes.try_into()
                    .map_err(|_| IntegrationError::StorageError("Invalid round data".to_string()))?);
                Ok(round)
            },
            None => Ok(0), // No consensus state saved yet
        }
    }
    
    /// Save sync progress for resuming after restart
    pub fn save_sync_progress(&self, from_height: u64, to_height: u64, current: u64) -> IntegrationResult<()> {
        let sync_cf = self.db.cf_handle("sync_state")
            .ok_or_else(|| IntegrationError::StorageError("sync_state column family not found".to_string()))?;
        
        let data = bincode::serialize(&(from_height, to_height, current))
            .map_err(|e| IntegrationError::SerializationError(e.to_string()))?;
        
        self.db.put_cf(&sync_cf, b"sync_progress", &data)?;
        Ok(())
    }
    
    /// Load sync progress for resuming
    pub fn load_sync_progress(&self) -> IntegrationResult<Option<(u64, u64, u64)>> {
        let sync_cf = self.db.cf_handle("sync_state")
            .ok_or_else(|| IntegrationError::StorageError("sync_state column family not found".to_string()))?;
        
        match self.db.get_cf(&sync_cf, b"sync_progress")? {
            Some(data) => {
                let progress = bincode::deserialize(&data)
                    .map_err(|e| IntegrationError::DeserializationError(e.to_string()))?;
                Ok(Some(progress))
            },
            None => Ok(None),
        }
    }
    
    /// Clear sync progress after completion
    pub fn clear_sync_progress(&self) -> IntegrationResult<()> {
        let sync_cf = self.db.cf_handle("sync_state")
            .ok_or_else(|| IntegrationError::StorageError("sync_state column family not found".to_string()))?;
        
        self.db.delete_cf(&sync_cf, b"sync_progress")?;
        Ok(())
    }
    
    /// Get microblock range for batch sync (raw format)
    /// NOTE: Use Storage::get_microblocks_range for network sync (it converts to full MicroBlock)
    pub async fn get_microblocks_range(&self, from: u64, to: u64) -> IntegrationResult<Vec<(u64, Vec<u8>)>> {
        let mut microblocks = Vec::new();
        
        for height in from..=to {
            if let Some(data) = self.load_microblock(height)? {
                microblocks.push((height, data));
            }
        }
        
        Ok(microblocks)
    }
    
    /// Legacy: Get block range for old Block format (only genesis)  
    pub async fn get_blocks_range(&self, from: u64, to: u64) -> IntegrationResult<Vec<qnet_state::Block>> {
        let mut blocks = Vec::new();
        
        for height in from..=to {
            if let Some(block) = self.load_block_by_height(height).await? {
                blocks.push(block);
            }
        }
        
        Ok(blocks)
    }

    /// Find transaction by hash in blockchain storage
    pub async fn find_transaction_by_hash(&self, tx_hash: &str) -> IntegrationResult<Option<qnet_state::Transaction>> {
        // PRODUCTION: Search for transaction in blockchain storage
        let tx_cf = self.db.cf_handle("transactions")
            .ok_or_else(|| IntegrationError::StorageError("transactions column family not found".to_string()))?;
        
        let tx_key = format!("tx_{}", tx_hash);
        match self.db.get_cf(&tx_cf, tx_key.as_bytes())? {
            Some(data) => {
                // SIMPLIFIED (v2.19.10): Only Zstd compression used (lossless)
                // Pattern Recognition was removed because it was LOSSY
                
                // Strategy 1: Zstd-compressed (check magic number 0x28B52FFD)
                if data.len() >= 4 && data[0..4] == [0x28, 0xb5, 0x2f, 0xfd] {
                    let decompressed = zstd::decode_all(&data[..])
                        .map_err(|e| IntegrationError::Other(format!("Zstd decompression error: {}", e)))?;
                    let transaction: qnet_state::Transaction = bincode::deserialize(&decompressed)
                        .map_err(|e| IntegrationError::SerializationError(e.to_string()))?;
                    return Ok(Some(transaction));
                }
                
                // Strategy 2: Uncompressed raw transaction (legacy data)
                let transaction: qnet_state::Transaction = bincode::deserialize(&data)
                    .map_err(|e| IntegrationError::SerializationError(e.to_string()))?;
                Ok(Some(transaction))
            },
            None => {
                // Transaction not found in persistent storage
                Ok(None)
            }
        }
    }

    /// Get transaction block height from blockchain - O(1) with index
    pub async fn get_transaction_block_height(&self, tx_hash: &str) -> IntegrationResult<u64> {
        // OPTIMIZED: Use tx_index for O(1) lookup instead of O(n) iteration
        let tx_index_cf = self.db.cf_handle("tx_index")
            .ok_or_else(|| IntegrationError::StorageError("tx_index column family not found".to_string()))?;
        
        let tx_key = format!("tx_{}", tx_hash);
        match self.db.get_cf(&tx_index_cf, tx_key.as_bytes())? {
            Some(data) => {
                if data.len() >= 8 {
                    let height_bytes: [u8; 8] = data[0..8].try_into()
                        .map_err(|_| IntegrationError::StorageError("Invalid height data".to_string()))?;
                    Ok(u64::from_be_bytes(height_bytes))
                } else {
                    Err(IntegrationError::StorageError(format!("Invalid index data for transaction {}", tx_hash)))
                }
            },
            None => {
                // Fallback: Check microblocks for legacy data (will be removed in future)
        let microblocks_cf = self.db.cf_handle("microblocks")
            .ok_or_else(|| IntegrationError::StorageError("microblocks column family not found".to_string()))?;
        
        let iter = self.db.iterator_cf(&microblocks_cf, rocksdb::IteratorMode::Start);
        for item in iter {
            let (key, data) = item.map_err(|e| IntegrationError::StorageError(e.to_string()))?;
            let key_str = std::str::from_utf8(&key).unwrap_or("");
            
            if key_str.starts_with("microblock_") {
                // Try both legacy and efficient formats
                if let Ok(legacy_block) = bincode::deserialize::<qnet_state::MicroBlock>(&data) {
                    for tx in &legacy_block.transactions {
                        if tx.hash == tx_hash {
                            return Ok(legacy_block.height);
                        }
                    }
                } else if let Ok(efficient_block) = bincode::deserialize::<qnet_state::EfficientMicroBlock>(&data) {
                    // For efficient blocks, we need to check transaction pool
                    if let Ok(hash_bytes) = hex::decode(tx_hash) {
                        if hash_bytes.len() == 32 {
                            let mut hash_array = [0u8; 32];
                            hash_array.copy_from_slice(&hash_bytes);
                            
                            if efficient_block.transaction_hashes.contains(&hash_array) {
                                return Ok(efficient_block.height);
                            }
                        }
                    }
                }
            }
        }
        
                // Transaction not found
                Err(IntegrationError::StorageError(format!("Transaction {} not found in blockchain", tx_hash)))
            }
        }
    }
    
    /// Get transactions for an address (paginated, most recent first)
    pub async fn get_transactions_by_address(&self, address: &str, page: usize, per_page: usize) -> IntegrationResult<Vec<qnet_state::Transaction>> {
        let tx_by_addr_cf = self.db.cf_handle("tx_by_address")
            .ok_or_else(|| IntegrationError::StorageError("tx_by_address column family not found".to_string()))?;
        let tx_cf = self.db.cf_handle("transactions")
            .ok_or_else(|| IntegrationError::StorageError("transactions column family not found".to_string()))?;
        
        let prefix = format!("addr_{}_", address);
        
        // Iterate in reverse to get most recent first (keys are sorted by timestamp)
        let iter = self.db.iterator_cf(
            &tx_by_addr_cf,
            rocksdb::IteratorMode::From(
                format!("{}~", prefix).as_bytes(), // ~ is after hex digits in ASCII
                rocksdb::Direction::Reverse
            )
        );
        
        let mut transactions = Vec::new();
        let skip = page * per_page;
        let mut count = 0;
        let mut seen_hashes = std::collections::HashSet::new();
        
        for item in iter {
            let (key, value) = item?;
            let key_str = std::str::from_utf8(&key).unwrap_or("");
            
            if !key_str.starts_with(&prefix) {
                break;
            }
            
            // Get tx_hash from value
            let tx_hash = std::str::from_utf8(&value).unwrap_or("");
            
            // Deduplicate (same tx may appear twice if from==to)
            if seen_hashes.contains(tx_hash) {
                continue;
            }
            seen_hashes.insert(tx_hash.to_string());
            
            count += 1;
            if count <= skip {
                continue;
            }
            
            // Fetch full transaction (with Zstd decompression if needed)
            let tx_key = format!("tx_{}", tx_hash);
            if let Some(tx_data) = self.db.get_cf(&tx_cf, tx_key.as_bytes())? {
                // PRODUCTION: Decompress if Zstd compressed
                let decompressed = if tx_data.len() >= 4 && tx_data[0..4] == [0x28, 0xb5, 0x2f, 0xfd] {
                    zstd::decode_all(&tx_data[..]).unwrap_or_else(|_| tx_data.to_vec())
                } else {
                    tx_data.to_vec()
                };
                
                if let Ok(tx) = bincode::deserialize::<qnet_state::Transaction>(&decompressed) {
                    transactions.push(tx);
                    if transactions.len() >= per_page {
                        break;
                    }
                }
            }
        }
        
        Ok(transactions)
    }
    
    /// Count transactions for an address
    pub async fn count_transactions_by_address(&self, address: &str) -> IntegrationResult<usize> {
        let tx_by_addr_cf = self.db.cf_handle("tx_by_address")
            .ok_or_else(|| IntegrationError::StorageError("tx_by_address column family not found".to_string()))?;
        
        let prefix = format!("addr_{}_", address);
        let iter = self.db.iterator_cf(&tx_by_addr_cf, rocksdb::IteratorMode::From(prefix.as_bytes(), rocksdb::Direction::Forward));
        
        let mut count = 0;
        let mut seen_hashes = std::collections::HashSet::new();
        
        for item in iter {
            let (key, value) = item?;
            let key_str = std::str::from_utf8(&key).unwrap_or("");
            
            if !key_str.starts_with(&prefix) {
                break;
            }
            
            let tx_hash = std::str::from_utf8(&value).unwrap_or("");
            if !seen_hashes.contains(tx_hash) {
                seen_hashes.insert(tx_hash.to_string());
                count += 1;
            }
        }
        
        Ok(count)
    }
}

/// Storage modes for different node types
#[derive(Debug, Clone, Copy, PartialEq)]
pub enum StorageMode {
    /// Light node - headers only, no full blocks
    Light,
    /// Full node - sliding window of recent blocks + snapshots
    Full,
    /// Super node - keeps complete blockchain history + sharding support
    Super,
}

/// Adaptive compression levels based on block age
#[derive(Debug, Clone, Copy, PartialEq)]
pub enum CompressionLevel {
    /// No compression for hot data (< 1 day)
    None,
    /// Light compression for recent data (1-7 days)
    Light,     // Zstd level 3
    /// Medium compression for month-old data (8-30 days) 
    Medium,    // Zstd level 9
    /// Heavy compression for year-old data (31-365 days)
    Heavy,     // Zstd level 15
    /// Extreme compression for ancient data (> 365 days)
    Extreme,   // Zstd level 22
}

// NOTE: Delta Encoding was evaluated but removed in v2.19.10
// Reason: Pattern Recognition + Zstd provides better compression without complexity
// - Pattern Recognition: 89% reduction for simple transfers (140 ‚Üí 16 bytes)
// - Zstd adaptive: 30-80% additional compression based on block age
// - EfficientMicroBlock: stores only TX hashes, full TX stored separately
// Delta encoding would add complexity without significant benefit

/// Transaction pattern for optimized storage
#[derive(Debug, Clone, Copy, PartialEq, Eq, Hash, Serialize, Deserialize)]
pub enum TransactionPattern {
    /// Simple transfer (90% of transactions)
    SimpleTransfer,
    /// Node activation (5% of transactions)
    NodeActivation,
    /// Reward distribution (3% of transactions)
    RewardDistribution,
    /// Contract deployment (1% of transactions)
    ContractDeploy,
    /// Contract call (0.9% of transactions)
    ContractCall,
    /// Create account (0.1% of transactions)
    CreateAccount,
    /// Unknown pattern
    Unknown,
}

/// Compressed transaction using pattern recognition
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct CompressedTransaction {
    /// Pattern type
    pub pattern: TransactionPattern,
    /// Compressed data based on pattern
    pub data: Vec<u8>,
    /// Original size before compression
    pub original_size: usize,
}

/// Pattern-based transaction compressor
pub struct PatternRecognizer {
    /// Statistics for pattern recognition
    pattern_stats: HashMap<TransactionPattern, u64>,
}

pub struct Storage {
    persistent: PersistentStorage,
    /// Transaction pool for efficient storage without duplication
    pub transaction_pool: TransactionPool,
    /// Maximum storage size per node in bytes (300 GB default)
    max_storage_size: u64,
    /// Current storage usage in bytes
    current_storage_usage: Arc<RwLock<u64>>,
    /// Emergency cleanup enabled
    emergency_cleanup_enabled: bool,
    /// Node storage mode configuration
    storage_mode: StorageMode,
    /// Sliding window size for pruning (blocks to keep)
    sliding_window_size: u64,
    /// Pattern recognizer for transaction compression
    pattern_recognizer: Arc<RwLock<PatternRecognizer>>,
    /// Tiered storage configuration (Light/Full/Super)
    tier_config: StorageTierConfig,
    /// Graceful degradation manager
    graceful_degradation: Arc<RwLock<GracefulDegradation>>,
    /// Light node header rotation (for Light mode only)
    light_rotation: Arc<RwLock<LightNodeRotation>>,
}

// ============================================================================
// TIERED STORAGE IMPLEMENTATION
// ============================================================================
// ALL nodes receive ALL blocks. Storage differs by:
// - Light: Headers only (~100MB)
// - Full: Full blocks + pruning (~500GB, 30 days)
// - Super/Bootstrap: Full blocks, NO pruning (~2TB, full history)
// ============================================================================

/// Statistics for tiered storage
#[derive(Debug, Clone)]
pub struct TieredStorageStats {
    pub node_type: String,
    pub max_storage_bytes: u64,
    pub pruning_window_blocks: u64,
    pub current_storage_bytes: u64,
    pub blocks_stored: u64,
    pub transactions_stored: u64,
}

impl Storage {
    // ========================================================================
    // GRACEFUL DEGRADATION & STORAGE HEALTH
    // ========================================================================
    
    /// Get current storage health status
    pub fn get_storage_health(&self) -> IntegrationResult<StorageHealth> {
        let percentage = self.get_storage_usage_percentage()?;
        Ok(StorageHealth::from_percentage(percentage))
    }
    
    /// Check storage health and apply graceful degradation if needed
    /// Returns true if mode was changed
    pub fn check_and_apply_degradation(&self) -> IntegrationResult<bool> {
        let health = self.get_storage_health()?;
        
        let mut degradation = self.graceful_degradation.write()
            .map_err(|e| IntegrationError::Other(format!("Lock error: {}", e)))?;
        
        if let Some(new_mode) = degradation.check_and_degrade(health) {
            // Log the change
            println!("[Storage] üîÑ Storage mode changed due to disk space:");
            println!("[Storage]    Health: {}", health.as_str());
            println!("[Storage]    New mode: {:?}", new_mode);
            
            // If degraded to Light mode, need to cleanup full block data
            if new_mode == StorageMode::Light {
                println!("[Storage] üßπ Cleaning up full block data (keeping headers only)...");
                // Note: Actual cleanup happens in background to not block
            }
            
            return Ok(true);
        }
        
        Ok(false)
    }
    
    /// Get effective storage mode (may be degraded from original)
    pub fn get_effective_storage_mode(&self) -> StorageMode {
        self.graceful_degradation.read()
            .map(|g| g.get_current_mode())
            .unwrap_or(self.storage_mode)
    }
    
    /// Check if storage is currently degraded
    pub fn is_storage_degraded(&self) -> bool {
        self.graceful_degradation.read()
            .map(|g| g.is_degraded())
            .unwrap_or(false)
    }
    
    // ========================================================================
    // LIGHT NODE ROTATION (Auto-cleanup old headers)
    // ========================================================================
    
    /// Rotate light node headers - delete oldest to maintain max size
    /// Called automatically when saving new headers in Light mode
    pub fn rotate_light_headers(&self, current_height: u64) -> IntegrationResult<u64> {
        let mut rotation = self.light_rotation.write()
            .map_err(|e| IntegrationError::Other(format!("Lock error: {}", e)))?;
        
        if !rotation.needs_rotation() {
            rotation.increment();
            return Ok(0);
        }
        
        let to_delete = rotation.headers_to_delete();
        if to_delete == 0 {
            rotation.increment();
            return Ok(0);
        }
        
        // Delete oldest headers
        let microblocks_cf = self.persistent.db.cf_handle("microblocks")
            .ok_or_else(|| IntegrationError::StorageError("microblocks CF not found".to_string()))?;
        
        let start_height = current_height.saturating_sub(rotation.max_headers + to_delete);
        let end_height = start_height + to_delete;
        
        let mut batch = WriteBatch::default();
        let mut deleted = 0u64;
        
        for height in start_height..end_height {
            let key = format!("microblock_{}", height);
            batch.delete_cf(&microblocks_cf, key.as_bytes());
            deleted += 1;
        }
        
        if deleted > 0 {
            self.persistent.db.write(batch)?;
            rotation.decrement(deleted);
            println!("[LightRotation] üîÑ Rotated {} old headers (keeping last {})", 
                deleted, rotation.max_headers);
        }
        
        rotation.increment();
        Ok(deleted)
    }
    
    /// Check if this node should store full block data (vs headers only)
    pub fn should_store_full_blocks(&self) -> bool {
        // Check effective mode (may be degraded)
        let effective_mode = self.get_effective_storage_mode();
        effective_mode != StorageMode::Light
    }
    
    // NOTE: save_microblock_tiered() removed - logic integrated into main save_microblock()
    
    /// Check if a block should be pruned based on tier configuration
    pub fn should_prune_block(&self, block_height: u64) -> bool {
        let current_height = self.get_chain_height().unwrap_or(0);
        self.tier_config.should_prune_block(block_height, current_height)
    }
    
    /// Get storage statistics for tiered storage
    pub fn get_tiered_storage_stats(&self) -> TieredStorageStats {
        let mode_str = match self.storage_mode {
            StorageMode::Light => "Light (headers only, ~100MB)",
            StorageMode::Full => "Full (full blocks + pruning, ~500GB)",
            StorageMode::Super => "Super/Bootstrap (full history, ~2TB)",
        };
        
        let current_bytes = self.current_storage_usage.read()
            .map(|v| *v)
            .unwrap_or(0);
        
        TieredStorageStats {
            node_type: mode_str.to_string(),
            max_storage_bytes: self.tier_config.max_storage_bytes,
            pruning_window_blocks: self.tier_config.pruning_window_blocks,
            current_storage_bytes: current_bytes,
            blocks_stored: self.get_chain_height().unwrap_or(0),
            transactions_stored: 0, // Would need to count from DB
        }
    }
    
    /// Get the tier configuration
    pub fn get_tier_config(&self) -> &StorageTierConfig {
        &self.tier_config
    }
    
    /// Save raw data with a custom key (for PoH checkpoints, etc.)
    pub fn save_raw(&self, key: &str, data: &[u8]) -> IntegrationResult<()> {
        self.persistent.save_raw(key, data)
    }
    
    /// Load raw data with a custom key (for PoH checkpoints, etc.)
    pub fn load_raw(&self, key: &str) -> IntegrationResult<Option<Vec<u8>>> {
        self.persistent.load_raw(key)
    }
    
    pub fn new(data_dir: &str) -> IntegrationResult<Self> {
        let persistent = PersistentStorage::new(data_dir)?;
        let transaction_pool = TransactionPool::new();
        
        // Detect node type from environment or config
        let node_type = std::env::var("QNET_NODE_TYPE").unwrap_or_else(|_| "full".to_string());
        
        // DYNAMIC SHARD CALCULATION: Automatically scales with network growth
        // Uses existing calculate_optimal_shards() from reward_sharding module
        // NOTE: Shard count is calculated ONCE at startup and remains fixed during operation
        // This ensures storage consistency. Recalculation happens on node restart/update.
        // Production workflow: Rolling restart updates shard count across network.
        let active_shards = if let Ok(manual_shards) = std::env::var("QNET_ACTIVE_SHARDS") {
            // Manual override for testing or specific deployment needs
            manual_shards.parse::<u64>().unwrap_or_else(|_| {
                let network_size = Self::estimate_network_size_from_storage(&persistent);
                crate::reward_sharding::calculate_optimal_shards(network_size) as u64
            })
        } else {
            // AUTO-DETECTION: Calculate based on blockchain registry and heuristics
            let network_size = Self::estimate_network_size_from_storage(&persistent);
            let optimal_shards = crate::reward_sharding::calculate_optimal_shards(network_size) as u64;
            
            println!("[Storage] ‚ö° AUTO-SCALING: Calculated optimal shards: {}", optimal_shards);
            
            optimal_shards
        };
        
        // TIERED STORAGE CONFIGURATION
        // ============================================================================
        // ALL nodes receive ALL blocks from network (via P2P broadcast)
        // Storage differs by WHAT is kept and for HOW LONG:
        // - Light: Headers only (~100MB, last 1000 blocks)
        // - Full: Full blocks + pruning (~500GB, last 30 days)
        // - Super/Bootstrap: Full blocks, NO pruning (~2TB, complete history)
        // ============================================================================
        
        let (storage_mode, max_storage_gb, base_window, tier_config) = match node_type.as_str() {
            "light" => (
                StorageMode::Light, 
                1,  // ~100 MB
                1_000, // Keep last 1000 block headers
                StorageTierConfig::light()
            ),
            "full" => (
                StorageMode::Full, 
                500, // ~500 GB
                2_592_000, // ~30 days at 1 block/sec
                StorageTierConfig::full()
            ),
            "super" | "bootstrap" => (
                StorageMode::Super, 
                2000, // ~2 TB
                0, // No pruning - keep EVERYTHING
                StorageTierConfig::super_node()
            ),
            _ => {
                println!("[Storage] Unknown node type '{}', defaulting to Full mode", node_type);
                (
                    StorageMode::Full, 
                    500, 
                    2_592_000,
                    StorageTierConfig::full()
                )
            }
        };
        
        // Log tiered storage configuration
        println!("[Storage] üì¶ Tiered Storage Configuration:");
        match storage_mode {
            StorageMode::Light => {
                println!("[Storage]    Mode: LIGHT");
                println!("[Storage]    Storage: Headers only (~100MB)");
                println!("[Storage]    Pruning: Keep last {} block headers", tier_config.pruning_window_blocks);
            },
            StorageMode::Full => {
                println!("[Storage]    Mode: FULL");
                println!("[Storage]    Storage: Full blocks with pruning (~500GB)");
                println!("[Storage]    Pruning: Keep last {} blocks (~30 days)", tier_config.pruning_window_blocks);
            },
            StorageMode::Super => {
                println!("[Storage]    Mode: SUPER/BOOTSTRAP");
                println!("[Storage]    Storage: Full history, NO pruning (~2TB)");
                println!("[Storage]    Archive: Complete blockchain history");
            },
        }
        
        // CRITICAL FIX: Scale sliding window with active shards
        // This ensures we store ~1 day of data regardless of shard count
        let sliding_window = if storage_mode == StorageMode::Full && base_window > 0 {
            let scaled_window = base_window * active_shards;
            println!("[Storage] üìä Scaling window for {} active shards: {} blocks", 
                    active_shards, scaled_window);
            scaled_window
        } else {
            base_window
        };
        
        // Allow override via environment
        let max_storage_size = std::env::var("QNET_MAX_STORAGE_GB")
            .ok()
            .and_then(|s| s.parse::<u64>().ok())
            .unwrap_or(max_storage_gb) * 1024 * 1024 * 1024;
            
        let sliding_window_size = std::env::var("QNET_SLIDING_WINDOW")
            .ok()
            .and_then(|s| s.parse::<u64>().ok())
            .unwrap_or(sliding_window);
            
        println!("[Storage] üéØ Node configured as {:?} mode:", storage_mode);
        println!("[Storage]    Max storage: {} GB", max_storage_size / (1024 * 1024 * 1024));
        println!("[Storage]    Sliding window: {} blocks", 
                if sliding_window_size == u64::MAX { "unlimited".to_string() } else { sliding_window_size.to_string() });
        
        // SAFETY WARNING: Check aggressive pruning settings
        let aggressive_pruning_enabled = std::env::var("QNET_AGGRESSIVE_PRUNING")
            .unwrap_or_else(|_| "0".to_string()) == "1";
        
        if aggressive_pruning_enabled && storage_mode == StorageMode::Full {
            let super_node_count = Self::estimate_super_node_count();
            let min_safe_super_nodes = 50u64;
            
            println!("");
            println!("‚ö†Ô∏è‚ö†Ô∏è‚ö†Ô∏è WARNING: AGGRESSIVE PRUNING ENABLED ‚ö†Ô∏è‚ö†Ô∏è‚ö†Ô∏è");
            println!("This Full node will delete microblocks immediately after finalization!");
            println!("");
            println!("Network Status:");
            println!("  Super nodes in network: {}", super_node_count);
            println!("  Recommended minimum: {}", min_safe_super_nodes);
            
            if super_node_count < min_safe_super_nodes {
                println!("");
                println!("üö® CRITICAL: Network safety at RISK!");
                println!("   Not enough Super nodes to maintain full blockchain archive.");
                println!("   Aggressive pruning will be AUTOMATICALLY DISABLED during macroblock finalization.");
                println!("   Consider setting QNET_AGGRESSIVE_PRUNING=0 until network grows.");
            } else {
                println!("");
                println!("‚úÖ Network safety: OK ({} Super nodes maintain archive)", super_node_count);
                println!("   Aggressive pruning is safe but irreversible.");
                println!("   You will depend on Super nodes for historical data.");
            }
            println!("");
        }
        
        let pattern_recognizer = PatternRecognizer {
            pattern_stats: HashMap::new(),
        };
        
        // Initialize graceful degradation manager
        let graceful_degradation = GracefulDegradation::new(storage_mode);
        
        // Initialize light node rotation (1000 headers = ~100MB)
        let light_rotation = LightNodeRotation::new(tier_config.pruning_window_blocks);
            
        Ok(Self { 
            persistent,
            transaction_pool,
            max_storage_size,
            current_storage_usage: Arc::new(RwLock::new(0)),
            emergency_cleanup_enabled: true,
            storage_mode,
            sliding_window_size,
            pattern_recognizer: Arc::new(RwLock::new(pattern_recognizer)),
            tier_config,
            graceful_degradation: Arc::new(RwLock::new(graceful_degradation)),
            light_rotation: Arc::new(RwLock::new(light_rotation)),
        })
    }
    
    pub fn get_chain_height(&self) -> IntegrationResult<u64> {
        self.persistent.get_chain_height()
    }
    
    /// Set chain height to a specific value (for fork resolution)
    pub fn set_chain_height(&self, height: u64) -> IntegrationResult<()> {
        self.persistent.set_chain_height(height)
    }
    
    /// DATA CONSISTENCY: Reset chain height to 0 (wrapper for persistent storage)
    pub fn reset_chain_height(&self) -> IntegrationResult<()> {
        self.persistent.reset_chain_height()
    }
    
    pub fn get_block_hash(&self, height: u64) -> IntegrationResult<Option<String>> {
        self.persistent.get_block_hash(height)
    }
    
    pub async fn save_block(&self, block: &qnet_state::Block) -> IntegrationResult<()> {
        // Check if storage is critically full before accepting new blocks
        if self.is_storage_critically_full()? {
            // Try emergency cleanup first
            println!("[Storage] üö® Storage critically full - attempting emergency cleanup before save_block");
            self.emergency_cleanup()?;
            
            // Re-check after cleanup
            if self.is_storage_critically_full()? {
                return Err(IntegrationError::StorageError(
                    "Cannot save block: Storage is critically full even after emergency cleanup. Increase QNET_MAX_STORAGE_GB or add more disk space.".to_string()
                ));
            }
        }
        
        self.persistent.save_block(block).await
    }
    
    pub async fn load_block_by_height(&self, height: u64) -> IntegrationResult<Option<qnet_state::Block>> {
        self.persistent.load_block_by_height(height).await
    }
    
    pub fn save_microblock(&self, height: u64, data: &[u8]) -> IntegrationResult<()> {
        // =====================================================================
        // TIERED STORAGE + GRACEFUL DEGRADATION (v2.19.9)
        // =====================================================================
        // This method now includes:
        // 1. Storage health check with graceful degradation
        // 2. Tiered storage based on node type (Light/Full/Super)
        // 3. Light node auto-rotation to maintain ~100MB
        // =====================================================================
        
        // Step 1: Check for graceful degradation (every 100 blocks to reduce overhead)
        if height % 100 == 0 {
            let _ = self.check_and_apply_degradation();
        }
        
        // Step 2: Check if storage is critically full
        if self.is_storage_critically_full()? {
            println!("[Storage] üö® Storage critically full - attempting emergency cleanup");
            self.emergency_cleanup()?;
            
            // If still full after cleanup, try graceful degradation
            if self.is_storage_critically_full()? {
                // Force degradation check
                let _ = self.check_and_apply_degradation();
                
                // If STILL full after degradation, error out
                if self.is_storage_critically_full()? && self.get_effective_storage_mode() == StorageMode::Light {
                return Err(IntegrationError::StorageError(
                        "Cannot save microblock: Storage full even after degradation to Light mode. Add disk space!".to_string()
                    ));
                }
            }
        }
        
        // Step 3: Use effective storage mode (may be degraded)
        let effective_mode = self.get_effective_storage_mode();
        
        match effective_mode {
            StorageMode::Light => {
                // LIGHT MODE: Store header only + auto-rotate
                if let Ok(microblock) = bincode::deserialize::<qnet_state::MicroBlock>(data) {
                    let header = qnet_state::LightMicroBlock {
                        height: microblock.height,
                        timestamp: microblock.timestamp,
                        tx_count: microblock.transactions.len() as u32,
                        merkle_root: microblock.merkle_root,
                        size_bytes: data.len() as u32,
                        producer: microblock.producer.clone(),
                    };
                    
                    let header_data = bincode::serialize(&header)
                        .map_err(|e| IntegrationError::SerializationError(e.to_string()))?;
                    
                    let compressed = zstd::encode_all(&header_data[..], 3)
                        .map_err(|e| IntegrationError::Other(format!("Zstd error: {}", e)))?;
                    
                    // Auto-rotate old headers to maintain ~100MB limit
                    let rotated = self.rotate_light_headers(height)?;
                    if rotated > 0 && height % 1000 == 0 {
                        println!("[Storage] üîÑ Light mode: rotated {} old headers", rotated);
                    }
                    
                    return self.persistent.save_microblock(height, &compressed);
                }
                // Fallback for non-MicroBlock data
                self.persistent.save_microblock(height, data)
            },
            StorageMode::Full | StorageMode::Super => {
                // FULL/SUPER MODE: Full block storage with EfficientMicroBlock format
                if let Ok(microblock) = bincode::deserialize::<qnet_state::MicroBlock>(data) {
                    return self.save_microblock_efficient(height, &microblock);
                }
                
                // Fallback: Apply adaptive compression to raw data
        let compressed_data = if height > 0 {
            self.compress_block_adaptive(data, height)?
        } else {
            data.to_vec()
        };
        
        self.persistent.save_microblock(height, &compressed_data)
            }
        }
    }
    
    /// PRODUCTION: Save microblock in efficient format with separate TX storage
    /// This is the PRIMARY storage method for new blocks (v2.19.8+)
    /// 
    /// Architecture:
    /// - EfficientMicroBlock (hashes only) ‚Üí microblocks CF (~3-6 KB/block)
    /// - Full transactions ‚Üí transactions CF with Zstd-3 (~30-50% reduction)
    /// - TX indices ‚Üí tx_index, tx_by_address CFs
    /// 
    /// Storage savings: ~80% compared to legacy MicroBlock format
    fn save_microblock_efficient(&self, height: u64, microblock: &qnet_state::MicroBlock) -> IntegrationResult<()> {
        let tx_cf = self.persistent.db.cf_handle("transactions")
            .ok_or_else(|| IntegrationError::StorageError("transactions column family not found".to_string()))?;
        let tx_index_cf = self.persistent.db.cf_handle("tx_index")
            .ok_or_else(|| IntegrationError::StorageError("tx_index column family not found".to_string()))?;
        let tx_by_addr_cf = self.persistent.db.cf_handle("tx_by_address")
            .ok_or_else(|| IntegrationError::StorageError("tx_by_address column family not found".to_string()))?;
        
        let mut batch = WriteBatch::default();
        let mut tx_hashes: Vec<[u8; 32]> = Vec::with_capacity(microblock.transactions.len());
        let mut total_original_size = 0usize;
        let mut total_compressed_size = 0usize;
        
        // Step 1: Save each transaction with PATTERN RECOGNITION + Zstd compression
        // Pattern Recognition provides 80-95% compression for common TX types
        for tx in &microblock.transactions {
            // Calculate transaction hash
            let tx_hash = {
                use sha3::{Sha3_256, Digest};
                let mut hasher = Sha3_256::new();
                hasher.update(bincode::serialize(tx).unwrap_or_default());
                let result = hasher.finalize();
                let mut hash = [0u8; 32];
                hash.copy_from_slice(&result);
                hash
            };
            tx_hashes.push(tx_hash);
            
            let tx_key = format!("tx_{}", hex::encode(tx_hash));
            
            // Serialize original transaction for size tracking
            let tx_data = bincode::serialize(tx)
                .map_err(|e| IntegrationError::SerializationError(e.to_string()))?;
            total_original_size += tx_data.len();
            
            // COMPRESSION: Use Zstd-3 for all transactions (lossless, ~50% reduction)
            // NOTE: Pattern Recognition was removed in v2.19.10 because it was LOSSY
            // - SimpleTransfer: 140‚Üí16 bytes BUT could not be reconstructed!
            // - find_transaction_by_hash() would fail for pattern-compressed TX
            // Zstd-3 provides good compression (~50%) while remaining fully lossless
            
            // Track pattern for statistics only (no lossy compression)
            let pattern = self.recognize_transaction_pattern(tx);
            if let Ok(mut recognizer) = self.pattern_recognizer.write() {
                *recognizer.pattern_stats.entry(pattern).or_insert(0) += 1;
            }
            
            // LOSSLESS: Always use Zstd-3 compression
            let compressed_tx = zstd::encode_all(&tx_data[..], 3)
                .unwrap_or_else(|_| tx_data.clone());
            
            total_compressed_size += compressed_tx.len();
            batch.put_cf(&tx_cf, tx_key.as_bytes(), &compressed_tx);
            
            // INDEX: tx_hash -> block_height for O(1) transaction location
            batch.put_cf(&tx_index_cf, tx_key.as_bytes(), &height.to_be_bytes());
            
            // INDEX: address -> tx_hash for account transaction queries
            let timestamp = tx.timestamp;
            let from_key = format!("addr_{}_{:016x}_{}", tx.from, timestamp, hex::encode(tx_hash));
            batch.put_cf(&tx_by_addr_cf, from_key.as_bytes(), &tx_hash);
            
            if let Some(ref to) = tx.to {
                let to_key = format!("addr_{}_{:016x}_{}", to, timestamp, hex::encode(tx_hash));
                batch.put_cf(&tx_by_addr_cf, to_key.as_bytes(), &tx_hash);
            }
        }
        
        // Log pattern compression results (every 100 blocks)
        if height % 100 == 0 && total_original_size > 0 {
            let tx_savings = (1.0 - total_compressed_size as f64 / total_original_size as f64) * 100.0;
            println!("[PATTERN] üéØ Block #{}: TX compression {} ‚Üí {} bytes ({:.1}% reduction)",
                     height, total_original_size, total_compressed_size, tx_savings);
        }
        
        // Step 2: Create EfficientMicroBlock with hashes only (includes PoH data)
        let efficient_block = qnet_state::EfficientMicroBlock {
            height: microblock.height,
            timestamp: microblock.timestamp,
            transaction_hashes: tx_hashes,
            producer: microblock.producer.clone(),
            signature: microblock.signature.clone(),
            previous_hash: microblock.previous_hash,
            merkle_root: microblock.merkle_root,
            poh_hash: microblock.poh_hash.clone(),
            poh_count: microblock.poh_count,
        };
        
        // Step 3: Save PoH state separately for fast validation (v2.19.13)
        // This enables O(1) PoH validation without loading full block
        let poh_state = qnet_state::PoHState::from_microblock(microblock);
        self.persistent.save_poh_state(&poh_state)?;
        
        // Serialize EfficientMicroBlock (much smaller than full MicroBlock)
        let efficient_data = bincode::serialize(&efficient_block)
            .map_err(|e| IntegrationError::SerializationError(e.to_string()))?;
        
        // Apply adaptive compression to EfficientMicroBlock
        let compressed_block = self.compress_block_adaptive(&efficient_data, height)?;
        
        // Write all in single atomic batch
        self.persistent.db.write(batch)?;
        
        // Save the efficient block
        self.persistent.save_microblock(height, &compressed_block)?;
        
        // Log savings for monitoring (every 100 blocks)
        if height % 100 == 0 {
            let original_size = bincode::serialize(microblock).unwrap_or_default().len();
            let efficient_size = compressed_block.len();
            let savings = (1.0 - efficient_size as f64 / original_size as f64) * 100.0;
            println!("[EFFICIENT] üì¶ Block #{}: {} ‚Üí {} bytes ({:.1}% reduction, {} TXs stored separately)",
                     height, original_size, efficient_size, savings, microblock.transactions.len());
        }
        
        Ok(())
    }
    
    pub fn load_microblock(&self, height: u64) -> IntegrationResult<Option<Vec<u8>>> {
        self.persistent.load_microblock(height)
    }
    
    /// Delete a microblock at the specified height (for fork resolution)
    pub fn delete_microblock(&self, height: u64) -> IntegrationResult<()> {
        println!("[Storage] üóëÔ∏è Deleting microblock at height {}", height);
        // Also delete associated PoH state
        let _ = self.persistent.delete_poh_state(height);
        self.persistent.delete_microblock(height)
    }
    
    // ========================================================================
    // POH STATE API (v2.19.13)
    // ========================================================================
    // Fast PoH validation without loading full blocks
    // ========================================================================
    
    /// Save PoH state for a block
    pub fn save_poh_state(&self, poh_state: &qnet_state::PoHState) -> IntegrationResult<()> {
        self.persistent.save_poh_state(poh_state)
    }
    
    /// Load PoH state for a specific height
    pub fn load_poh_state(&self, height: u64) -> IntegrationResult<Option<qnet_state::PoHState>> {
        self.persistent.load_poh_state(height)
    }
    
    /// Get the latest PoH state
    pub fn get_latest_poh_state(&self) -> IntegrationResult<Option<qnet_state::PoHState>> {
        self.persistent.get_latest_poh_state()
    }
    
    /// Extract and save PoH state from a microblock
    pub fn save_poh_state_from_microblock(&self, microblock: &qnet_state::MicroBlock) -> IntegrationResult<()> {
        let poh_state = qnet_state::PoHState::from_microblock(microblock);
        self.save_poh_state(&poh_state)
    }
    
    pub fn get_latest_macroblock_hash(&self) -> Result<[u8; 32], IntegrationError> {
        self.persistent.get_latest_macroblock_hash()
    }
    
    /// Get macroblock by its index (height / 90)
    pub fn get_macroblock_by_height(&self, macroblock_index: u64) -> IntegrationResult<Option<Vec<u8>>> {
        self.persistent.get_macroblock_by_height(macroblock_index)
    }
    
    /// Save state snapshot for efficient storage
    pub async fn save_state_snapshot(&self, height: u64, state_root: [u8; 32], state_data: Vec<u8>) -> IntegrationResult<()> {
        // State snapshots are saved separately for efficient retrieval
        let snapshots_cf = self.persistent.db.cf_handle("snapshots")
            .ok_or_else(|| IntegrationError::StorageError("snapshots column family not found".to_string()))?;
        
        let key = format!("state_{}", height);
        
        // Compress state data aggressively (Zstd-15)
        let compressed = zstd::encode_all(&state_data[..], 15)
            .map_err(|e| IntegrationError::Other(format!("State compression error: {}", e)))?;
        
        self.persistent.db.put_cf(&snapshots_cf, key.as_bytes(), &compressed)?;
        
        // Store state root for verification
        let root_key = format!("state_root_{}", height);
        self.persistent.db.put_cf(&snapshots_cf, root_key.as_bytes(), &state_root)?;
        
        println!("[STATE] üíæ Saved state snapshot at height {} ({} KB compressed)", 
                height, compressed.len() / 1024);
        
        Ok(())
    }
    
    /// Save checkpoint block for Progressive Finalization
    pub async fn save_checkpoint(&self, height: u64, block: &qnet_state::MacroBlock) -> Result<(), String> {
        // Serialize and save as checkpoint
        let serialized = bincode::serialize(block)
            .map_err(|e| format!("Failed to serialize checkpoint: {}", e))?;
        
        let key = format!("checkpoint_{}", height);
        self.persistent.db.put(key, serialized)
            .map_err(|e| format!("Failed to save checkpoint: {}", e))?;
        
        println!("[STORAGE] üìç Checkpoint saved at height {}", height);
        Ok(())
    }
    
    /// Set a flag in storage (for emergency/critical markers)
    pub fn set_flag(&self, key: &str, value: bool) -> Result<(), String> {
        let flag_value = if value { vec![1u8] } else { vec![0u8] };
        self.persistent.db.put(key, flag_value)
            .map_err(|e| format!("Failed to set flag {}: {}", key, e))
    }
    
    /// Save data with a custom key
    pub fn save_data<T: serde::Serialize>(&self, key: &str, data: &T) -> Result<(), String> {
        let serialized = bincode::serialize(data)
            .map_err(|e| format!("Failed to serialize data: {}", e))?;
        
        self.persistent.db.put(key, serialized)
            .map_err(|e| format!("Failed to save data: {}", e))
    }
    
    
    pub async fn save_macroblock(&self, height: u64, macroblock: &qnet_state::MacroBlock) -> IntegrationResult<()> {
        // Check if storage is critically full before accepting new macroblocks
        if self.is_storage_critically_full()? {
            println!("[Storage] üö® Storage critically full - attempting emergency cleanup before save_macroblock");
            self.emergency_cleanup()?;
            
            if self.is_storage_critically_full()? {
                return Err(IntegrationError::StorageError(
                    "Cannot save macroblock: Storage is critically full. Increase QNET_MAX_STORAGE_GB.".to_string()
                ));
            }
        }
        
        // Save the macroblock
        self.persistent.save_macroblock(height, macroblock).await?;
        
        // CRITICAL: Save state snapshot for efficient storage
        // This is what allows us to reconstruct state without all microblocks
        if let Ok(state_data) = bincode::serialize(&macroblock) {
            // SECURITY: Verify state root is correctly calculated from microblocks
            // state_root MUST be XOR of all microblock hashes in this macroblock
            use sha3::{Sha3_256, Digest};
            let mut computed_state_root = [0u8; 32];
            
            // Recalculate state root from the microblock hashes stored in macroblock
            for microblock_hash in &macroblock.micro_blocks {
                for (i, &byte) in microblock_hash.iter().enumerate() {
                    computed_state_root[i] ^= byte;
                }
            }
            
            // NOW we can verify - comparing XOR with XOR!
            if computed_state_root != macroblock.state_root {
                return Err(IntegrationError::StorageError(
                    format!("State root verification failed at height {}: expected {:?}, computed {:?}", 
                            height, macroblock.state_root, computed_state_root)
                ));
            }
            
            // In production, this would be actual state data, not just macroblock
            // For now, we use macroblock as placeholder for state
            self.save_state_snapshot(height, macroblock.state_root, state_data).await?;
            println!("[STATE] üì∏ State snapshot saved at macroblock #{} (verified)", height);
        }
        
        // OPTIMIZATION: Prune finalized microblocks based on node type
        // CRITICAL: Only prune if enabled AND network has enough Super nodes for safety
        if std::env::var("QNET_AGGRESSIVE_PRUNING").unwrap_or_else(|_| "0".to_string()) == "1" {
            // SAFETY CHECK: Verify network has enough archival Super nodes
            let super_node_count = Self::estimate_super_node_count();
            let min_required_super_nodes = 10u64; // Minimum for network safety
            
            if super_node_count < min_required_super_nodes {
                println!("[PRUNING] üõ°Ô∏è SAFETY: Aggressive pruning DISABLED - insufficient Super nodes in network");
                println!("[PRUNING]    Current Super nodes: {} | Required minimum: {}", 
                        super_node_count, min_required_super_nodes);
                println!("[PRUNING]    Full blockchain archive must be maintained until more Super nodes join!");
                // Skip aggressive pruning for network safety
            } else if self.storage_mode == StorageMode::Full {
                // Safe to prune: network has enough archival nodes
                println!("[PRUNING] ‚úÖ Network safety verified: {} Super nodes maintain full archive", 
                        super_node_count);
                self.prune_finalized_microblocks(macroblock).await?;
            }
        }
        // Normal operation: rely on sliding window pruning in prune_old_blocks()
        
        Ok(())
    }
    
    /// Public wrapper for network size estimation (used by node configuration)
    pub fn estimate_network_size_for_config(&self) -> usize {
        Self::estimate_network_size_from_storage(&self.persistent)
    }
    
    /// Estimate total network size for dynamic shard calculation
    /// Uses multi-source detection: blockchain, environment, heuristics
    fn estimate_network_size_from_storage(persistent: &PersistentStorage) -> usize {
        // Priority 1: Explicit network size from monitoring/orchestration
        if let Ok(size_str) = std::env::var("QNET_TOTAL_NETWORK_NODES") {
            if let Ok(size) = size_str.parse::<usize>() {
                println!("[Storage] üìä Network size from monitoring: {} nodes", size);
                return size;
            }
        }
        
        // Priority 2: Genesis phase detection (5 bootstrap nodes)
        if std::env::var("QNET_BOOTSTRAP_ID").is_ok() {
            println!("[Storage] üå± Genesis phase: 5 bootstrap nodes");
            return 5;
        }
        
        // Priority 3: Read actual node activations from blockchain storage
        if let Some(activations_cf) = persistent.db.cf_handle("activations") {
            let mut count = 0;
            let iter = persistent.db.iterator_cf(activations_cf, rocksdb::IteratorMode::Start);
            for _ in iter {
                count += 1;
            }
            
            if count > 0 {
                println!("[Storage] üîó Blockchain registry: {} activated nodes", count);
                return count;
            }
        }
        
        // Priority 4: Conservative default (small network assumption)
        println!("[Storage] ‚ö†Ô∏è No network data found, using conservative default: 100 nodes");
        100 // Conservative: assume small network to avoid over-sharding
    }
    
    /// Estimate Super node count in the network (conservative approximation)
    /// Used for safety checks before aggressive pruning
    fn estimate_super_node_count() -> u64 {
        // Try to get from environment (set by monitoring/stats system)
        if let Ok(count_str) = std::env::var("QNET_SUPER_NODE_COUNT") {
            if let Ok(count) = count_str.parse::<u64>() {
                return count;
            }
        }
        
        // Conservative estimation based on network phase
        let bootstrap_id = std::env::var("QNET_BOOTSTRAP_ID").ok();
        
        if bootstrap_id.is_some() {
            // Genesis phase: 5 bootstrap Super nodes
            5
        } else {
            // Production: Conservative estimate based on total network size
            // In real deployment, this would query P2P or consensus layer
            // For now, return safe default that allows aggressive pruning
            50 // Assume mature network has enough Super nodes
        }
    }
    
    /// Remove microblocks that have been finalized by a macroblock
    /// This dramatically reduces storage as we only keep macroblocks + state
    async fn prune_finalized_microblocks(&self, macroblock: &qnet_state::MacroBlock) -> IntegrationResult<()> {
        // Only prune if enabled (safety check)
        if std::env::var("QNET_PRUNE_FINALIZED_MICROS").unwrap_or_else(|_| "1".to_string()) != "1" {
            return Ok(());
        }
        
        println!("[PRUNING] üéØ Pruning microblocks finalized by macroblock {}", macroblock.height);
        
        let microblocks_cf = self.persistent.db.cf_handle("microblocks")
            .ok_or_else(|| IntegrationError::StorageError("microblocks column family not found".to_string()))?;
        
        let mut batch = WriteBatch::default();
        let mut pruned = 0;
        
        // CRITICAL FIX: Macroblock height != microblock heights!
        // Macroblock #1 finalizes microblocks 1-90
        // Macroblock #2 finalizes microblocks 91-180
        // Formula: macro_num * 90 gives us the last microblock finalized
        
        // Calculate which microblocks this macroblock finalizes
        // Each macroblock finalizes 90 microblocks (3 leaders √ó 30 blocks each)
        let macro_number = macroblock.height; // This is macroblock number, not microblock!
        let last_micro = macro_number * 90;
        let first_micro = last_micro.saturating_sub(89); // 90 blocks total
        
        println!("[PRUNING] Macroblock #{} finalizes microblocks {}-{}", 
                macro_number, first_micro, last_micro);
        
        // Delete the finalized microblocks
        for micro_height in first_micro..=last_micro {
            let key = format!("microblock_{}", micro_height);
            if self.persistent.db.get_cf(&microblocks_cf, key.as_bytes())?.is_some() {
                batch.delete_cf(&microblocks_cf, key.as_bytes());
                pruned += 1;
                
                // Log leader transitions (every 30 blocks)
                if micro_height % 30 == 0 {
                    println!("[PRUNING] üîÑ Leader rotation point at microblock {}", micro_height);
                }
            }
        }
        
        if pruned > 0 {
            self.persistent.db.write(batch)?;
            println!("[PRUNING] ‚úÖ Pruned {} microblocks (3 leader rotations finalized)", pruned);
        }
        
        Ok(())
    }
    
    pub fn get_stats(&self) -> IntegrationResult<StorageStats> {
        self.persistent.get_stats()
    }

    // Activation code methods
    pub fn save_activation_code(&self, code: &str, node_type: u8, timestamp: u64) -> IntegrationResult<()> {
        self.persistent.save_activation_code(code, node_type, timestamp)
    }

    pub fn load_activation_code(&self) -> IntegrationResult<Option<(String, u8, u64)>> {
        self.persistent.load_activation_code()
    }

    pub fn clear_activation_code(&self) -> IntegrationResult<()> {
        self.persistent.clear_activation_code()
    }
    
    /// Get burn transaction hash for activation code (for XOR decryption)
    pub fn get_activation_burn_tx(&self) -> IntegrationResult<String> {
        self.persistent.get_activation_burn_tx()
    }
    
    /// Save burn transaction hash for activation code (for XOR decryption)
    pub fn save_activation_burn_tx(&self, burn_tx: &str) -> IntegrationResult<()> {
        self.persistent.save_activation_burn_tx(burn_tx)
    }
    
    /// Find transaction by hash
    pub async fn find_transaction_by_hash(&self, tx_hash: &str) -> IntegrationResult<Option<qnet_state::Transaction>> {
        self.persistent.find_transaction_by_hash(tx_hash).await
    }

    /// Get transaction block height
    pub async fn get_transaction_block_height(&self, tx_hash: &str) -> IntegrationResult<u64> {
        self.persistent.get_transaction_block_height(tx_hash).await
    }
    
    /// Get transactions for an address (paginated)
    pub async fn get_transactions_by_address(&self, address: &str, page: usize, per_page: usize) -> IntegrationResult<Vec<qnet_state::Transaction>> {
        self.persistent.get_transactions_by_address(address, page, per_page).await
    }
    
    /// Count transactions for an address
    pub async fn count_transactions_by_address(&self, address: &str) -> IntegrationResult<usize> {
        self.persistent.count_transactions_by_address(address).await
    }
    
    /// Get reputation history for a node
    pub fn get_reputation_history(&self, node_id: &str, limit: usize) -> IntegrationResult<Vec<serde_json::Value>> {
        self.get_reputation_history_internal(node_id, limit)
    }
    
    /// Save reputation change event
    pub fn save_reputation_change(&self, node_id: &str, old_value: f64, new_value: f64, reason: &str) -> IntegrationResult<()> {
        self.save_reputation_change_internal(node_id, old_value, new_value, reason)
    }

    pub fn update_activation_for_migration(&self, code: &str, node_type: u8, timestamp: u64, new_device_signature: &str) -> IntegrationResult<()> {
        self.persistent.update_activation_for_migration(code, node_type, timestamp, new_device_signature)
    }
    
    /// Save consensus state for persistence
    pub fn save_consensus_state(&self, round: u64, state: &[u8]) -> IntegrationResult<()> {
        self.persistent.save_consensus_state(round, state)
    }
    
    /// Load consensus state after restart
    pub fn load_consensus_state(&self, round: u64) -> IntegrationResult<Option<Vec<u8>>> {
        self.persistent.load_consensus_state(round)
    }
    
    /// Get latest consensus round
    pub fn get_latest_consensus_round(&self) -> IntegrationResult<u64> {
        self.persistent.get_latest_consensus_round()
    }
    
    /// Save sync progress
    pub fn save_sync_progress(&self, from_height: u64, to_height: u64, current: u64) -> IntegrationResult<()> {
        self.persistent.save_sync_progress(from_height, to_height, current)
    }
    
    /// Load sync progress
    pub fn load_sync_progress(&self) -> IntegrationResult<Option<(u64, u64, u64)>> {
        self.persistent.load_sync_progress()
    }
    
    /// Clear sync progress
    pub fn clear_sync_progress(&self) -> IntegrationResult<()> {
        self.persistent.clear_sync_progress()
    }
    
    /// Get microblocks range for batch sync  
    /// CRITICAL: Returns full MicroBlock format for network sync (not EfficientMicroBlock)
    /// This ensures receiving nodes can deserialize blocks with full transaction data
    pub async fn get_microblocks_range(&self, from: u64, to: u64) -> IntegrationResult<Vec<(u64, Vec<u8>)>> {
        let mut microblocks = Vec::new();
        
        // Get RocksDB column family for transactions
        let tx_cf = self.persistent.db.cf_handle("transactions")
            .ok_or_else(|| IntegrationError::StorageError("transactions column family not found".to_string()))?;
        
        for height in from..=to {
            if let Some(raw_data) = self.load_microblock(height)? {
                // CRITICAL: Convert EfficientMicroBlock back to full MicroBlock for network sync
                // First try to deserialize as EfficientMicroBlock (new format)
                if let Ok(efficient_block) = bincode::deserialize::<qnet_state::EfficientMicroBlock>(&raw_data) {
                    // Reconstruct full MicroBlock with transactions from PERSISTENT storage
                    let mut transactions = Vec::with_capacity(efficient_block.transaction_hashes.len());
                    
                    for tx_hash in &efficient_block.transaction_hashes {
                        let tx_hash_hex = hex::encode(tx_hash);
                        
                        // First try in-memory cache for speed
                        if let Some(tx) = self.transaction_pool.get_transaction(tx_hash) {
                            transactions.push(tx);
                            continue;
                        }
                        
                        // Fallback to persistent RocksDB storage
                        let tx_key = format!("tx_{}", tx_hash_hex);
                        if let Ok(Some(data)) = self.persistent.db.get_cf(&tx_cf, tx_key.as_bytes()) {
                            // Decompress if Zstd-compressed
                            let tx_data = if data.len() >= 4 && data[0..4] == [0x28, 0xb5, 0x2f, 0xfd] {
                                zstd::decode_all(&data[..]).unwrap_or(data.to_vec())
                            } else {
                                data.to_vec()
                            };
                            
                            if let Ok(tx) = bincode::deserialize::<qnet_state::Transaction>(&tx_data) {
                                // Cache for future use
                                let _ = self.transaction_pool.store_transaction(*tx_hash, tx.clone());
                                transactions.push(tx);
                            }
                        }
                    }
                    
                    // Create full MicroBlock
                    let full_block = qnet_state::MicroBlock {
                        height: efficient_block.height,
                        timestamp: efficient_block.timestamp,
                        transactions,
                        producer: efficient_block.producer,
                        signature: efficient_block.signature,
                        previous_hash: efficient_block.previous_hash,
                        merkle_root: efficient_block.merkle_root,
                        poh_hash: efficient_block.poh_hash,
                        poh_count: efficient_block.poh_count,
                    };
                    
                    // Serialize as full MicroBlock for network transmission
                    let full_data = bincode::serialize(&full_block)
                        .map_err(|e| IntegrationError::SerializationError(e.to_string()))?;
                    
                    microblocks.push((height, full_data));
                } else {
                    // Already in MicroBlock format (legacy) - use as-is
                    microblocks.push((height, raw_data));
                }
            }
        }
        
        Ok(microblocks)
    }
    
    /// Legacy: Get blocks range for old Block format
    pub async fn get_blocks_range(&self, from: u64, to: u64) -> IntegrationResult<Vec<qnet_state::Block>> {
        self.persistent.get_blocks_range(from, to).await
    }
    
    /// Get transaction pool statistics
    pub fn get_transaction_pool_stats(&self) -> IntegrationResult<(usize, usize)> {
        self.transaction_pool.get_stats()
    }
    
    // =========================================================================
    // MACROBLOCK SYNC METHODS (PRODUCTION v2.19.12)
    // =========================================================================
    
    /// Get macroblocks range for batch sync
    /// PRODUCTION: Returns serialized MacroBlock data for network transmission
    /// 
    /// Architecture:
    /// - Macroblocks are indexed by INDEX (not height): index 1 = blocks 1-90
    /// - Max 10 macroblocks per batch (~1MB max)
    /// - Decompresses if stored compressed
    pub async fn get_macroblocks_range(&self, from_index: u64, to_index: u64) -> IntegrationResult<Vec<(u64, Vec<u8>)>> {
        let mut macroblocks = Vec::new();
        
        // SCALABILITY: Limit to 10 macroblocks per batch
        let actual_to = if to_index > from_index && to_index - from_index > 10 {
            from_index + 9
        } else {
            to_index
        };
        
        for index in from_index..=actual_to {
            if let Some(raw_data) = self.get_macroblock_by_height(index)? {
                // Decompress if needed (Zstd magic bytes check)
                let data = if raw_data.len() >= 4 && raw_data[0..4] == [0x28, 0xb5, 0x2f, 0xfd] {
                    zstd::decode_all(&raw_data[..]).unwrap_or(raw_data)
                } else {
                    raw_data
                };
                
                // Verify it's a valid MacroBlock before sending
                if bincode::deserialize::<qnet_state::MacroBlock>(&data).is_ok() {
                    macroblocks.push((index, data));
                } else {
                    println!("[MACROBLOCK-SYNC] ‚ö†Ô∏è Invalid macroblock data at index {}", index);
                }
            }
        }
        
        println!("[MACROBLOCK-SYNC] üì¶ Prepared {} macroblocks for sync (indices {}-{})", 
                 macroblocks.len(), from_index, actual_to);
        
        Ok(macroblocks)
    }
    
    /// Get the latest macroblock index
    /// PRODUCTION: Used to determine sync target
    pub fn get_latest_macroblock_index(&self) -> IntegrationResult<u64> {
        let chain_height = self.get_chain_height()?;
        if chain_height == 0 {
            Ok(0)
        } else {
            // Macroblock index = (height / 90), but only if that macroblock is complete
            let complete_macroblocks = chain_height / 90;
            Ok(complete_macroblocks)
        }
    }
    
    /// Load microblock with automatic format detection (backward compatibility)
    /// Supports both EfficientMicroBlock (new) and MicroBlock (legacy) formats
    /// Handles Zstd compression transparently
    pub fn load_microblock_auto_format(&self, height: u64) -> IntegrationResult<Option<qnet_state::MicroBlock>> {
        // Try to load raw microblock data
        let raw_data = match self.load_microblock(height)? {
            Some(data) => data,
            None => return Ok(None),
        };
        
        // CRITICAL: Decompress if Zstd-compressed (magic bytes: 0x28 0xb5 0x2f 0xfd)
        // Data is compressed in save_microblock_efficient via compress_block_adaptive
        let microblock_data = if raw_data.len() >= 4 && raw_data[0..4] == [0x28, 0xb5, 0x2f, 0xfd] {
            zstd::decode_all(&raw_data[..])
                .map_err(|e| IntegrationError::Other(format!("Zstd decompression failed: {}", e)))?
        } else {
            raw_data
        };
        
        // First, try to deserialize as EfficientMicroBlock (new format)
        if let Ok(efficient_block) = bincode::deserialize::<qnet_state::EfficientMicroBlock>(&microblock_data) {
            // Reconstruct full microblock from efficient format
            // CRITICAL: Load transactions from PERSISTENT RocksDB storage, NOT in-memory pool
            // This ensures transactions are available even after restart or TTL expiry
            let mut transactions = Vec::with_capacity(efficient_block.transaction_hashes.len());
            
            for tx_hash in &efficient_block.transaction_hashes {
                let tx_hash_hex = hex::encode(tx_hash);
                
                // First try in-memory cache for speed
                if let Some(tx) = self.transaction_pool.get_transaction(tx_hash) {
                    transactions.push(tx);
                    continue;
                }
                
                // Fallback to persistent RocksDB storage
                // Use blocking approach since this is a sync function
                let tx_cf = match self.persistent.db.cf_handle("transactions") {
                    Some(cf) => cf,
                    None => {
                        println!("[Storage] ‚ö†Ô∏è transactions CF not found for block {}", height);
                        continue;
                    }
                };
                
                let tx_key = format!("tx_{}", tx_hash_hex);
                match self.persistent.db.get_cf(&tx_cf, tx_key.as_bytes()) {
                    Ok(Some(data)) => {
                        // Decompress if Zstd-compressed
                        let tx_data = if data.len() >= 4 && data[0..4] == [0x28, 0xb5, 0x2f, 0xfd] {
                            zstd::decode_all(&data[..]).unwrap_or(data.to_vec())
                        } else {
                            data.to_vec()
                        };
                        
                        if let Ok(tx) = bincode::deserialize::<qnet_state::Transaction>(&tx_data) {
                            // Cache for future use
                            let _ = self.transaction_pool.store_transaction(*tx_hash, tx.clone());
                            transactions.push(tx);
                        } else {
                            println!("[Storage] ‚ö†Ô∏è Failed to deserialize TX {} for block {}", tx_hash_hex, height);
                        }
                    }
                    Ok(None) => {
                        println!("[Storage] ‚ö†Ô∏è Transaction {} not found in storage for block {}", tx_hash_hex, height);
                    }
                    Err(e) => {
                        println!("[Storage] ‚ö†Ô∏è Error loading TX {}: {}", tx_hash_hex, e);
                    }
                }
            }
            
            // Reconstruct full MicroBlock
            let microblock = qnet_state::MicroBlock {
                height: efficient_block.height,
                timestamp: efficient_block.timestamp,
                transactions,
                producer: efficient_block.producer,
                signature: efficient_block.signature,
                previous_hash: efficient_block.previous_hash,
                merkle_root: efficient_block.merkle_root,
                poh_hash: efficient_block.poh_hash,
                poh_count: efficient_block.poh_count,
            };
            
            return Ok(Some(microblock));
        }
        
        // Fallback: try to deserialize as legacy MicroBlock format
        if let Ok(legacy_block) = bincode::deserialize::<qnet_state::MicroBlock>(&microblock_data) {
            // For backward compatibility, also populate transaction pool with legacy data
            for tx in &legacy_block.transactions {
                // Convert string hash to [u8; 32]
                if let Ok(hash_bytes) = hex::decode(&tx.hash) {
                    if hash_bytes.len() == 32 {
                        let mut hash_array = [0u8; 32];
                        hash_array.copy_from_slice(&hash_bytes);
                        if let Err(e) = self.transaction_pool.store_transaction(hash_array, tx.clone()) {
                            println!("[Storage] ‚ö†Ô∏è Failed to cache legacy transaction {}: {}", hex::encode(hash_array), e);
                        }
                    }
                }
            }
            
            return Ok(Some(legacy_block));
        }
        
        Err(IntegrationError::StorageError(
            format!("Unable to deserialize microblock {} in any known format", height)
        ))
    }
    
    /// Convert legacy microblock to efficient format (migration utility)
    pub fn migrate_legacy_microblock_to_efficient(&self, height: u64) -> IntegrationResult<bool> {
        // Load raw data
        let microblock_data = match self.load_microblock(height)? {
            Some(data) => data,
            None => return Ok(false),
        };
        
        // Check if it's already in efficient format
        if bincode::deserialize::<qnet_state::EfficientMicroBlock>(&microblock_data).is_ok() {
            println!("[Storage] ‚úÖ Microblock {} already in efficient format", height);
            return Ok(false);
        }
        
        // Try to deserialize as legacy format
        let legacy_block = bincode::deserialize::<qnet_state::MicroBlock>(&microblock_data)
            .map_err(|e| IntegrationError::SerializationError(
                format!("Failed to deserialize legacy microblock {}: {}", height, e)
            ))?;
        
        println!("[Storage] üîÑ Converting legacy microblock {} to efficient format", height);
        
        // Save in new format with delta compression
        let block_data = bincode::serialize(&legacy_block)
            .map_err(|e| IntegrationError::SerializationError(e.to_string()))?;
        self.save_block_with_delta(height, &block_data)?;
        
        println!("[Storage] ‚úÖ Migrated microblock {} to efficient format", height);
        Ok(true)
    }
    
    /// Batch migration of legacy microblocks (for system upgrade)
    pub fn batch_migrate_legacy_microblocks(&self, start_height: u64, end_height: u64) -> IntegrationResult<u64> {
        let mut migrated_count = 0;
        
        println!("[Storage] üöÄ Starting batch migration of microblocks {} to {}", start_height, end_height);
        
        for height in start_height..=end_height {
            match self.migrate_legacy_microblock_to_efficient(height) {
                Ok(true) => {
                    migrated_count += 1;
                    if migrated_count % 100 == 0 {
                        println!("[Storage] üìä Migration progress: {} microblocks converted", migrated_count);
                    }
                },
                Ok(false) => {
                    // Already efficient or doesn't exist
                },
                Err(e) => {
                    println!("[Storage] ‚ö†Ô∏è Failed to migrate microblock {}: {}", height, e);
                }
            }
        }
        
        println!("[Storage] üéâ Batch migration completed: {} microblocks converted to efficient format", migrated_count);
        
        Ok(migrated_count)
    }
    
    // ========================================================================
    // POH STATE MIGRATION (v2.19.13)
    // ========================================================================
    // Migrate existing blocks to have separate PoH state for fast validation
    // This is a one-time migration that runs on node startup
    // ========================================================================
    
    /// Migrate PoH state for a single block (extract from block and save separately)
    pub fn migrate_poh_state_for_block(&self, height: u64) -> IntegrationResult<bool> {
        // Check if PoH state already exists
        if let Ok(Some(_)) = self.load_poh_state(height) {
            return Ok(false); // Already migrated
        }
        
        // Load block using auto-format detection
        let microblock = match self.load_microblock_auto_format(height)? {
            Some(block) => block,
            None => return Ok(false), // Block doesn't exist
        };
        
        // Extract and save PoH state
        let poh_state = qnet_state::PoHState::from_microblock(&microblock);
        self.save_poh_state(&poh_state)?;
        
        Ok(true)
    }
    
    /// Migrate PoH state for all existing blocks (run on startup)
    /// Returns number of blocks migrated
    pub fn migrate_all_poh_states(&self) -> IntegrationResult<u64> {
        let chain_height = self.persistent.get_chain_height()?;
        if chain_height == 0 {
            println!("[POH_MIGRATION] ‚ÑπÔ∏è No blocks to migrate");
            return Ok(0);
        }
        
        println!("[POH_MIGRATION] üöÄ Starting PoH state migration for {} blocks", chain_height + 1);
        
        let mut migrated = 0u64;
        let mut skipped = 0u64;
        let start_time = std::time::Instant::now();
        
        for height in 0..=chain_height {
            match self.migrate_poh_state_for_block(height) {
                Ok(true) => {
                    migrated += 1;
                    if migrated % 1000 == 0 {
                        let elapsed = start_time.elapsed().as_secs();
                        let rate = if elapsed > 0 { migrated / elapsed } else { migrated };
                        println!("[POH_MIGRATION] üìä Progress: {} migrated, {} skipped ({} blocks/sec)", 
                                migrated, skipped, rate);
                    }
                }
                Ok(false) => {
                    skipped += 1;
                }
                Err(e) => {
                    println!("[POH_MIGRATION] ‚ö†Ô∏è Failed to migrate PoH state for block {}: {}", height, e);
                }
            }
        }
        
        let elapsed = start_time.elapsed();
        println!("[POH_MIGRATION] ‚úÖ Migration completed in {:.2}s: {} migrated, {} skipped", 
                elapsed.as_secs_f64(), migrated, skipped);
        
        Ok(migrated)
    }
    
    /// Check if PoH state migration is needed
    pub fn needs_poh_migration(&self) -> IntegrationResult<bool> {
        let chain_height = self.persistent.get_chain_height()?;
        if chain_height == 0 {
            return Ok(false); // No blocks yet
        }
        
        // Check if PoH state exists for the latest block
        // If not, migration is needed
        match self.load_poh_state(chain_height)? {
            Some(_) => Ok(false), // Already have PoH state
            None => Ok(true),     // Need to migrate
        }
    }
    
    /// High-level compression utilities for archive data
    pub fn compress_archive_data(&self, data: &[u8]) -> IntegrationResult<Vec<u8>> {
        let compressed = zstd::encode_all(data, 9) // Level 9 for maximum compression (archive data)
            .map_err(|e| IntegrationError::Other(format!("Zstd compression error: {}", e)))?;
            
        if compressed.len() < data.len() {
            println!("[Compression] ‚úÖ Archive data compressed ({} -> {} bytes)", 
                    data.len(), compressed.len());
            Ok(compressed)
        } else {
            println!("[Compression] ‚è≠Ô∏è Archive data not compressed (no benefit)");
            Ok(data.to_vec())
        }
    }
    
    /// Decompress archive data
    pub fn decompress_archive_data(&self, data: &[u8]) -> IntegrationResult<Vec<u8>> {
        // Try to decompress with Zstd first
        match zstd::decode_all(data) {
            Ok(decompressed) => {
                println!("[Compression] ‚úÖ Archive data decompressed: {} -> {} bytes", 
                        data.len(), decompressed.len());
                Ok(decompressed)
            },
            Err(_) => {
                // Data might not be compressed, return as-is
                println!("[Compression] ‚è≠Ô∏è Data not compressed, returning as-is");
                Ok(data.to_vec())
            }
        }
    }
    
    /// Compress transaction pool for efficient storage
    pub fn compress_transaction_pool(&self) -> IntegrationResult<Vec<u8>> {
        let (tx_count, _) = self.transaction_pool.get_stats()?;
        
        if tx_count == 0 {
            return Ok(Vec::new());
        }
        
        println!("[Compression] üîÑ Compressing transaction pool with {} transactions", tx_count);
        
        // Serialize all transactions
        let transactions = self.transaction_pool.transactions.read()
            .map_err(|e| IntegrationError::Other(format!("Lock error: {}", e)))?;
        let creation_times = self.transaction_pool.creation_times.read()
            .map_err(|e| IntegrationError::Other(format!("Lock error: {}", e)))?;
            
        let pool_data = (&*transactions, &*creation_times);
        let serialized = bincode::serialize(&pool_data)
            .map_err(|e| IntegrationError::SerializationError(e.to_string()))?;
        
        drop(transactions);
        drop(creation_times);
        
        // Compress with high level for long-term storage
        let compressed = zstd::encode_all(&serialized[..], 6) // Level 6 for good compression
            .map_err(|e| IntegrationError::Other(format!("Zstd compression error: {}", e)))?;
            
        println!("[Compression] ‚úÖ Transaction pool compressed ({} -> {} bytes)", 
                serialized.len(), compressed.len());
                
        Ok(compressed)
    }
    
    /// PRODUCTION: Check storage usage and trigger emergency cleanup if needed
    pub fn check_storage_usage_and_cleanup(&self) -> IntegrationResult<bool> {
        let data_dir = std::env::var("QNET_DATA_DIR").unwrap_or_else(|_| "./node_data".to_string());
        
        // Get actual disk usage
        let actual_usage = self.get_directory_size(&data_dir)?;
        
        // Update current usage tracking
        {
            let mut usage = self.current_storage_usage.write()
                .map_err(|e| IntegrationError::Other(format!("Lock error: {}", e)))?;
            *usage = actual_usage;
        }
        
        let usage_percentage = (actual_usage as f64 / self.max_storage_size as f64) * 100.0;
        
        println!("[Storage] üìä Storage usage: {:.1} GB / {:.1} GB ({:.1}%)", 
                actual_usage as f64 / (1024.0 * 1024.0 * 1024.0),
                self.max_storage_size as f64 / (1024.0 * 1024.0 * 1024.0),
                usage_percentage);
        
        // Trigger cleanup at different thresholds
        match usage_percentage {
            p if p >= 95.0 => {
                println!("[Storage] üö® CRITICAL: Storage 95%+ full, triggering emergency cleanup");
                self.emergency_cleanup()?;
                Ok(false) // Emergency state
            },
            p if p >= 85.0 => {
                println!("[Storage] ‚ö†Ô∏è WARNING: Storage 85%+ full, triggering aggressive cleanup");
                self.aggressive_cleanup()?;
                Ok(false) // Warning state
            },
            p if p >= 70.0 => {
                println!("[Storage] üìã INFO: Storage 70%+ full, triggering standard cleanup");
                self.standard_cleanup()?;
                Ok(true) // Normal operation
            },
            _ => {
                println!("[Storage] ‚úÖ Storage usage normal ({:.1}%)", usage_percentage);
                Ok(true) // Normal operation
            }
        }
    }
    
    /// Get directory size in bytes
    fn get_directory_size(&self, path: &str) -> IntegrationResult<u64> {
        let mut total_size = 0u64;
        
        fn visit_dir(dir: &std::path::Path, total: &mut u64) -> Result<(), Box<dyn std::error::Error>> {
            if dir.is_dir() {
                for entry in std::fs::read_dir(dir)? {
                    let entry = entry?;
                    let path = entry.path();
                    if path.is_dir() {
                        visit_dir(&path, total)?;
                    } else {
                        if let Ok(metadata) = entry.metadata() {
                            *total += metadata.len();
                        }
                    }
                }
            }
            Ok(())
        }
        
        if let Err(e) = visit_dir(std::path::Path::new(path), &mut total_size) {
            println!("[Storage] ‚ö†Ô∏è Failed to calculate directory size: {}", e);
            // Fallback: return estimated size
            return Ok(self.estimate_storage_usage());
        }
        
        Ok(total_size)
    }
    
    /// Estimate storage usage based on blockchain height
    fn estimate_storage_usage(&self) -> u64 {
        // Rough estimate: 32 KB per microblock + transaction pool
        if let Ok(height) = self.get_chain_height() {
            let microblock_size = height * 32 * 1024; // 32 KB per microblock
            let pool_size = 500 * 1024 * 1024; // 500 MB estimated pool size
            microblock_size + pool_size
        } else {
            0
        }
    }
    
    /// Standard cleanup (70-85% full) - remove ONLY cache data, preserve blockchain history
    fn standard_cleanup(&self) -> IntegrationResult<()> {
        println!("[Storage] üßπ Starting standard cleanup (cache cleanup only - blockchain history preserved)");
        
        // 1. Clean transaction pool cache (this is OK - only removes duplicates)
        let removed_tx = self.transaction_pool.cleanup_old_duplicates()?;
        println!("[Storage] üì¶ Removed {} old transaction duplicates from cache", removed_tx);
        
        // 2. CRITICAL CORRECTION: DO NOT delete blockchain history!
        // Instead, implement proper cache management
        
        // 3. PRODUCTION: Compress old data instead of deleting
        // Note: Compression now happens automatically via adaptive compression
        // Force RocksDB compaction to optimize storage efficiency
        
        // 4. Force RocksDB compaction to optimize storage efficiency
        self.persistent.db.compact_range::<&[u8], &[u8]>(None, None);
        println!("[Storage] üóúÔ∏è Database compaction completed - optimized storage layout");
        
        println!("[Storage] ‚úÖ Standard cleanup completed (blockchain history preserved)");
        Ok(())
    }
    
    /// Aggressive cleanup (85-95% full) - CACHE cleanup only, blockchain history preserved
    fn aggressive_cleanup(&self) -> IntegrationResult<()> {
        println!("[Storage] üî• Starting aggressive cleanup (cache optimization - blockchain history preserved)");
        
        // 1. PRODUCTION: More aggressive transaction pool cleanup (6 hours instead of 24)
        let current_time = SystemTime::now()
            .duration_since(UNIX_EPOCH)
            .map_err(|e| IntegrationError::Other(format!("Time error: {}", e)))?
            .as_secs();
        let aggressive_cutoff = current_time.saturating_sub(6 * 3600); // 6 hours
        
        // Force aggressive cleanup of transaction pool CACHE only
        {
            let mut transactions = self.transaction_pool.transactions.write()
                .map_err(|e| IntegrationError::Other(format!("Lock error: {}", e)))?;
            let mut creation_times = self.transaction_pool.creation_times.write()
                .map_err(|e| IntegrationError::Other(format!("Lock error: {}", e)))?;
            
            let old_hashes: Vec<[u8; 32]> = creation_times.iter()
                .filter(|(_, &time)| time < aggressive_cutoff)
                .map(|(hash, _)| *hash)
                .collect();
                
            for hash in old_hashes {
                transactions.remove(&hash);
                creation_times.remove(&hash);
            }
            
            println!("[Storage] üß® Aggressive transaction CACHE cleanup: removed duplicates older than 6 hours");
        }
        
        // 2. CRITICAL CORRECTION: DO NOT delete blockchain history!
        // 3. PRODUCTION: Maximum compression instead of deletion
        // Note: Compression now happens automatically via adaptive compression
        
        // 4. PRODUCTION: Force RocksDB compaction to reclaim space immediately
        self.persistent.db.compact_range::<&[u8], &[u8]>(None, None);
        println!("[Storage] üóúÔ∏è Database compaction completed - optimized storage efficiency");
        
        println!("[Storage] ‚ö° Aggressive cleanup completed (blockchain history preserved)");
        Ok(())
    }
    
    /// Emergency cleanup (95%+ full) - remove all non-essential data
    fn emergency_cleanup(&self) -> IntegrationResult<()> {
        println!("[Storage] üö® EMERGENCY CLEANUP: Storage critically full, removing all non-essential data");
        
        if !self.emergency_cleanup_enabled {
            return Err(IntegrationError::StorageError(
                "Emergency cleanup disabled, cannot continue operation".to_string()
            ));
        }
        
        // PRODUCTION EMERGENCY MEASURES:
        
        // 1. Clear ALL transaction pool except last hour
        let current_time = SystemTime::now()
            .duration_since(UNIX_EPOCH)
            .map_err(|e| IntegrationError::Other(format!("Time error: {}", e)))?
            .as_secs();
        let emergency_cutoff = current_time.saturating_sub(3600); // 1 hour only
        
        {
            let mut transactions = self.transaction_pool.transactions.write()
                .map_err(|e| IntegrationError::Other(format!("Lock error: {}", e)))?;
            let mut creation_times = self.transaction_pool.creation_times.write()
                .map_err(|e| IntegrationError::Other(format!("Lock error: {}", e)))?;
            
            let emergency_hashes: Vec<[u8; 32]> = creation_times.iter()
                .filter(|(_, &time)| time < emergency_cutoff)
                .map(|(hash, _)| *hash)
                .collect();
                
            for hash in emergency_hashes {
                transactions.remove(&hash);
                creation_times.remove(&hash);
            }
            
            println!("[Storage] üÜò EMERGENCY: Cleared transaction pool (kept only last 1 hour)");
        }
        
        // 2. CRITICAL CORRECTION: DO NOT delete blockchain history even in emergency!
        // Instead, maximum compression and cache optimization
        println!("[Storage] üÜò EMERGENCY: Applying maximum compression to blockchain data");
        
        // Emergency compression of blockchain data
        // Note: Compression now happens automatically via adaptive compression
        
        // 3. PRODUCTION: Force maximum compression on all remaining data
        self.persistent.db.compact_range::<&[u8], &[u8]>(None, None);
        println!("[Storage] üóúÔ∏è Emergency compaction completed");
        
        // 4. CRITICAL CORRECTION: DO NOT delete transaction history from blockchain!
        // Emergency optimization through compression only
        println!("[Storage] üÜò EMERGENCY: Optimizing storage through compression (history preserved)");
        
        println!("[Storage] üÜò Emergency cleanup completed - node should continue operation");
        
        // Check if we're still critically full after cleanup
        let post_cleanup_usage = self.get_directory_size(&std::env::var("QNET_DATA_DIR").unwrap_or_else(|_| "./node_data".to_string()))?;
        let post_cleanup_percentage = (post_cleanup_usage as f64 / self.max_storage_size as f64) * 100.0;
        
        if post_cleanup_percentage >= 90.0 {
            println!("[Storage] üö® CRITICAL: Even after emergency cleanup, storage is {:.1}% full!", post_cleanup_percentage);
            println!("[Storage] üí° IMMEDIATE ADMIN ACTIONS REQUIRED:");
            println!("[Storage]    1. Add more disk space immediately");
            println!("[Storage]    2. Set QNET_MAX_STORAGE_GB=500 or higher");
            println!("[Storage]    3. Monitor disk usage closely");
            println!("[Storage]    4. Consider moving to server with larger storage");
            println!("[Storage] ‚ö†Ô∏è  NODE WILL STRUGGLE TO ACCEPT NEW BLOCKS!");
        } else {
            println!("[Storage] ‚úÖ Emergency cleanup successful - storage now at {:.1}%", post_cleanup_percentage);
            println!("[Storage] üí° RECOMMENDED ACTIONS:");
            println!("[Storage]    1. Consider increasing QNET_MAX_STORAGE_GB=500");
            println!("[Storage]    2. Plan for long-term storage growth");
        }
        
        Ok(())
    }
    
    /// Get current storage usage percentage
    pub fn get_storage_usage_percentage(&self) -> IntegrationResult<f64> {
        let usage = *self.current_storage_usage.read()
            .map_err(|e| IntegrationError::Other(format!("Lock error: {}", e)))?;
        Ok((usage as f64 / self.max_storage_size as f64) * 100.0)
    }
    
    /// Check if storage is critically full
    pub fn is_storage_critically_full(&self) -> IntegrationResult<bool> {
        Ok(self.get_storage_usage_percentage()? >= 95.0)
    }
    
    /// Get maximum storage size
    pub fn get_max_storage_size(&self) -> u64 {
        self.max_storage_size
    }
    
    /// Update maximum storage size (for runtime configuration)
    pub fn update_max_storage_size(&mut self, new_size_gb: u64) {
        self.max_storage_size = new_size_gb * 1024 * 1024 * 1024;
        println!("[Storage] üîß Updated maximum storage size to {} GB", new_size_gb);
    }
    
    /// Get compression level based on block age
    pub fn get_compression_level(&self, block_height: u64) -> CompressionLevel {
        let current_height = self.get_chain_height().unwrap_or(0);
        if current_height <= block_height {
            return CompressionLevel::None;
        }
        
        let age_blocks = current_height - block_height;
        // 86400 blocks per day (1 block per second)
        let age_days = age_blocks / 86400;
        
        match age_days {
            0..=1 => CompressionLevel::None,
            2..=7 => CompressionLevel::Light,
            8..=30 => CompressionLevel::Medium,
            31..=365 => CompressionLevel::Heavy,
            _ => CompressionLevel::Extreme,
        }
    }
    
    /// Get Zstd compression level from enum
    fn get_zstd_level(&self, level: CompressionLevel) -> Option<i32> {
        match level {
            CompressionLevel::None => None,
            CompressionLevel::Light => Some(3),
            CompressionLevel::Medium => Some(9),
            CompressionLevel::Heavy => Some(15),
            CompressionLevel::Extreme => Some(22), // Maximum compression
        }
    }
    
    /// Compress block data with adaptive level
    pub fn compress_block_adaptive(&self, block_data: &[u8], height: u64) -> IntegrationResult<Vec<u8>> {
        let compression_level = self.get_compression_level(height);
        
        match self.get_zstd_level(compression_level) {
            None => {
                // No compression for hot data
                Ok(block_data.to_vec())
            },
            Some(zstd_level) => {
                let compressed = zstd::encode_all(block_data, zstd_level)
                    .map_err(|e| IntegrationError::Other(format!("Zstd compression error: {}", e)))?;
                
                // Only use compression if it reduces size by at least 10%
                if compressed.len() < (block_data.len() * 9 / 10) {
                    println!("[Compression] ‚úÖ Level {:?} applied ({} -> {} bytes, {:.1}% reduction)", 
                            compression_level, block_data.len(), compressed.len(),
                            (1.0 - compressed.len() as f64 / block_data.len() as f64) * 100.0);
                    Ok(compressed)
                } else {
                    Ok(block_data.to_vec())
                }
            }
        }
    }
    
    /// Decompress block data if it's compressed
    pub fn decompress_block(&self, data: &[u8]) -> IntegrationResult<Vec<u8>> {
        // Try to decompress with zstd - if it fails, data is not compressed
        match zstd::decode_all(data) {
            Ok(decompressed) => {
                println!("[Compression] ‚úÖ Decompressed {} -> {} bytes", data.len(), decompressed.len());
                Ok(decompressed)
            },
            Err(_) => {
                // Not compressed, return as-is
                Ok(data.to_vec())
            }
        }
    }
    
    // NOTE: calculate_block_delta() and apply_block_delta() removed in v2.19.10
    // Delta encoding was evaluated but Pattern Recognition + Zstd provides better results
    
    /// Save block with optimal compression (delegates to unified save_microblock)
    /// 
    /// UNIFIED STORAGE: All block saving goes through save_microblock() which handles:
    /// - Tiered storage (Light/Full/Super)
    /// - Pattern Recognition compression (89% for simple transfers)
    /// - EfficientMicroBlock format (hashes only + separate TX storage)
    /// - Adaptive Zstd compression (levels 3-22 based on age)
    /// - Graceful degradation when disk full
    /// 
    /// This method exists for backward compatibility with node.rs
    pub fn save_block_with_delta(&self, height: u64, data: &[u8]) -> IntegrationResult<()> {
        // UNIFIED: Delegate to save_microblock which has all compression logic
        self.save_microblock(height, data)
    }
    
    /// Pattern recognition for transaction compression
    pub fn recognize_transaction_pattern(&self, tx: &qnet_state::Transaction) -> TransactionPattern {
        // Analyze transaction type based on its fields
        // Note: This is simplified - in production would use actual transaction structure
        
        // Check by hash patterns (simplified heuristics)
        let tx_size = bincode::serialize(tx).unwrap_or_default().len();
        
        // Simple transfers are usually small (< 500 bytes)
        if tx_size < 500 {
            return TransactionPattern::SimpleTransfer;
        }
        
        // Node activations have specific size patterns
        if tx_size >= 500 && tx_size < 1000 {
            return TransactionPattern::NodeActivation;
        }
        
        // Contract deployments are large
        if tx_size > 10000 {
            return TransactionPattern::ContractDeploy;
        }
        
        // Contract calls are medium sized
        if tx_size >= 1000 && tx_size < 10000 {
            return TransactionPattern::ContractCall;
        }
        
        TransactionPattern::Unknown
    }
    
    /// Compress transaction based on pattern
    pub fn compress_transaction_by_pattern(
        &self,
        tx: &qnet_state::Transaction,
        pattern: TransactionPattern
    ) -> IntegrationResult<CompressedTransaction> {
        let original_data = bincode::serialize(tx)
            .map_err(|e| IntegrationError::SerializationError(e.to_string()))?;
        
        let compressed_data = match pattern {
            TransactionPattern::SimpleTransfer => {
                // For simple transfers, we can optimize heavily
                // Store only: from_index(4) + to_index(4) + amount(8) = 16 bytes
                // Instead of full addresses and metadata
                let mut compact = Vec::with_capacity(16);
                
                // Extract essential fields (simplified)
                // In production, would parse actual transaction fields
                if original_data.len() >= 100 {
                    // Take first 4 bytes as "from" identifier
                    compact.extend_from_slice(&original_data[8..12]);
                    // Take next 4 bytes as "to" identifier  
                    compact.extend_from_slice(&original_data[40..44]);
                    // Take amount (8 bytes)
                    compact.extend_from_slice(&original_data[72..80].get(..8).unwrap_or(&[0u8; 8]));
                }
                compact
            },
            TransactionPattern::NodeActivation => {
                // For node activations: node_type(1) + amount(8) + phase(1) = 10 bytes
                let mut compact = Vec::with_capacity(10);
                if original_data.len() >= 50 {
                    compact.push(original_data[20]); // node type
                    compact.extend_from_slice(&original_data[24..32]); // amount
                    compact.push(original_data[40]); // phase
                }
                compact
            },
            TransactionPattern::RewardDistribution => {
                // Rewards are predictable: recipient(4) + amount(8) + pool_id(1) = 13 bytes
                let mut compact = Vec::with_capacity(13);
                if original_data.len() >= 40 {
                    compact.extend_from_slice(&original_data[8..12]); // recipient
                    compact.extend_from_slice(&original_data[16..24]); // amount
                    compact.push(original_data[30]); // pool_id
                }
                compact
            },
            _ => {
                // For complex patterns, use standard compression
                zstd::encode_all(&original_data[..], 3)
                    .map_err(|e| IntegrationError::Other(format!("Compression error: {}", e)))?
            }
        };
        
        let compressed_tx = CompressedTransaction {
            pattern,
            data: compressed_data.clone(),
            original_size: original_data.len(),
        };
        
        // Log compression efficiency
        if compressed_data.len() < original_data.len() {
            let reduction = (1.0 - compressed_data.len() as f64 / original_data.len() as f64) * 100.0;
            println!("[PATTERN] ‚úÖ Transaction compressed via {:?} pattern: {} -> {} bytes ({:.1}% reduction)",
                    pattern, original_data.len(), compressed_data.len(), reduction);
        }
        
        Ok(compressed_tx)
    }
    
    /// Decompress transaction from pattern
    pub fn decompress_transaction_from_pattern(
        &self,
        compressed: &CompressedTransaction,
        full_tx_template: Option<&qnet_state::Transaction>
    ) -> IntegrationResult<Vec<u8>> {
        match compressed.pattern {
            TransactionPattern::SimpleTransfer | 
            TransactionPattern::NodeActivation | 
            TransactionPattern::RewardDistribution => {
                // For simple patterns, we need template to reconstruct
                if let Some(template) = full_tx_template {
                    let mut full_data = bincode::serialize(template)
                        .map_err(|e| IntegrationError::SerializationError(e.to_string()))?;
                    
                    // Overlay compressed data onto template
                    match compressed.pattern {
                        TransactionPattern::SimpleTransfer => {
                            if compressed.data.len() >= 16 {
                                full_data[8..12].copy_from_slice(&compressed.data[0..4]);
                                full_data[40..44].copy_from_slice(&compressed.data[4..8]);
                                full_data[72..80].copy_from_slice(&compressed.data[8..16]);
                            }
                        },
                        _ => {}
                    }
                    Ok(full_data)
                } else {
                    // Without template, can't reconstruct simple patterns
                    Err(IntegrationError::Other("Template required for pattern decompression".to_string()))
                }
            },
            _ => {
                // Complex patterns use standard decompression
                zstd::decode_all(&compressed.data[..])
                    .map_err(|e| IntegrationError::Other(format!("Decompression error: {}", e)))
            }
        }
    }
    
    /// PRODUCTION: Recompress old blocks with appropriate compression level
    pub async fn recompress_old_blocks(&self) -> IntegrationResult<()> {
        println!("[Storage] üóúÔ∏è Starting adaptive recompression of old blocks");
        
        let current_height = self.get_chain_height()?;
        let microblocks_cf = self.persistent.db.cf_handle("microblocks")
            .ok_or_else(|| IntegrationError::StorageError("microblocks column family not found".to_string()))?;
        
        let mut recompressed_count = 0;
        let mut space_saved = 0i64;
        
        // Process blocks in batches
        const BATCH_SIZE: u64 = 1000;
        
        // Process blocks in reverse order (newest to oldest)
        let mut batch_starts: Vec<u64> = Vec::new();
        let mut start = 1;
        while start <= current_height {
            batch_starts.push(start);
            start += BATCH_SIZE;
        }
        
        for batch_start in batch_starts.into_iter().rev() {
            let batch_end = std::cmp::min(batch_start + BATCH_SIZE - 1, current_height);
            let mut batch = WriteBatch::default();
            
            for height in batch_start..=batch_end {
                let key = format!("microblock_{}", height);
                
                if let Ok(Some(existing_data)) = self.persistent.db.get_cf(&microblocks_cf, key.as_bytes()) {
                    let original_size = existing_data.len();
                    let compression_level = self.get_compression_level(height);
                    
                    // Skip if already optimally compressed
                    if compression_level == CompressionLevel::None {
                        continue;
                    }
                    
                    // Decompress if needed (check if compressed)
                    let decompressed = if existing_data.starts_with(&[0x28, 0xb5, 0x2f, 0xfd]) {
                        // Zstd magic number
                        zstd::decode_all(&existing_data[..])
                            .unwrap_or_else(|_| existing_data.clone())
                    } else {
                        existing_data.clone()
                    };
                    
                    // Recompress with appropriate level
                    let recompressed = self.compress_block_adaptive(&decompressed, height)?;
                    
                    if recompressed.len() < original_size {
                        batch.put_cf(&microblocks_cf, key.as_bytes(), &recompressed);
                        space_saved += (original_size as i64) - (recompressed.len() as i64);
                        recompressed_count += 1;
                    }
                }
            }
            
            // Apply batch
            if !batch.is_empty() {
                self.persistent.db.write(batch)?;
                println!("[Storage] üì¶ Recompressed batch {}-{}: {} blocks, saved {} KB",
                        batch_start, batch_end, recompressed_count, space_saved / 1024);
            }
            
            // Limit processing to avoid blocking too long
            if recompressed_count >= 10000 {
                break;
            }
        }
        
        // Force compaction to reclaim space
        self.persistent.db.compact_range_cf(&microblocks_cf, None::<&[u8]>, None::<&[u8]>);
        
        println!("[Storage] ‚úÖ Adaptive recompression complete: {} blocks, {} MB saved",
                recompressed_count, space_saved / (1024 * 1024));
        
        // PRODUCTION: Also recompress old transactions with stronger Zstd
        // Done synchronously to avoid Send issues with RocksDB handles
        let tx_saved = self.recompress_old_transactions_sync()?;
        if tx_saved > 0 {
            println!("[Storage] ‚úÖ Transaction recompression saved {} MB", tx_saved / (1024 * 1024));
        }
        
        Ok(())
    }
    
    /// PRODUCTION: Recompress old transactions with stronger Zstd levels
    /// Called from recompress_old_blocks() as background task
    /// Synchronous to avoid Send issues with RocksDB column family handles
    /// Processes in batches to avoid blocking too long
    pub fn recompress_old_transactions_sync(&self) -> IntegrationResult<i64> {
        let tx_cf = self.persistent.db.cf_handle("transactions")
            .ok_or_else(|| IntegrationError::StorageError("transactions column family not found".to_string()))?;
        let tx_index_cf = self.persistent.db.cf_handle("tx_index")
            .ok_or_else(|| IntegrationError::StorageError("tx_index column family not found".to_string()))?;
        
        let current_height = self.get_chain_height()?;
        let mut space_saved: i64 = 0;
        let mut recompressed_count = 0;
        
        // Only recompress transactions older than 7 days (604800 blocks)
        let old_threshold = current_height.saturating_sub(604800);
        
        let iter = self.persistent.db.iterator_cf(&tx_index_cf, rocksdb::IteratorMode::Start);
        let mut batch = WriteBatch::default();
        
        for item in iter {
            let (tx_key, height_data) = item?;
            
            if height_data.len() < 8 {
                continue;
            }
            
            let block_height = u64::from_be_bytes(height_data[..8].try_into().unwrap_or([0u8; 8]));
            
            // Skip recent transactions (keep fast access)
            if block_height > old_threshold {
                continue;
            }
            
            // Get current transaction data
            if let Ok(Some(tx_data)) = self.persistent.db.get_cf(&tx_cf, &tx_key) {
                let original_size = tx_data.len();
                
                // Determine compression level based on age
                let age_days = (current_height - block_height) / 86400;
                let zstd_level = match age_days {
                    0..=7 => continue,      // Skip recent
                    8..=30 => 9,            // Medium compression
                    31..=365 => 15,         // Heavy compression
                    _ => 22,                // Extreme compression for old data
                };
                
                // Decompress if already compressed
                let decompressed = if tx_data.len() >= 4 && tx_data[0..4] == [0x28, 0xb5, 0x2f, 0xfd] {
                    // Check current compression level (approximate by ratio)
                    // Skip if already heavily compressed
                    if let Ok(dec) = zstd::decode_all(&tx_data[..]) {
                        let current_ratio = tx_data.len() as f64 / dec.len() as f64;
                        if current_ratio < 0.3 && age_days < 365 {
                            // Already well compressed, skip unless very old
                            continue;
                        }
                        dec
                    } else {
                        continue;
                    }
                } else {
                    tx_data.to_vec()
                };
                
                // Recompress with stronger level
                if let Ok(recompressed) = zstd::encode_all(&decompressed[..], zstd_level) {
                    if recompressed.len() < original_size {
                        batch.put_cf(&tx_cf, &tx_key, &recompressed);
                        space_saved += (original_size as i64) - (recompressed.len() as i64);
                        recompressed_count += 1;
                        
                        // Apply batch every 1000 transactions
                        if recompressed_count % 1000 == 0 {
                            self.persistent.db.write(batch)?;
                            batch = WriteBatch::default();
                            // Brief pause to allow other operations (non-blocking)
                            std::thread::sleep(std::time::Duration::from_millis(1));
                        }
                    }
                }
            }
            
            // Limit total processing per run
            if recompressed_count >= 10000 {
                break;
            }
        }
        
        // Apply remaining batch
        if !batch.is_empty() {
            self.persistent.db.write(batch)?;
        }
        
        // Compact to reclaim space
        if space_saved > 0 {
            self.persistent.db.compact_range_cf(&tx_cf, None::<&[u8]>, None::<&[u8]>);
        }
        
        println!("[Storage] üóúÔ∏è Recompressed {} old transactions, saved {} KB",
                recompressed_count, space_saved / 1024);
        
        Ok(space_saved)
    }
    
    /// Calculate recommended storage size based on blockchain age and activity
    pub fn get_recommended_storage_size_gb(&self) -> IntegrationResult<u64> {
        let stats = self.get_stats()?;
        let current_height = stats.latest_height;
        
        // Estimate blockchain age in years (assuming 1 microblock/second)
        let blockchain_age_years = current_height as f64 / (86400.0 * 365.0); // seconds per year
        
        // Base storage requirements
        let microblocks_gb_per_year = 20; // ~20 GB per year for microblocks
        let transactions_gb_per_year = 10; // ~10 GB per year for average transaction volume
        let buffer_multiplier = 1.5; // 50% buffer for growth and overhead
        
        // Calculate recommended size
        let estimated_total_gb = (blockchain_age_years * (microblocks_gb_per_year + transactions_gb_per_year) as f64 * buffer_multiplier) as u64;
        
        // Minimum recommendations by blockchain age
        let min_recommended = match blockchain_age_years {
            age if age < 1.0 => 300,  // First year: 300 GB
            age if age < 3.0 => 400,  // 1-3 years: 400 GB  
            age if age < 5.0 => 500,  // 3-5 years: 500 GB
            age if age < 10.0 => 750, // 5-10 years: 750 GB
            _ => 1000,                // 10+ years: 1 TB
        };
        
        let recommended = std::cmp::max(estimated_total_gb, min_recommended);
        
        if recommended > (self.max_storage_size / (1024 * 1024 * 1024)) {
            println!("[Storage] üí° RECOMMENDATION: Current limit {} GB, recommended {} GB for blockchain age {:.1} years", 
                    self.max_storage_size / (1024 * 1024 * 1024),
                    recommended,
                    blockchain_age_years);
        }
        
        Ok(recommended)
    }
    
    // ============================================
    // SCALABILITY: PENDING REWARDS IN ROCKSDB
    // ============================================
    
    /// Save pending reward for a node
    pub fn save_pending_reward(&self, node_id: &str, reward: &qnet_consensus::lazy_rewards::PhaseAwareReward) -> IntegrationResult<()> {
        let rewards_cf = self.persistent.db.cf_handle("pending_rewards")
            .ok_or_else(|| IntegrationError::StorageError("pending_rewards column family not found".to_string()))?;
        
        let key = format!("reward_{}", node_id);
        let data = bincode::serialize(reward)
            .map_err(|e| IntegrationError::SerializationError(e.to_string()))?;
        
        self.persistent.db.put_cf(&rewards_cf, key.as_bytes(), &data)?;
        Ok(())
    }
    
    /// Load pending reward for a node
    pub fn load_pending_reward(&self, node_id: &str) -> IntegrationResult<Option<qnet_consensus::lazy_rewards::PhaseAwareReward>> {
        let rewards_cf = self.persistent.db.cf_handle("pending_rewards")
            .ok_or_else(|| IntegrationError::StorageError("pending_rewards column family not found".to_string()))?;
        
        let key = format!("reward_{}", node_id);
        match self.persistent.db.get_cf(&rewards_cf, key.as_bytes())? {
            Some(data) => {
                let reward = bincode::deserialize(&data)
                    .map_err(|e| IntegrationError::DeserializationError(e.to_string()))?;
                Ok(Some(reward))
            },
            None => Ok(None),
        }
    }
    
    /// Delete pending reward after claim
    pub fn delete_pending_reward(&self, node_id: &str) -> IntegrationResult<()> {
        let rewards_cf = self.persistent.db.cf_handle("pending_rewards")
            .ok_or_else(|| IntegrationError::StorageError("pending_rewards column family not found".to_string()))?;
        
        let key = format!("reward_{}", node_id);
        self.persistent.db.delete_cf(&rewards_cf, key.as_bytes())?;
        Ok(())
    }
    
    /// Get all pending rewards (for batch processing)
    pub fn get_all_pending_rewards(&self) -> IntegrationResult<Vec<(String, qnet_consensus::lazy_rewards::PhaseAwareReward)>> {
        let rewards_cf = self.persistent.db.cf_handle("pending_rewards")
            .ok_or_else(|| IntegrationError::StorageError("pending_rewards column family not found".to_string()))?;
        
        let mut rewards = Vec::new();
        let iter = self.persistent.db.iterator_cf(&rewards_cf, rocksdb::IteratorMode::Start);
        
        for item in iter {
            let (key, value) = item?;
            if let Ok(key_str) = std::str::from_utf8(&key) {
                if key_str.starts_with("reward_") {
                    let node_id = key_str.strip_prefix("reward_").unwrap().to_string();
                    let reward: qnet_consensus::lazy_rewards::PhaseAwareReward = bincode::deserialize(&value)
                        .map_err(|e| IntegrationError::DeserializationError(e.to_string()))?;
                    rewards.push((node_id, reward));
                }
            }
        }
        
        Ok(rewards)
    }
    
    // ============================================
    // SCALABILITY: NODE REGISTRY IN ROCKSDB
    // ============================================
    
    /// Save node registration information
    pub fn save_node_registration(&self, node_id: &str, node_type: &str, wallet: &str, reputation: f64) -> IntegrationResult<()> {
        let registry_cf = self.persistent.db.cf_handle("node_registry")
            .ok_or_else(|| IntegrationError::StorageError("node_registry column family not found".to_string()))?;
        
        let key = format!("node_{}", node_id);
        let data = json!({
            "node_type": node_type,
            "wallet": wallet,
            "reputation": reputation,
            "timestamp": SystemTime::now().duration_since(UNIX_EPOCH).unwrap().as_secs()
        });
        
        self.persistent.db.put_cf(&registry_cf, key.as_bytes(), data.to_string().as_bytes())?;
        Ok(())
    }
    
    /// Load node registration
    pub fn load_node_registration(&self, node_id: &str) -> IntegrationResult<Option<(String, String, f64)>> {
        let registry_cf = self.persistent.db.cf_handle("node_registry")
            .ok_or_else(|| IntegrationError::StorageError("node_registry column family not found".to_string()))?;
        
        let key = format!("node_{}", node_id);
        match self.persistent.db.get_cf(&registry_cf, key.as_bytes())? {
            Some(data) => {
                let json_str = std::str::from_utf8(&data)
                    .map_err(|e| IntegrationError::DeserializationError(e.to_string()))?;
                let parsed: serde_json::Value = serde_json::from_str(json_str)
                    .map_err(|e| IntegrationError::DeserializationError(e.to_string()))?;
                
                Ok(Some((
                    parsed["node_type"].as_str().unwrap_or("light").to_string(),
                    parsed["wallet"].as_str().unwrap_or("").to_string(),
                    parsed["reputation"].as_f64().unwrap_or(70.0)
                )))
            },
            None => Ok(None),
        }
    }
    
    // ============================================
    // SCALABILITY: PING HISTORY IN ROCKSDB
    // ============================================
    
    /// Save ping attempt result
    pub fn save_ping_attempt(&self, node_id: &str, timestamp: u64, success: bool, response_time_ms: u32) -> IntegrationResult<()> {
        let ping_cf = self.persistent.db.cf_handle("ping_history")
            .ok_or_else(|| IntegrationError::StorageError("ping_history column family not found".to_string()))?;
        
        // Use timestamp in key for ordering
        let key = format!("ping_{}_{}", node_id, timestamp);
        let data = json!({
            "success": success,
            "response_time_ms": response_time_ms,
            "timestamp": timestamp
        });
        
        self.persistent.db.put_cf(&ping_cf, key.as_bytes(), data.to_string().as_bytes())?;
        
        // Cleanup old pings (older than 24 hours)
        self.cleanup_old_pings(node_id, timestamp - 86400)?;
        
        Ok(())
    }
    
    /// Get ping history for a node
    pub fn get_ping_history(&self, node_id: &str, since_timestamp: u64) -> IntegrationResult<Vec<(u64, bool, u32)>> {
        let ping_cf = self.persistent.db.cf_handle("ping_history")
            .ok_or_else(|| IntegrationError::StorageError("ping_history column family not found".to_string()))?;
        
        let mut pings = Vec::new();
        let prefix = format!("ping_{}_", node_id);
        let iter = self.persistent.db.iterator_cf(&ping_cf, rocksdb::IteratorMode::From(prefix.as_bytes(), rocksdb::Direction::Forward));
        
        for item in iter {
            let (key, value) = item?;
            let key_str = std::str::from_utf8(&key).unwrap_or("");
            
            if !key_str.starts_with(&prefix) {
                break; // Reached end of this node's pings
            }
            
            if let Ok(parsed) = serde_json::from_slice::<serde_json::Value>(&value) {
                let timestamp = parsed["timestamp"].as_u64().unwrap_or(0);
                if timestamp >= since_timestamp {
                    let success = parsed["success"].as_bool().unwrap_or(false);
                    let response_time = parsed["response_time_ms"].as_u64().unwrap_or(0) as u32;
                    pings.push((timestamp, success, response_time));
                }
            }
        }
        
        Ok(pings)
    }
    
    /// Cleanup old ping records
    fn cleanup_old_pings(&self, node_id: &str, cutoff_timestamp: u64) -> IntegrationResult<()> {
        let ping_cf = self.persistent.db.cf_handle("ping_history")
            .ok_or_else(|| IntegrationError::StorageError("ping_history column family not found".to_string()))?;
        
        let prefix = format!("ping_{}_", node_id);
        let iter = self.persistent.db.iterator_cf(&ping_cf, rocksdb::IteratorMode::From(prefix.as_bytes(), rocksdb::Direction::Forward));
        
        let mut batch = WriteBatch::default();
        for item in iter {
            let (key, value) = item?;
            let key_str = std::str::from_utf8(&key).unwrap_or("");
            
            if !key_str.starts_with(&prefix) {
                break;
            }
            
            if let Ok(parsed) = serde_json::from_slice::<serde_json::Value>(&value) {
                let timestamp = parsed["timestamp"].as_u64().unwrap_or(0);
                if timestamp < cutoff_timestamp {
                    batch.delete_cf(&ping_cf, &key);
                }
            }
        }
        
        if batch.len() > 0 {
            self.persistent.db.write(batch)?;
        }
        
        Ok(())
    }
    
    // ============================================
    // PRODUCTION: REPUTATION HISTORY STORAGE
    // ============================================
    
    /// Save reputation change event (for audit trail and history)
    fn save_reputation_change_internal(&self, node_id: &str, old_value: f64, new_value: f64, reason: &str) -> IntegrationResult<()> {
        let rep_cf = self.persistent.db.cf_handle("node_registry")
            .ok_or_else(|| IntegrationError::StorageError("node_registry column family not found".to_string()))?;
        
        let timestamp = std::time::SystemTime::now()
            .duration_since(std::time::UNIX_EPOCH)
            .unwrap()
            .as_secs();
        
        // Key: rep_history_{node_id}_{timestamp} for chronological ordering
        let key = format!("rep_history_{}_{}", node_id, timestamp);
        let data = serde_json::json!({
            "node_id": node_id,
            "old_value": old_value,
            "new_value": new_value,
            "delta": new_value - old_value,
            "reason": reason,
            "timestamp": timestamp
        });
        
        self.persistent.db.put_cf(&rep_cf, key.as_bytes(), data.to_string().as_bytes())?;
        
        // Cleanup old history (keep only last 7 days)
        self.cleanup_old_reputation_history(node_id, timestamp - (7 * 86400))?;
        
        Ok(())
    }
    
    /// Get reputation history for a node
    fn get_reputation_history_internal(&self, node_id: &str, limit: usize) -> IntegrationResult<Vec<serde_json::Value>> {
        let rep_cf = self.persistent.db.cf_handle("node_registry")
            .ok_or_else(|| IntegrationError::StorageError("node_registry column family not found".to_string()))?;
        
        let mut history = Vec::new();
        let prefix = format!("rep_history_{}_", node_id);
        
        // Iterate in reverse to get most recent first
        let iter = self.persistent.db.iterator_cf(
            &rep_cf, 
            rocksdb::IteratorMode::From(
                format!("{}~", prefix).as_bytes(), // ~ is after digits in ASCII
                rocksdb::Direction::Reverse
            )
        );
        
        for item in iter {
            let (key, value) = item?;
            let key_str = std::str::from_utf8(&key).unwrap_or("");
            
            if !key_str.starts_with(&prefix) {
                break;
            }
            
            if let Ok(parsed) = serde_json::from_slice::<serde_json::Value>(&value) {
                history.push(parsed);
                if history.len() >= limit {
                    break;
                }
            }
        }
        
        Ok(history)
    }
    
    /// Cleanup old reputation history records
    fn cleanup_old_reputation_history(&self, node_id: &str, cutoff_timestamp: u64) -> IntegrationResult<()> {
        let rep_cf = self.persistent.db.cf_handle("node_registry")
            .ok_or_else(|| IntegrationError::StorageError("node_registry column family not found".to_string()))?;
        
        let prefix = format!("rep_history_{}_", node_id);
        let iter = self.persistent.db.iterator_cf(&rep_cf, rocksdb::IteratorMode::From(prefix.as_bytes(), rocksdb::Direction::Forward));
        
        let mut batch = WriteBatch::default();
        for item in iter {
            let (key, value) = item?;
            let key_str = std::str::from_utf8(&key).unwrap_or("");
            
            if !key_str.starts_with(&prefix) {
                break;
            }
            
            if let Ok(parsed) = serde_json::from_slice::<serde_json::Value>(&value) {
                let timestamp = parsed["timestamp"].as_u64().unwrap_or(0);
                if timestamp < cutoff_timestamp {
                    batch.delete_cf(&rep_cf, &key);
                }
            }
        }
        
        if batch.len() > 0 {
            self.persistent.db.write(batch)?;
        }
        
        Ok(())
    }
    
    // ============================================
    // PRODUCTION: ATTESTATION STORAGE (Light nodes)
    // ============================================
    
    /// Save Light node attestation (persistent for reward calculation)
    pub fn save_attestation(&self, light_node_id: &str, slot: u64, pinger_id: &str, timestamp: u64) -> IntegrationResult<()> {
        let att_cf = self.persistent.db.cf_handle("attestations")
            .ok_or_else(|| IntegrationError::StorageError("attestations column family not found".to_string()))?;
        
        // Key: att_{light_node_id}_{slot} for deduplication
        let key = format!("att_{}_{}", light_node_id, slot);
        let data = json!({
            "light_node_id": light_node_id,
            "slot": slot,
            "pinger_id": pinger_id,
            "timestamp": timestamp
        });
        
        self.persistent.db.put_cf(&att_cf, key.as_bytes(), data.to_string().as_bytes())?;
        Ok(())
    }
    
    /// Check if attestation exists for Light node in slot
    pub fn has_attestation(&self, light_node_id: &str, slot: u64) -> IntegrationResult<bool> {
        let att_cf = self.persistent.db.cf_handle("attestations")
            .ok_or_else(|| IntegrationError::StorageError("attestations column family not found".to_string()))?;
        
        let key = format!("att_{}_{}", light_node_id, slot);
        Ok(self.persistent.db.get_cf(&att_cf, key.as_bytes())?.is_some())
    }
    
    /// Count attestations for Light node in 4h window (for reward eligibility)
    pub fn count_attestations_in_window(&self, light_node_id: &str, window_start_slot: u64, window_end_slot: u64) -> IntegrationResult<u32> {
        let att_cf = self.persistent.db.cf_handle("attestations")
            .ok_or_else(|| IntegrationError::StorageError("attestations column family not found".to_string()))?;
        
        let mut count = 0u32;
        for slot in window_start_slot..=window_end_slot {
            let key = format!("att_{}_{}", light_node_id, slot);
            if self.persistent.db.get_cf(&att_cf, key.as_bytes())?.is_some() {
                count += 1;
            }
        }
        Ok(count)
    }
    
    /// Cleanup old attestations (older than 24 hours)
    pub fn cleanup_old_attestations(&self, cutoff_timestamp: u64) -> IntegrationResult<u32> {
        let att_cf = self.persistent.db.cf_handle("attestations")
            .ok_or_else(|| IntegrationError::StorageError("attestations column family not found".to_string()))?;
        
        let iter = self.persistent.db.iterator_cf(&att_cf, rocksdb::IteratorMode::Start);
        let mut batch = WriteBatch::default();
        let mut removed = 0u32;
        
        for item in iter {
            let (key, value) = item?;
            if let Ok(parsed) = serde_json::from_slice::<serde_json::Value>(&value) {
                let timestamp = parsed["timestamp"].as_u64().unwrap_or(0);
                if timestamp < cutoff_timestamp {
                    batch.delete_cf(&att_cf, &key);
                    removed += 1;
                }
            }
        }
        
        if batch.len() > 0 {
            self.persistent.db.write(batch)?;
        }
        
        Ok(removed)
    }
    
    // ============================================
    // PRODUCTION: HEARTBEAT STORAGE (Full/Super nodes)
    // ============================================
    
    /// Save Full/Super node heartbeat (persistent for reward calculation)
    pub fn save_heartbeat(&self, node_id: &str, heartbeat_index: u8, timestamp: u64, block_height: u64) -> IntegrationResult<()> {
        let hb_cf = self.persistent.db.cf_handle("heartbeats")
            .ok_or_else(|| IntegrationError::StorageError("heartbeats column family not found".to_string()))?;
        
        // Key: hb_{node_id}_{4h_window}_{index} for deduplication per window
        let window = timestamp - (timestamp % (4 * 60 * 60));
        let key = format!("hb_{}_{}_{}", node_id, window, heartbeat_index);
        let data = json!({
            "node_id": node_id,
            "heartbeat_index": heartbeat_index,
            "timestamp": timestamp,
            "block_height": block_height,
            "window": window
        });
        
        self.persistent.db.put_cf(&hb_cf, key.as_bytes(), data.to_string().as_bytes())?;
        Ok(())
    }
    
    /// Count heartbeats for node in 4h window (for reward eligibility)
    pub fn count_heartbeats_in_window(&self, node_id: &str, window_timestamp: u64) -> IntegrationResult<u8> {
        let hb_cf = self.persistent.db.cf_handle("heartbeats")
            .ok_or_else(|| IntegrationError::StorageError("heartbeats column family not found".to_string()))?;
        
        let mut count = 0u8;
        for index in 0..10 {
            let key = format!("hb_{}_{}_{}", node_id, window_timestamp, index);
            if self.persistent.db.get_cf(&hb_cf, key.as_bytes())?.is_some() {
                count += 1;
            }
        }
        Ok(count)
    }
    
    /// Check heartbeat eligibility (8/10 for Full, 9/10 for Super)
    pub fn check_heartbeat_eligibility(&self, node_id: &str, node_type: &str, window_timestamp: u64) -> IntegrationResult<(u8, u8, bool)> {
        let count = self.count_heartbeats_in_window(node_id, window_timestamp)?;
        let required = match node_type {
            "super" => 9,
            "full" => 8,
            _ => 10,
        };
        Ok((count, required, count >= required))
    }
    
    /// Cleanup old heartbeats (older than 24 hours)
    pub fn cleanup_old_heartbeats(&self, cutoff_timestamp: u64) -> IntegrationResult<u32> {
        let hb_cf = self.persistent.db.cf_handle("heartbeats")
            .ok_or_else(|| IntegrationError::StorageError("heartbeats column family not found".to_string()))?;
        
        let iter = self.persistent.db.iterator_cf(&hb_cf, rocksdb::IteratorMode::Start);
        let mut batch = WriteBatch::default();
        let mut removed = 0u32;
        
        for item in iter {
            let (key, value) = item?;
            if let Ok(parsed) = serde_json::from_slice::<serde_json::Value>(&value) {
                let timestamp = parsed["timestamp"].as_u64().unwrap_or(0);
                if timestamp < cutoff_timestamp {
                    batch.delete_cf(&hb_cf, &key);
                    removed += 1;
                }
            }
        }
        
        if batch.len() > 0 {
            self.persistent.db.write(batch)?;
        }
        
        Ok(removed)
    }
    
    // ===== FAILOVER EVENT METHODS =====
    
    /// Save a failover event (optimized with bincode serialization and LZ4 compression)
    /// NOTE: Light nodes should NOT call this method - they don't store failover history
    pub fn save_failover_event(&self, event: &FailoverEvent) -> IntegrationResult<()> {
        // OPTIMIZATION: Light nodes don't store failover events
        if std::env::var("QNET_NODE_TYPE").unwrap_or_default() == "light" {
            return Ok(()); // Skip storage for light nodes
        }
        
        let failover_cf = self.persistent.db.cf_handle("failover_events")
            .ok_or_else(|| IntegrationError::StorageError("failover_events column family not found".to_string()))?;
        
        // Use height as key for efficient range queries
        // Format: failover_<height>_<timestamp> for uniqueness
        let key = format!("failover_{:012}_{}", event.height, event.timestamp);
        
        // Serialize with bincode (more efficient than JSON)
        let value = bincode::serialize(event)
            .map_err(|e| IntegrationError::StorageError(format!("Failed to serialize failover event: {}", e)))?;
        
        self.persistent.db.put_cf(&failover_cf, key.as_bytes(), &value)?;
        
        // Auto-cleanup old events based on time relevance, not node type
        // Keep ~30 days of history (assuming ~100 failovers per day worst case)
        let max_events = match std::env::var("QNET_NODE_TYPE").unwrap_or_default().as_str() {
            "super" => 10_000,   // Super nodes: ~30 days (400KB) - enough for analysis
            "full" => 10_000,    // Full nodes: same as Super - they participate in consensus
            _ => 0,              // Light nodes: don't store (mobile devices)
        };
        
        // Only cleanup if we're not a light node
        if max_events > 0 {
            self.cleanup_old_failovers(max_events)?;
        }
        
        Ok(())
    }
    
    /// Get failover history (optimized with range queries and limit)
    pub fn get_failover_history(&self, from_height: u64, limit: usize) -> IntegrationResult<Vec<FailoverEvent>> {
        let failover_cf = self.persistent.db.cf_handle("failover_events")
            .ok_or_else(|| IntegrationError::StorageError("failover_events column family not found".to_string()))?;
        
        let mut events = Vec::new();
        let start_key = format!("failover_{:012}_", from_height);
        
        let iter = self.persistent.db.iterator_cf(
            &failover_cf,
            rocksdb::IteratorMode::From(start_key.as_bytes(), rocksdb::Direction::Forward)
        );
        
        for item in iter.take(limit) {
            let (_, value) = item?;
            
            if let Ok(event) = bincode::deserialize::<FailoverEvent>(&value) {
                if event.height >= from_height {
                    events.push(event);
                }
            }
        }
        
        Ok(events)
    }
    
    /// Get failover statistics for monitoring
    pub fn get_failover_stats(&self) -> IntegrationResult<serde_json::Value> {
        let failover_cf = self.persistent.db.cf_handle("failover_events")
            .ok_or_else(|| IntegrationError::StorageError("failover_events column family not found".to_string()))?;
        
        let mut total_count = 0;
        let mut by_producer = HashMap::<String, u32>::new();
        let mut by_reason = HashMap::<String, u32>::new();
        
        let iter = self.persistent.db.iterator_cf(&failover_cf, rocksdb::IteratorMode::Start);
        
        for item in iter {
            let (_, value) = item?;
            
            if let Ok(event) = bincode::deserialize::<FailoverEvent>(&value) {
                total_count += 1;
                *by_producer.entry(event.failed_producer).or_insert(0) += 1;
                *by_reason.entry(event.reason).or_insert(0) += 1;
            }
        }
        
        Ok(json!({
            "total_failovers": total_count,
            "by_producer": by_producer,
            "by_reason": by_reason
        }))
    }
    
    /// Cleanup old failover events with smart retention policy
    fn cleanup_old_failovers(&self, max_events: usize) -> IntegrationResult<()> {
        let failover_cf = self.persistent.db.cf_handle("failover_events")
            .ok_or_else(|| IntegrationError::StorageError("failover_events column family not found".to_string()))?;
        
        // Two-phase cleanup strategy:
        // 1. Remove events older than 30 days (primary)
        // 2. Keep max_events limit (secondary safety)
        
        let thirty_days_ago = chrono::Utc::now().timestamp() - (30 * 24 * 3600);
        let mut batch = WriteBatch::default();
        let mut count = 0;
        let mut old_count = 0;
        
        // First pass: count and remove old events
        let iter = self.persistent.db.iterator_cf(&failover_cf, rocksdb::IteratorMode::Start);
        for item in iter {
            let (key, value) = item?;
            count += 1;
            
            // Try to deserialize to check timestamp
            if let Ok(event) = bincode::deserialize::<FailoverEvent>(&value) {
                if event.timestamp < thirty_days_ago {
                    batch.delete_cf(&failover_cf, &key);
                    old_count += 1;
                }
            }
        }
        
        // Apply time-based cleanup
        if old_count > 0 {
            self.persistent.db.write(batch)?;
            println!("[STORAGE] Cleaned up {} failover events older than 30 days", old_count);
        }
        
        // Second safety check: if still too many events, trim oldest
        if count - old_count > max_events {
            let to_delete = (count - old_count) - max_events;
            let mut batch = WriteBatch::default();
            let iter = self.persistent.db.iterator_cf(&failover_cf, rocksdb::IteratorMode::Start);
            
            for item in iter.take(to_delete) {
                let (key, _) = item?;
                batch.delete_cf(&failover_cf, &key);
            }
            
            self.persistent.db.write(batch)?;
            println!("[STORAGE] Trimmed {} oldest failover events to maintain {} limit", to_delete, max_events);
        }
        
        Ok(())
    }
    
    // PRODUCTION: Snapshot system for fast node synchronization
    // Creates FULL snapshots every 10,000 blocks (~2.7 hours at 1s/block)
    // Creates INCREMENTAL snapshots every 1,000 blocks (~16.7 minutes at 1s/block)
    
    /// Create incremental state snapshot at specified height
    pub async fn create_incremental_snapshot(&self, height: u64) -> IntegrationResult<()> {
        const INCREMENTAL_INTERVAL: u64 = 1_000;
        const FULL_SNAPSHOT_INTERVAL: u64 = 10_000;
        
        // Check if this is a full snapshot height (priority)
        if height % FULL_SNAPSHOT_INTERVAL == 0 {
            return self.create_state_snapshot(height).await;
        }
        
        // Check if this is an incremental snapshot height
        if height % INCREMENTAL_INTERVAL != 0 {
            return Ok(()); // Not a snapshot height
        }
        
        println!("[SNAPSHOT] üì∏ Creating incremental snapshot at height {}", height);
        let start_time = std::time::Instant::now();
        
        // Find the previous snapshot to base delta on
        let base_height = (height / FULL_SNAPSHOT_INTERVAL) * FULL_SNAPSHOT_INTERVAL;
        if base_height == 0 {
            // No base snapshot yet, create full instead
            return self.create_state_snapshot(height).await;
        }
        
        let snapshots_cf = self.persistent.db.cf_handle("snapshots")
            .ok_or_else(|| IntegrationError::StorageError("snapshots column family not found".to_string()))?;
        
        // Collect only changes since base snapshot
        let mut delta_data = Vec::new();
        
        // 1. Add metadata
        delta_data.extend_from_slice(b"DELTA"); // Magic bytes for delta snapshot
        delta_data.extend_from_slice(&crate::node::PROTOCOL_VERSION.to_le_bytes());
        delta_data.extend_from_slice(&height.to_le_bytes());
        delta_data.extend_from_slice(&base_height.to_le_bytes());
        
        // 2. Collect changed accounts since base height
        // In production, track changes via state diffs
        let accounts_cf = self.persistent.db.cf_handle("accounts")
            .ok_or_else(|| IntegrationError::StorageError("accounts column family not found".to_string()))?;
        
        let metadata_cf = self.persistent.db.cf_handle("metadata")
            .ok_or_else(|| IntegrationError::StorageError("metadata column family not found".to_string()))?;
        
        // For now, include accounts modified in last 1000 blocks (simplified)
        // PRODUCTION: Would use change tracking from StateManager
        let mut change_count = 0u32;
        delta_data.extend_from_slice(&change_count.to_le_bytes()); // Placeholder for count
        let count_position = delta_data.len() - 4;
        
        // Collect recent transaction data to identify changed accounts
        // This is a simplified approach - production would track actual state changes
        let microblocks_cf = self.persistent.db.cf_handle("microblocks")
            .ok_or_else(|| IntegrationError::StorageError("microblocks column family not found".to_string()))?;
        
        let mut changed_accounts: std::collections::HashSet<String> = std::collections::HashSet::new();
        for block_height in (base_height + 1)..=height {
            let block_key = format!("microblock_{}", block_height);
            if let Ok(Some(_block_data)) = self.persistent.db.get_cf(&microblocks_cf, block_key.as_bytes()) {
                // In production, parse block and extract account changes
                // For now, we'll include a sample of accounts
            }
        }
        
        // 3. Compress delta
        let compressed = lz4_flex::compress_prepend_size(&delta_data);
        
        // 4. Calculate hash
        use sha3::{Sha3_256, Digest};
        let mut hasher = Sha3_256::new();
        hasher.update(&compressed);
        let hash = hasher.finalize();
        
        // Save incremental snapshot
        let snapshot_key = format!("delta_{}", height);
        let mut final_data = Vec::new();
        final_data.extend_from_slice(&hash);
        final_data.extend_from_slice(&(compressed.len() as u64).to_le_bytes());
        final_data.extend_from_slice(&compressed);
        
        self.persistent.db.put_cf(&snapshots_cf, snapshot_key.as_bytes(), &final_data)?;
        
        let duration = start_time.elapsed();
        println!("[SNAPSHOT] ‚úÖ Incremental snapshot created: {} bytes in {:.2}s (base: {})", 
                 compressed.len(), duration.as_secs_f64(), base_height);
        
        Ok(())
    }
    
    /// Create full state snapshot at specified height
    pub async fn create_state_snapshot(&self, height: u64) -> IntegrationResult<()> {
        // PRODUCTION: Only create snapshots at round boundaries (every 10,000 blocks)
        const SNAPSHOT_INTERVAL: u64 = 10_000;
        if height % SNAPSHOT_INTERVAL != 0 && height != 0 {
            return Ok(()); // Not a full snapshot height
        }
        
        println!("[SNAPSHOT] üì∏ Creating state snapshot at height {}", height);
        let start_time = std::time::Instant::now();
        
        // Get snapshot column family
        let snapshots_cf = self.persistent.db.cf_handle("snapshots")
            .ok_or_else(|| IntegrationError::StorageError("snapshots column family not found".to_string()))?;
        
        // Collect state data for snapshot
        let mut snapshot_data = Vec::new();
        
        // 1. Add protocol version for compatibility check
        snapshot_data.extend_from_slice(&crate::node::PROTOCOL_VERSION.to_le_bytes());
        
        // 2. Add height marker
        snapshot_data.extend_from_slice(&height.to_le_bytes());
        
        // 3. Add timestamp
        let timestamp = std::time::SystemTime::now()
            .duration_since(std::time::UNIX_EPOCH)
            .unwrap_or_default()
            .as_secs();
        snapshot_data.extend_from_slice(&timestamp.to_le_bytes());
        
        // 4. Serialize current state (accounts, balances, reputation)
        // Note: In production, would serialize from StateManager
        let accounts_cf = self.persistent.db.cf_handle("accounts")
            .ok_or_else(|| IntegrationError::StorageError("accounts column family not found".to_string()))?;
        
        let mut account_count = 0u64;
        let iter = self.persistent.db.iterator_cf(&accounts_cf, rocksdb::IteratorMode::Start);
        
        // Serialize account data
        for item in iter {
            let (key, value) = item?;
            snapshot_data.extend_from_slice(&(key.len() as u32).to_le_bytes());
            snapshot_data.extend_from_slice(&key);
            snapshot_data.extend_from_slice(&(value.len() as u32).to_le_bytes());
            snapshot_data.extend_from_slice(&value);
            account_count += 1;
        }
        
        // 5. Compress snapshot with LZ4 for efficient storage
        let compressed = lz4_flex::compress_prepend_size(&snapshot_data);
        
        // 6. Calculate hash for integrity check
        use sha3::{Sha3_256, Digest};
        let mut hasher = Sha3_256::new();
        hasher.update(&compressed);
        let hash = hasher.finalize();
        
        // Save snapshot with metadata
        let snapshot_key = format!("snapshot_{}", height);
        let mut final_data = Vec::new();
        final_data.extend_from_slice(&hash); // 32 bytes hash
        final_data.extend_from_slice(&(compressed.len() as u64).to_le_bytes()); // 8 bytes size
        final_data.extend_from_slice(&compressed); // Compressed data
        
        self.persistent.db.put_cf(&snapshots_cf, snapshot_key.as_bytes(), &final_data)?;
        
        // Update latest snapshot pointer
        self.persistent.db.put_cf(&snapshots_cf, b"latest_snapshot", &height.to_le_bytes())?;
        
        let duration = start_time.elapsed();
        println!("[SNAPSHOT] ‚úÖ Snapshot created: {} accounts, {} bytes compressed in {:.2}s", 
                 account_count, compressed.len(), duration.as_secs_f64());
        
        // PRODUCTION: Clean up old snapshots (keep only last 5)
        self.cleanup_old_snapshots(height, 5)?;
        
        Ok(())
    }
    
    /// Load state snapshot from specified height
    pub async fn load_state_snapshot(&self, height: u64) -> IntegrationResult<()> {
        println!("[SNAPSHOT] üìÇ Loading state snapshot from height {}", height);
        
        let snapshots_cf = self.persistent.db.cf_handle("snapshots")
            .ok_or_else(|| IntegrationError::StorageError("snapshots column family not found".to_string()))?;
        
        let snapshot_key = format!("snapshot_{}", height);
        let snapshot_data = self.persistent.db.get_cf(&snapshots_cf, snapshot_key.as_bytes())?
            .ok_or_else(|| IntegrationError::StorageError(format!("Snapshot at height {} not found", height)))?;
        
        // Verify hash
        let stored_hash = &snapshot_data[..32];
        let size = u64::from_le_bytes(snapshot_data[32..40].try_into().unwrap()) as usize;
        let compressed_data = &snapshot_data[40..];
        
        use sha3::{Sha3_256, Digest};
        let mut hasher = Sha3_256::new();
        hasher.update(compressed_data);
        let computed_hash = hasher.finalize();
        
        if stored_hash != computed_hash.as_slice() {
            return Err(IntegrationError::StorageError("Snapshot integrity check failed".to_string()));
        }
        
        // Decompress
        let decompressed = lz4_flex::decompress_size_prepended(compressed_data)
            .map_err(|e| IntegrationError::StorageError(format!("Decompression failed: {}", e)))?;
        
        // Parse and restore state
        let mut cursor = 0;
        
        // Check protocol version
        let version = u32::from_le_bytes(decompressed[0..4].try_into().unwrap());
        cursor += 4;
        
        if version != crate::node::PROTOCOL_VERSION {
            println!("[SNAPSHOT] ‚ö†Ô∏è Version mismatch: snapshot v{}, current v{}", 
                     version, crate::node::PROTOCOL_VERSION);
        }
        
        // Skip height and timestamp
        cursor += 16;
        
        // Restore accounts
        let accounts_cf = self.persistent.db.cf_handle("accounts")
            .ok_or_else(|| IntegrationError::StorageError("accounts column family not found".to_string()))?;
        
        let mut batch = WriteBatch::default();
        let mut account_count = 0;
        
        while cursor < decompressed.len() {
            let key_len = u32::from_le_bytes(decompressed[cursor..cursor+4].try_into().unwrap()) as usize;
            cursor += 4;
            let key = &decompressed[cursor..cursor+key_len];
            cursor += key_len;
            
            let value_len = u32::from_le_bytes(decompressed[cursor..cursor+4].try_into().unwrap()) as usize;
            cursor += 4;
            let value = &decompressed[cursor..cursor+value_len];
            cursor += value_len;
            
            batch.put_cf(&accounts_cf, key, value);
            account_count += 1;
        }
        
        self.persistent.db.write(batch)?;
        
        println!("[SNAPSHOT] ‚úÖ Restored {} accounts from snapshot", account_count);
        
        Ok(())
    }
    
    /// Clean up old snapshots, keeping only the most recent ones
    fn cleanup_old_snapshots(&self, current_height: u64, keep_count: usize) -> IntegrationResult<()> {
        let snapshots_cf = self.persistent.db.cf_handle("snapshots")
            .ok_or_else(|| IntegrationError::StorageError("snapshots column family not found".to_string()))?;
        
        // Find all snapshots
        let mut snapshots = Vec::new();
        let iter = self.persistent.db.iterator_cf(&snapshots_cf, rocksdb::IteratorMode::Start);
        
        for item in iter {
            let (key, _) = item?;
            let key_str = String::from_utf8_lossy(&key);
            if key_str.starts_with("snapshot_") {
                if let Some(height_str) = key_str.strip_prefix("snapshot_") {
                    if let Ok(height) = height_str.parse::<u64>() {
                        snapshots.push(height);
                    }
                }
            }
        }
        
        // Sort and keep only recent ones
        snapshots.sort_unstable();
        snapshots.reverse(); // Most recent first
        
        if snapshots.len() > keep_count {
            let mut batch = WriteBatch::default();
            for &height in &snapshots[keep_count..] {
                let key = format!("snapshot_{}", height);
                batch.delete_cf(&snapshots_cf, key.as_bytes());
                println!("[SNAPSHOT] üóëÔ∏è Removing old snapshot at height {}", height);
            }
            self.persistent.db.write(batch)?;
        }
        
        Ok(())
    }
    
    // PRODUCTION: IPFS integration for decentralized snapshot distribution
    
    /// Upload snapshot to IPFS and return CID (Content Identifier)
    pub async fn upload_snapshot_to_ipfs(&self, height: u64) -> IntegrationResult<String> {
        // PRODUCTION: Check if IPFS is available (OPTIONAL feature)
        let ipfs_api = match std::env::var("IPFS_API_URL") {
            Ok(url) => url,
            Err(_) => {
                // IPFS is OPTIONAL - skip if not configured
                return Err(IntegrationError::Other("IPFS not configured (set IPFS_API_URL to enable)".to_string()));
            }
        };
        
        println!("[IPFS] üì§ Uploading snapshot at height {} to IPFS...", height);
        
        // Get snapshot data BEFORE any async operations (avoids Send issues)
        let snapshot_data = {
            let snapshots_cf = self.persistent.db.cf_handle("snapshots")
                .ok_or_else(|| IntegrationError::StorageError("snapshots column family not found".to_string()))?;
            
            let snapshot_key = format!("snapshot_{}", height);
            self.persistent.db.get_cf(&snapshots_cf, snapshot_key.as_bytes())?
                .ok_or_else(|| IntegrationError::StorageError(format!("Snapshot at height {} not found", height)))?
        }; // RocksDB handle is dropped here
        
        // PRODUCTION: Create IPFS-compatible metadata
        let metadata = json!({
            "version": crate::node::PROTOCOL_VERSION,
            "height": height,
            "timestamp": chrono::Utc::now().timestamp(),
            "type": "qnet_snapshot",
            "compression": "lz4",
            "size": snapshot_data.len()
        });
        
        // PRODUCTION: Use HTTP client to upload to IPFS
        // In production environment, would use ipfs-api crate
        let client = reqwest::Client::builder()
            .timeout(std::time::Duration::from_secs(120)) // 2 minutes for large snapshots
            .build()
            .map_err(|e| IntegrationError::Other(format!("HTTP client error: {}", e)))?;
        
        // Create multipart form for IPFS add endpoint
        let form = reqwest::multipart::Form::new()
            .part("file", reqwest::multipart::Part::bytes(snapshot_data)
                .file_name(format!("qnet_snapshot_{}.dat", height)));
        
        // Upload to IPFS
        let response = client.post(&format!("{}/api/v0/add", ipfs_api))
            .multipart(form)
            .send()
            .await
            .map_err(|e| IntegrationError::Other(format!("IPFS upload failed: {}", e)))?;
        
        if response.status().is_success() {
            let result: serde_json::Value = response.json().await
                .map_err(|e| IntegrationError::Other(format!("IPFS response parse error: {}", e)))?;
            
            if let Some(cid) = result.get("Hash").and_then(|v| v.as_str()) {
                // Store IPFS CID reference (in a scope to drop cf_handle)
                {
                    let ipfs_key = format!("ipfs_{}", height);
                    let snapshots_cf = self.persistent.db.cf_handle("snapshots")
                        .ok_or_else(|| IntegrationError::StorageError("snapshots column family not found".to_string()))?;
                    self.persistent.db.put_cf(&snapshots_cf, ipfs_key.as_bytes(), cid.as_bytes())?;
                } // cf_handle is dropped here
                
                println!("[IPFS] ‚úÖ Snapshot uploaded to IPFS: {}", cid);
                
                // PRODUCTION: Pin the content to ensure persistence (now safe after cf_handle is dropped)
                self.pin_ipfs_content(&ipfs_api, cid).await?;
                
                return Ok(cid.to_string());
            }
        }
        
        Err(IntegrationError::StorageError("Failed to upload snapshot to IPFS".to_string()))
    }
    
    /// Pin IPFS content to ensure it stays available
    async fn pin_ipfs_content(&self, ipfs_api: &str, cid: &str) -> IntegrationResult<()> {
        let client = reqwest::Client::builder()
            .timeout(std::time::Duration::from_secs(30))
            .build()
            .map_err(|e| IntegrationError::Other(format!("HTTP client error: {}", e)))?;
        
        let response = client.post(&format!("{}/api/v0/pin/add", ipfs_api))
            .query(&[("arg", cid)])
            .send()
            .await
            .map_err(|e| IntegrationError::Other(format!("IPFS pin failed: {}", e)))?;
        
        if response.status().is_success() {
            println!("[IPFS] üìå Content pinned: {}", cid);
            Ok(())
        } else {
            Err(IntegrationError::StorageError(format!("Failed to pin IPFS content: {}", cid)))
        }
    }
    
    /// Download snapshot from IPFS by CID
    pub async fn download_snapshot_from_ipfs(&self, cid: &str, height: u64) -> IntegrationResult<()> {
        let ipfs_gateway = match std::env::var("IPFS_GATEWAY_URL") {
            Ok(url) => url,
            Err(_) => {
                // DECENTRALIZED: No default to centralized services!
                // User must configure their own IPFS gateway or local node
                return Err(IntegrationError::Other(
                    "IPFS gateway not configured (set IPFS_GATEWAY_URL or run local IPFS node)".to_string()
                ));
            }
        };
        
        println!("[IPFS] üì• Downloading snapshot from IPFS: {}", cid);
        
        // PRODUCTION: Try gateways from environment or peers
        let mut gateways = vec![ipfs_gateway.clone()];
        
        // Add additional gateways from environment (comma-separated)
        if let Ok(extra_gateways) = std::env::var("IPFS_EXTRA_GATEWAYS") {
            for gateway in extra_gateways.split(',') {
                gateways.push(gateway.trim().to_string());
            }
        }
        
        // DECENTRALIZED: Prefer local IPFS nodes from peers
        // In production, would discover IPFS gateways from P2P network
        // Not hardcoding any centralized services!
        
        let client = reqwest::Client::builder()
            .timeout(std::time::Duration::from_secs(300)) // 5 minutes for large downloads
            .build()
            .map_err(|e| IntegrationError::Other(format!("HTTP client error: {}", e)))?;
        
        let mut snapshot_data = None;
        
        // Try each gateway until success
        for gateway in &gateways {
            let url = format!("{}/ipfs/{}", gateway, cid);
            println!("[IPFS] üîÑ Trying gateway: {}", gateway);
            
            match client.get(&url).send().await {
                Ok(response) if response.status().is_success() => {
                    match response.bytes().await {
                        Ok(data) => {
                            snapshot_data = Some(data.to_vec());
                            println!("[IPFS] ‚úÖ Downloaded {} bytes from {}", data.len(), gateway);
                            break;
                        },
                        Err(e) => {
                            println!("[IPFS] ‚ö†Ô∏è Failed to read data from {}: {}", gateway, e);
                            continue;
                        }
                    }
                },
                Ok(response) => {
                    println!("[IPFS] ‚ö†Ô∏è Gateway {} returned status: {}", gateway, response.status());
                    continue;
                },
                Err(e) => {
                    println!("[IPFS] ‚ö†Ô∏è Failed to connect to {}: {}", gateway, e);
                    continue;
                }
            }
        }
        
        let data = snapshot_data
            .ok_or_else(|| IntegrationError::StorageError("Failed to download from any IPFS gateway".to_string()))?;
        
        // Verify and save snapshot
        let snapshots_cf = self.persistent.db.cf_handle("snapshots")
            .ok_or_else(|| IntegrationError::StorageError("snapshots column family not found".to_string()))?;
        
        // Verify hash before saving
        use sha3::{Sha3_256, Digest};
        let mut hasher = Sha3_256::new();
        hasher.update(&data[40..]); // Skip hash and size fields
        let computed_hash = hasher.finalize();
        
        if &data[..32] != computed_hash.as_slice() {
            return Err(IntegrationError::StorageError("IPFS snapshot integrity check failed".to_string()));
        }
        
        // Save snapshot locally
        let snapshot_key = format!("snapshot_{}", height);
        self.persistent.db.put_cf(&snapshots_cf, snapshot_key.as_bytes(), &data)?;
        
        // Save IPFS reference
        let ipfs_key = format!("ipfs_{}", height);
        self.persistent.db.put_cf(&snapshots_cf, ipfs_key.as_bytes(), cid.as_bytes())?;
        
        println!("[IPFS] ‚úÖ Snapshot saved from IPFS (height: {})", height);
        
        Ok(())
    }
    
    /// Get IPFS CID for a snapshot at given height
    pub fn get_snapshot_ipfs_cid(&self, height: u64) -> IntegrationResult<Option<String>> {
        let snapshots_cf = self.persistent.db.cf_handle("snapshots")
            .ok_or_else(|| IntegrationError::StorageError("snapshots column family not found".to_string()))?;
        
        let ipfs_key = format!("ipfs_{}", height);
        match self.persistent.db.get_cf(&snapshots_cf, ipfs_key.as_bytes())? {
            Some(cid_bytes) => Ok(Some(String::from_utf8_lossy(&cid_bytes).to_string())),
            None => Ok(None)
        }
    }
    
    /// Share snapshot via P2P network (announce IPFS CID to peers)
    pub async fn announce_snapshot_to_peers(&self, height: u64, cid: &str, p2p: &crate::unified_p2p::SimplifiedP2P) {
        println!("[P2P] üì¢ Announcing snapshot to peers: height={}, CID={}", height, cid);
        
        // Create announcement message
        let announcement = json!({
            "type": "snapshot_available",
            "height": height,
            "ipfs_cid": cid,
            "timestamp": chrono::Utc::now().timestamp(),
            "node_id": p2p.node_id.clone()
        });
        
        // Broadcast to all connected peers
        let peers = p2p.get_validated_active_peers();
        for peer in &peers {
            let message = crate::unified_p2p::NetworkMessage::StateSnapshot {
                height,
                ipfs_cid: cid.to_string(),
                sender_id: p2p.node_id.clone(),
            };
            
            p2p.send_network_message(&peer.addr, message);
        }
        
        println!("[P2P] ‚úÖ Snapshot announcement sent to {} peers", peers.len());
    }
    
    /// SLIDING WINDOW: Prune old blocks outside of retention window
    pub fn prune_old_blocks(&self) -> IntegrationResult<()> {
        // Super nodes keep everything (archival role)
        if self.storage_mode == StorageMode::Super {
            return Ok(()); // Super nodes are our "archive" nodes - keep everything
        }
        
        // Light nodes don't store full blocks at all
        if self.storage_mode == StorageMode::Light {
            return self.prune_for_light_node();
        }
        
        let current_height = self.get_chain_height()?;
        if current_height <= self.sliding_window_size {
            return Ok(()); // Not enough blocks yet
        }
        
        let prune_before = current_height - self.sliding_window_size;
        
        // Find last snapshot before pruning point
        let last_snapshot = (prune_before / 10_000) * 10_000; // Round down to snapshot
        if last_snapshot == 0 {
            return Ok(()); // Don't prune before first snapshot
        }
        
        println!("[PRUNING] üóëÔ∏è Starting block pruning (keeping blocks {} and newer)", prune_before);
        
        let microblocks_cf = self.persistent.db.cf_handle("microblocks")
            .ok_or_else(|| IntegrationError::StorageError("microblocks column family not found".to_string()))?;
        
        let mut batch = WriteBatch::default();
        let mut pruned_count = 0;
        
        // Prune blocks before the window, but after last snapshot
        for height in (last_snapshot + 1)..prune_before {
            // Prune microblocks
            let micro_key = format!("microblock_{}", height);
            if self.persistent.db.get_cf(&microblocks_cf, micro_key.as_bytes())?.is_some() {
                batch.delete_cf(&microblocks_cf, micro_key.as_bytes());
                pruned_count += 1;
            }
            
            // CRITICAL FIX: Also prune macroblocks (they were NEVER deleted!)
            // Macroblocks have their own numbering: macro #1 = after micro 90, macro #2 = after micro 180
            // Check if this microblock height corresponds to a macroblock
            if height % 90 == 0 && height > 0 {
                // This microblock height has a corresponding macroblock
                let macro_number = height / 90;
                let macro_key = format!("macroblock_{}", macro_number);
                if self.persistent.db.get_cf(&microblocks_cf, macro_key.as_bytes())?.is_some() {
                    batch.delete_cf(&microblocks_cf, macro_key.as_bytes());
                    pruned_count += 1;
                    println!("[PRUNING] üî• Pruning macroblock #{} (at microblock height {})", 
                            macro_number, height);
                }
            }
                
                // Apply batch every 1000 blocks to avoid memory issues
                if pruned_count % 1000 == 0 {
                    self.persistent.db.write(batch)?;
                    batch = WriteBatch::default();
                    println!("[PRUNING] Pruned {} blocks...", pruned_count);
            }
        }
        
        // Apply remaining batch
        if !batch.is_empty() {
            self.persistent.db.write(batch)?;
        }
        
        // Force compaction to reclaim space
        self.persistent.db.compact_range_cf(&microblocks_cf, 
            Some(format!("microblock_{}", last_snapshot).as_bytes()),
            Some(format!("microblock_{}", prune_before).as_bytes()));
        
        println!("[PRUNING] ‚úÖ Pruned {} blocks (before height {}), keeping snapshot at {}", 
                pruned_count, prune_before, last_snapshot);
        
        // CRITICAL: Also prune transactions from pruned blocks
        // Transactions are stored separately and must be cleaned up
        let tx_pruned = self.prune_old_transactions(prune_before)?;
        if tx_pruned > 0 {
            println!("[PRUNING] ‚úÖ Pruned {} old transactions", tx_pruned);
        }
        
        // Update metadata
        let metadata_cf = self.persistent.db.cf_handle("metadata")
            .ok_or_else(|| IntegrationError::StorageError("metadata column family not found".to_string()))?;
        self.persistent.db.put_cf(&metadata_cf, b"oldest_block", &prune_before.to_le_bytes())?;
        
        Ok(())
    }
    
    /// PRODUCTION: Prune old transactions that are no longer in retained blocks
    /// Transactions are stored separately from blocks for fast lookup
    /// After block pruning, orphaned transactions must also be removed
    fn prune_old_transactions(&self, prune_before_height: u64) -> IntegrationResult<u64> {
        let tx_cf = self.persistent.db.cf_handle("transactions")
            .ok_or_else(|| IntegrationError::StorageError("transactions column family not found".to_string()))?;
        let tx_index_cf = self.persistent.db.cf_handle("tx_index")
            .ok_or_else(|| IntegrationError::StorageError("tx_index column family not found".to_string()))?;
        let tx_by_addr_cf = self.persistent.db.cf_handle("tx_by_address")
            .ok_or_else(|| IntegrationError::StorageError("tx_by_address column family not found".to_string()))?;
        
        let mut batch = WriteBatch::default();
        let mut pruned_count: u64 = 0;
        let mut tx_hashes_to_prune: Vec<String> = Vec::new();
        
        // Step 1: Find transactions in blocks before prune_before_height using tx_index
        let iter = self.persistent.db.iterator_cf(&tx_index_cf, rocksdb::IteratorMode::Start);
        for item in iter {
            let (key, value) = item?;
            
            // tx_index stores: tx_hash -> block_height
            if value.len() >= 8 {
                let block_height = u64::from_be_bytes(value[..8].try_into().unwrap_or([0u8; 8]));
                
                if block_height < prune_before_height {
                    let tx_key = String::from_utf8_lossy(&key).to_string();
                    tx_hashes_to_prune.push(tx_key);
                }
            }
        }
        
        // Step 2: Delete transactions and their indices
        for tx_key in &tx_hashes_to_prune {
            // Delete from transactions CF
            batch.delete_cf(&tx_cf, tx_key.as_bytes());
            
            // Delete from tx_index CF
            batch.delete_cf(&tx_index_cf, tx_key.as_bytes());
            
            pruned_count += 1;
            
            // Apply batch every 1000 transactions to avoid memory issues
            if pruned_count % 1000 == 0 {
                self.persistent.db.write(batch)?;
                batch = WriteBatch::default();
                println!("[PRUNING] Pruned {} transactions...", pruned_count);
            }
        }
        
        // Step 3: Clean up tx_by_address index (more complex - need to scan)
        // This index stores: addr_{address}_{timestamp}_{tx_hash}
        // We need to remove entries for pruned transactions
        let addr_iter = self.persistent.db.iterator_cf(&tx_by_addr_cf, rocksdb::IteratorMode::Start);
        let mut addr_keys_to_delete: Vec<Vec<u8>> = Vec::new();
        
        for item in addr_iter {
            let (key, _value) = item?;
            let key_str = String::from_utf8_lossy(&key);
            
            // Extract tx_hash from key format: addr_{address}_{timestamp}_{tx_hash}
            if let Some(tx_hash) = key_str.rsplit('_').next() {
                let tx_key = format!("tx_{}", tx_hash);
                if tx_hashes_to_prune.contains(&tx_key) {
                    addr_keys_to_delete.push(key.to_vec());
                }
            }
        }
        
        for key in addr_keys_to_delete {
            batch.delete_cf(&tx_by_addr_cf, &key);
        }
        
        // Apply remaining batch
        if !batch.is_empty() {
            self.persistent.db.write(batch)?;
        }
        
        // Force compaction on transaction CFs to reclaim space
        if pruned_count > 0 {
            self.persistent.db.compact_range_cf(&tx_cf, None::<&[u8]>, None::<&[u8]>);
            self.persistent.db.compact_range_cf(&tx_index_cf, None::<&[u8]>, None::<&[u8]>);
            self.persistent.db.compact_range_cf(&tx_by_addr_cf, None::<&[u8]>, None::<&[u8]>);
        }
        
        Ok(pruned_count)
    }
    
    /// Light node pruning - keep only block headers and recent state
    fn prune_for_light_node(&self) -> IntegrationResult<()> {
        println!("[PRUNING] ü™∂ Light node mode - keeping only headers and state");
        
        let microblocks_cf = self.persistent.db.cf_handle("microblocks")
            .ok_or_else(|| IntegrationError::StorageError("microblocks column family not found".to_string()))?;
        
        let mut batch = WriteBatch::default();
        let mut converted = 0;
        
        // Convert full blocks to headers only
        let iter = self.persistent.db.iterator_cf(&microblocks_cf, rocksdb::IteratorMode::Start);
        for item in iter {
            let (key, value) = item?;
            
            // Skip if already a header
            if value.len() < 1000 { // Headers are much smaller than full blocks
                continue;
            }
            
            // Extract header from full block (simplified - in production would deserialize properly)
            let header = &value[..200.min(value.len())]; // First 200 bytes as header
            batch.put_cf(&microblocks_cf, &key, header);
            converted += 1;
            
            if converted % 100 == 0 {
                self.persistent.db.write(batch)?;
                batch = WriteBatch::default();
            }
        }
        
        if !batch.is_empty() {
            self.persistent.db.write(batch)?;
        }
        
        println!("[PRUNING] ‚úÖ Converted {} blocks to headers-only format", converted);
        
        Ok(())
    }
    
    /// Get current storage mode
    pub fn get_storage_mode(&self) -> StorageMode {
        self.storage_mode
    }
    
    /// Check if block is within retention window
    pub fn is_block_retained(&self, height: u64) -> bool {
        if self.storage_mode == StorageMode::Super {
            return true; // Super nodes keep everything
        }
        
        if self.storage_mode == StorageMode::Light {
            return false; // Light nodes don't keep full blocks
        }
        
        let current = self.get_chain_height().unwrap_or(0);
        height + self.sliding_window_size > current
    }
    
    /// Estimate storage requirements for current configuration
    pub fn estimate_storage_requirements(&self) -> String {
        match self.storage_mode {
            StorageMode::Light => "~50-100 MB (headers + minimal state, mobile/IoT devices)".to_string(),
            StorageMode::Full => format!("~50-100 GB (last {} blocks + snapshots)", self.sliding_window_size),
            StorageMode::Super => "500 GB - 1 TB (complete blockchain history with compression)".to_string(),
        }
    }
    
    /// Get latest snapshot height for fast sync
    pub fn get_latest_snapshot_height(&self) -> IntegrationResult<Option<u64>> {
        let snapshots_cf = self.persistent.db.cf_handle("snapshots")
            .ok_or_else(|| IntegrationError::StorageError("snapshots column family not found".to_string()))?;
        
        // Check for latest snapshot pointer
        if let Ok(Some(data)) = self.persistent.db.get_cf(&snapshots_cf, b"latest_snapshot") {
            let height = u64::from_le_bytes(data[..8].try_into()
                .map_err(|_| IntegrationError::StorageError("Invalid snapshot height format".to_string()))?);
            return Ok(Some(height));
        }
        
        // Otherwise scan for snapshots
        let mut latest_height = 0u64;
        let iter = self.persistent.db.iterator_cf(&snapshots_cf, rocksdb::IteratorMode::Start);
        
        for item in iter {
            let (key, _) = item?;
            let key_str = String::from_utf8_lossy(&key);
            
            if key_str.starts_with("snapshot_") {
                if let Ok(height) = key_str.trim_start_matches("snapshot_").parse::<u64>() {
                    if height > latest_height {
                        latest_height = height;
                    }
                }
            }
        }
        
        if latest_height > 0 {
            Ok(Some(latest_height))
        } else {
            Ok(None)
        }
    }
    
    /// Get raw snapshot data for P2P download (v2.19.12)
    /// Returns compressed binary snapshot data
    pub fn get_snapshot_data(&self, height: u64) -> IntegrationResult<Option<Vec<u8>>> {
        let snapshots_cf = self.persistent.db.cf_handle("snapshots")
            .ok_or_else(|| IntegrationError::StorageError("snapshots column family not found".to_string()))?;
        
        let snapshot_key = format!("snapshot_{}", height);
        
        match self.persistent.db.get_cf(&snapshots_cf, snapshot_key.as_bytes())? {
            Some(data) => Ok(Some(data)),
            None => {
                // Try state_ prefix as fallback
                let state_key = format!("state_{}", height);
                match self.persistent.db.get_cf(&snapshots_cf, state_key.as_bytes())? {
                    Some(data) => Ok(Some(data)),
                    None => Ok(None)
                }
            }
        }
    }
    
    /// Download snapshot from network for fast bootstrap
    pub async fn download_and_load_snapshot(&self, p2p: &crate::unified_p2p::SimplifiedP2P) -> IntegrationResult<u64> {
        println!("[SNAPSHOT] üîç Searching for network snapshots...");
        
        let peers = p2p.get_validated_active_peers();
        if peers.is_empty() {
            return Err(IntegrationError::Other("No peers available for snapshot download".to_string()));
        }
        
        // Query peers for latest snapshot
        for peer in peers {
            match self.query_peer_snapshot(&peer.addr).await {
                Ok(Some((height, cid))) => {
                    println!("[SNAPSHOT] üì• Found snapshot at height {} from peer {}", height, peer.id);
                    
                    // Download from IPFS or directly from peer
                    if !cid.is_empty() && std::env::var("IPFS_ENABLED").unwrap_or_default() == "1" {
                        // Try IPFS first
                        if let Ok(_) = self.download_snapshot_from_ipfs(&cid, height).await {
                            println!("[SNAPSHOT] ‚úÖ Downloaded snapshot from IPFS");
                            return Ok(height);
                        }
                    }
                    
                    // Fallback to direct P2P download
                    if let Ok(_) = self.download_snapshot_from_peer(&peer.addr, height).await {
                        println!("[SNAPSHOT] ‚úÖ Downloaded snapshot from peer {}", peer.id);
                        return Ok(height);
                    }
                },
                _ => continue,
            }
        }
        
        Err(IntegrationError::Other("No snapshots available from network".to_string()))
    }
    
    /// Query peer for available snapshots
    async fn query_peer_snapshot(&self, peer_addr: &str) -> IntegrationResult<Option<(u64, String)>> {
        // Query peer's /api/v1/snapshot endpoint
        let url = format!("http://{}/api/v1/snapshot/latest", peer_addr);
        
        match reqwest::get(&url).await {
            Ok(response) => {
                if response.status().is_success() {
                    let data: serde_json::Value = response.json().await
                        .map_err(|e| IntegrationError::Other(format!("JSON error: {}", e)))?;
                    
                    if let (Some(height), Some(cid)) = (
                        data["height"].as_u64(),
                        data["ipfs_cid"].as_str()
                    ) {
                        return Ok(Some((height, cid.to_string())));
                    }
                }
            },
            Err(e) => println!("[SNAPSHOT] Failed to query peer {}: {}", peer_addr, e),
        }
        
        Ok(None)
    }
    
    /// Download snapshot directly from peer
    async fn download_snapshot_from_peer(&self, peer_addr: &str, height: u64) -> IntegrationResult<()> {
        let url = format!("http://{}/api/v1/snapshot/{}", peer_addr, height);
        
        let response = reqwest::get(&url).await
            .map_err(|e| IntegrationError::Other(format!("Download error: {}", e)))?;
        
        if !response.status().is_success() {
            return Err(IntegrationError::Other("Snapshot download failed".to_string()));
        }
        
        let data = response.bytes().await
            .map_err(|e| IntegrationError::Other(format!("Download error: {}", e)))?;
        
        // Save and load snapshot
        let snapshots_cf = self.persistent.db.cf_handle("snapshots")
            .ok_or_else(|| IntegrationError::StorageError("snapshots column family not found".to_string()))?;
        
        let snapshot_key = format!("snapshot_{}", height);
        self.persistent.db.put_cf(&snapshots_cf, snapshot_key.as_bytes(), &data)?;
        
        // Load into state
        self.load_state_snapshot(height).await?;
        
        Ok(())
    }
    
    /// Fast sync with snapshot for new nodes
    pub async fn fast_sync_with_snapshot(&self, p2p: &crate::unified_p2p::SimplifiedP2P, target_height: u64) -> IntegrationResult<()> {
        println!("[SYNC] ‚ö° Starting fast sync to height {}", target_height);
        
        // For Light nodes, only sync recent state
        if self.storage_mode == StorageMode::Light {
            println!("[SYNC] üì± Light node: syncing only recent headers");
            return Ok(());
        }
        
        // Try to find and load a snapshot
        match self.download_and_load_snapshot(p2p).await {
            Ok(snapshot_height) => {
                println!("[SYNC] üì∏ Loaded snapshot at height {}", snapshot_height);
                
                // Now sync remaining blocks from snapshot to target
                if target_height > snapshot_height {
                    println!("[SYNC] üì• Syncing remaining {} blocks...", 
                            target_height - snapshot_height);
                    // The node will handle syncing remaining blocks
                }
                
                Ok(())
            },
            Err(e) => {
                println!("[SYNC] ‚ö†Ô∏è Snapshot sync failed: {:?}, falling back to full sync", e);
                // Fall back to normal sync
                Err(e)
            }
        }
    }
    
    // =========================================================================
    // SMART CONTRACT STORAGE METHODS
    // =========================================================================
    
    /// Get contract info by address
    pub fn get_contract_info(&self, contract_address: &str) -> IntegrationResult<Option<StoredContractInfo>> {
        let key = format!("contract:info:{}", contract_address);
        
        match self.persistent.load_raw(&key)? {
            Some(data) => {
                match serde_json::from_slice::<StoredContractInfo>(&data) {
                    Ok(stored) => Ok(Some(stored)),
                    Err(e) => {
                        println!("[Storage] Failed to deserialize contract info: {:?}", e);
                        Ok(None)
                    }
                }
            }
            None => Ok(None)
        }
    }
    
    /// Save contract info
    pub fn save_contract_info(&self, contract_address: &str, info: &StoredContractInfo) -> IntegrationResult<()> {
        let key = format!("contract:info:{}", contract_address);
        
        let data = serde_json::to_vec(info)
            .map_err(|e| IntegrationError::SerializationError(e.to_string()))?;
        
        self.persistent.save_raw(&key, &data)?;
        
        // Also save to contract list for enumeration
        self.add_contract_to_list(contract_address)?;
        
        Ok(())
    }
    
    /// Add contract address to the list of all contracts
    fn add_contract_to_list(&self, contract_address: &str) -> IntegrationResult<()> {
        let list_key = "contract:list";
        
        // Load existing list
        let mut contracts: Vec<String> = match self.persistent.load_raw(list_key)? {
            Some(data) => serde_json::from_slice(&data).unwrap_or_default(),
            None => Vec::new(),
        };
        
        // Add if not already present
        if !contracts.contains(&contract_address.to_string()) {
            contracts.push(contract_address.to_string());
            let data = serde_json::to_vec(&contracts)
                .map_err(|e| IntegrationError::SerializationError(e.to_string()))?;
            self.persistent.save_raw(list_key, &data)?;
        }
        
        Ok(())
    }
    
    /// Get list of all contract addresses
    pub fn get_all_contract_addresses(&self) -> IntegrationResult<Vec<String>> {
        let list_key = "contract:list";
        
        match self.persistent.load_raw(list_key)? {
            Some(data) => {
                let contracts: Vec<String> = serde_json::from_slice(&data)
                    .unwrap_or_default();
                Ok(contracts)
            }
            None => Ok(Vec::new())
        }
    }
    
    /// Get contract state value by key
    pub fn get_contract_state(&self, contract_address: &str, state_key: &str) -> IntegrationResult<Option<String>> {
        let key = format!("contract:state:{}:{}", contract_address, state_key);
        
        match self.persistent.load_raw(&key)? {
            Some(data) => {
                match String::from_utf8(data) {
                    Ok(value) => Ok(Some(value)),
                    Err(e) => {
                        println!("[Storage] Failed to decode contract state: {:?}", e);
                        Ok(None)
                    }
                }
            }
            None => Ok(None)
        }
    }
    
    /// Save contract state value
    pub fn save_contract_state(&self, contract_address: &str, state_key: &str, value: &str) -> IntegrationResult<()> {
        let key = format!("contract:state:{}:{}", contract_address, state_key);
        self.persistent.save_raw(&key, value.as_bytes())
    }
    
    /// Save contract WASM code
    pub fn save_contract_code(&self, code_hash: &str, wasm_code: &[u8]) -> IntegrationResult<()> {
        let key = format!("contract:code:{}", code_hash);
        self.persistent.save_raw(&key, wasm_code)
    }
    
    /// Get contract WASM code by hash
    pub fn get_contract_code(&self, code_hash: &str) -> IntegrationResult<Option<Vec<u8>>> {
        let key = format!("contract:code:{}", code_hash);
        self.persistent.load_raw(&key)
    }
    
    // =========================================================================
    // JAIL PERSISTENCE (for network-wide consistency)
    // =========================================================================
    
    /// Save jail status for a node (persists across restarts)
    pub fn save_jail_status(&self, node_id: &str, jailed_until: u64, jail_count: u32, reason: &str) -> IntegrationResult<()> {
        let key = format!("jail:{}", node_id);
        let value = format!("{}:{}:{}", jailed_until, jail_count, reason);
        self.persistent.save_raw(&key, value.as_bytes())
    }
    
    /// Get jail status for a node
    pub fn get_jail_status(&self, node_id: &str) -> IntegrationResult<Option<(u64, u32, String)>> {
        let key = format!("jail:{}", node_id);
        match self.persistent.load_raw(&key)? {
            Some(data) => {
                match String::from_utf8(data) {
                    Ok(value) => {
                        let parts: Vec<&str> = value.splitn(3, ':').collect();
                        if parts.len() >= 3 {
                            let jailed_until = parts[0].parse().unwrap_or(0);
                            let jail_count = parts[1].parse().unwrap_or(0);
                            let reason = parts[2].to_string();
                            Ok(Some((jailed_until, jail_count, reason)))
                        } else {
                            Ok(None)
                        }
                    }
                    Err(_) => Ok(None)
                }
            }
            None => Ok(None)
        }
    }
    
    /// Remove jail status for a node (when released)
    pub fn remove_jail_status(&self, node_id: &str) -> IntegrationResult<()> {
        let key = format!("jail:{}", node_id);
        // Save empty to mark as removed (RocksDB doesn't have direct delete in our wrapper)
        self.persistent.save_raw(&key, &[])
    }
    
    /// Get all jail statuses (for loading on startup)
    pub fn get_all_jail_statuses(&self) -> IntegrationResult<Vec<(String, u64, u32, String)>> {
        // Scan for all jail: prefixed keys
        let mut result = Vec::new();
        
        // Use iterator if available, otherwise return empty
        // Note: This is a simplified implementation - in production you'd use RocksDB iterator
        // For now, we rely on network sync for jail propagation
        
        Ok(result)
    }
}

// =========================================================================
// SMART CONTRACT STORAGE STRUCTURES (outside impl block)
// =========================================================================

/// Contract information stored on-chain
#[derive(Debug, Clone, serde::Serialize, serde::Deserialize)]
pub struct StoredContractInfo {
    pub address: String,
    pub deployer: String,
    pub deployed_at: u64,
    pub code_hash: String,
    pub version: String,
    pub total_gas_used: u64,
    pub call_count: u64,
    pub is_active: bool,
} 